<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Build, criticise, and expand time series models with NumPyro | Difference and Repetition</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Build, criticise, and expand time series models with NumPyro" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bayesian modeling of global trend, local variation, seasonality, and heterogeneity in time series" />
<meta property="og:description" content="Bayesian modeling of global trend, local variation, seasonality, and heterogeneity in time series" />
<link rel="canonical" href="https://riversdark.github.io/variation/data/2021/05/18/lgts.html" />
<meta property="og:url" content="https://riversdark.github.io/variation/data/2021/05/18/lgts.html" />
<meta property="og:site_name" content="Difference and Repetition" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-18T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Build, criticise, and expand time series models with NumPyro" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-05-18T00:00:00-05:00","datePublished":"2021-05-18T00:00:00-05:00","description":"Bayesian modeling of global trend, local variation, seasonality, and heterogeneity in time series","headline":"Build, criticise, and expand time series models with NumPyro","mainEntityOfPage":{"@type":"WebPage","@id":"https://riversdark.github.io/variation/data/2021/05/18/lgts.html"},"url":"https://riversdark.github.io/variation/data/2021/05/18/lgts.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/variation/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://riversdark.github.io/variation/feed.xml" title="Difference and Repetition" /><link rel="shortcut icon" type="image/x-icon" href="/variation/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/variation/">Difference and Repetition</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/variation/about/">About Me</a><a class="page-link" href="/variation/search/">Search</a><a class="page-link" href="/variation/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Build, criticise, and expand time series models with NumPyro</h1><p class="page-description">Bayesian modeling of global trend, local variation, seasonality, and heterogeneity in time series</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-18T00:00:00-05:00" itemprop="datePublished">
        May 18, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      41 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/variation/categories/#Data">Data</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/riversdark/variation/tree/master/_notebooks/2021-05-18-lgts.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/variation/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/riversdark/variation/master?filepath=_notebooks%2F2021-05-18-lgts.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/variation/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/riversdark/variation/blob/master/_notebooks/2021-05-18-lgts.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/variation/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Friversdark%2Fvariation%2Fblob%2Fmaster%2F_notebooks%2F2021-05-18-lgts.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/variation/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#Modeling-global-trend-and-local-variation">Modeling global trend and local variation </a></li>
<li class="toc-entry toc-h2"><a href="#Adding-a-regression-component-to-the-model">Adding a regression component to the model </a></li>
<li class="toc-entry toc-h2"><a href="#Using-a-different-variation-modeling-approach">Using a different variation modeling approach </a></li>
<li class="toc-entry toc-h2"><a href="#Adding-multiplicative-seasonality-to-the-model">Adding multiplicative seasonality to the model </a></li>
<li class="toc-entry toc-h2"><a href="#Using-additive-seasonality">Using additive seasonality </a></li>
<li class="toc-entry toc-h2"><a href="#Calculating-hidden-level-with-moving-average">Calculating hidden level with moving average </a></li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-05-18-lgts.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this post we'll implement a time series framework based on a flexible
smoothed exponential process. The framework can be used to model global
trend, local variation, seasonality and other features that are
essential for flexible time series modeling.</p>
<p>This post also covers several other features under this general
framework, notably:</p>
<ul>
<li>modeling over-dispersed observations with Student-T distribution</li>
<li>explicit and extensive modeling of the variation to capture data
heteroscedasticity</li>
<li>adding regression components when necessary</li>
</ul>
<p>Some other general but important features include:</p>
<ul>
<li>extensive usage of visualisation for model criticism</li>
<li>prior and posterior simulation for model inspection</li>
<li>and a unified prediction framework.</li>
</ul>
<p>Last but maybe most importantly, apart from the specific features and
techniques, the post gives a general procedure for Bayesian model
building and criticism, which should be useful for building any kind of
model for any kind of data.</p>
<p>This is mostly a port of the R package <a href="https://cran.r-project.org/web/packages/Rlgt/index.html">Rlgt: Bayesian Exponential Smoothing Models with Trend Modifications</a>, although the implementation details might differ here and there. One of the models has already been <a href="http://num.pyro.ai/en/stable/tutorials/time_series_forecasting.html">ported to Numpyro</a> in one of the Numpyro tutorials, so that specific model is not covered here.</p>
<p>Different options for modeling trends and seasonalities have been
considered in this post, to understand their varying capabilities and
characteristics, and to prepare an arsenal for building bespoke models
for future data sets with different characteristics.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install numpyro<span class="o">[</span>cpu<span class="o">]</span>
<span class="o">!</span>pip install rdatasets
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">palette</span><span class="o">=</span><span class="s1">'Set2'</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span><span class="p">,</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">jax.config</span> <span class="kn">import</span> <span class="n">config</span><span class="p">;</span> <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">"jax_enable_x64"</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">jnp</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpyro</span>
<span class="n">numpyro</span><span class="o">.</span><span class="n">set_host_device_count</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpyro.distributions</span> <span class="k">as</span> <span class="nn">dist</span>
<span class="kn">from</span> <span class="nn">numpyro.contrib.control_flow</span> <span class="kn">import</span> <span class="n">scan</span>
<span class="kn">from</span> <span class="nn">numpyro.diagnostics</span> <span class="kn">import</span> <span class="n">autocorrelation</span><span class="p">,</span> <span class="n">hpdi</span>
<span class="kn">from</span> <span class="nn">numpyro.infer</span> <span class="kn">import</span> <span class="n">MCMC</span><span class="p">,</span> <span class="n">NUTS</span><span class="p">,</span> <span class="n">Predictive</span>
<span class="kn">from</span> <span class="nn">numpyro</span> <span class="kn">import</span> <span class="n">deterministic</span><span class="p">,</span> <span class="n">sample</span>
<span class="kn">from</span> <span class="nn">numpyro.handlers</span> <span class="kn">import</span> <span class="n">seed</span><span class="p">,</span> <span class="n">condition</span><span class="p">,</span> <span class="n">plate</span>

<span class="kn">from</span> <span class="nn">rdatasets</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">descr</span><span class="p">,</span> <span class="n">summary</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are going to use two data sets, the BJ sales data and the monthly
airline passenger data set. For the BJ sales data set we have an extra
lead indicator, which can be used to build a regression component to the
model.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bj</span> <span class="o">=</span> <span class="n">data</span><span class="p">(</span><span class="s2">"BJsales"</span><span class="p">)[</span><span class="s1">'value'</span><span class="p">]</span>
<span class="n">air_passengers</span> <span class="o">=</span> <span class="n">data</span><span class="p">(</span><span class="s2">"AirPassengers"</span><span class="p">)[</span><span class="s1">'value'</span><span class="p">]</span>
<span class="n">bj_lead</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[</span><span class="mf">10.01</span><span class="p">,</span><span class="mf">10.07</span><span class="p">,</span><span class="mf">10.32</span><span class="p">,</span> <span class="mf">9.75</span><span class="p">,</span><span class="mf">10.33</span><span class="p">,</span><span class="mf">10.13</span><span class="p">,</span><span class="mf">10.36</span><span class="p">,</span><span class="mf">10.32</span><span class="p">,</span><span class="mf">10.13</span><span class="p">,</span><span class="mf">10.16</span><span class="p">,</span><span class="mf">10.58</span><span class="p">,</span><span class="mf">10.62</span><span class="p">,</span> <span class="mf">10.86</span><span class="p">,</span><span class="mf">11.20</span><span class="p">,</span><span class="mf">10.74</span><span class="p">,</span><span class="mf">10.56</span><span class="p">,</span><span class="mf">10.48</span><span class="p">,</span><span class="mf">10.77</span><span class="p">,</span><span class="mf">11.33</span><span class="p">,</span><span class="mf">10.96</span><span class="p">,</span><span class="mf">11.16</span><span class="p">,</span><span class="mf">11.70</span><span class="p">,</span><span class="mf">11.39</span><span class="p">,</span><span class="mf">11.42</span><span class="p">,</span> <span class="mf">11.94</span><span class="p">,</span><span class="mf">11.24</span><span class="p">,</span><span class="mf">11.59</span><span class="p">,</span><span class="mf">10.96</span><span class="p">,</span><span class="mf">11.40</span><span class="p">,</span><span class="mf">11.02</span><span class="p">,</span><span class="mf">11.01</span><span class="p">,</span><span class="mf">11.23</span><span class="p">,</span><span class="mf">11.33</span><span class="p">,</span><span class="mf">10.83</span><span class="p">,</span><span class="mf">10.84</span><span class="p">,</span><span class="mf">11.14</span><span class="p">,</span> <span class="mf">10.38</span><span class="p">,</span><span class="mf">10.90</span><span class="p">,</span><span class="mf">11.05</span><span class="p">,</span><span class="mf">11.11</span><span class="p">,</span><span class="mf">11.01</span><span class="p">,</span><span class="mf">11.22</span><span class="p">,</span><span class="mf">11.21</span><span class="p">,</span><span class="mf">11.91</span><span class="p">,</span><span class="mf">11.69</span><span class="p">,</span><span class="mf">10.93</span><span class="p">,</span><span class="mf">10.99</span><span class="p">,</span><span class="mf">11.01</span><span class="p">,</span> <span class="mf">10.84</span><span class="p">,</span><span class="mf">10.76</span><span class="p">,</span><span class="mf">10.77</span><span class="p">,</span><span class="mf">10.88</span><span class="p">,</span><span class="mf">10.49</span><span class="p">,</span><span class="mf">10.50</span><span class="p">,</span><span class="mf">11.00</span><span class="p">,</span><span class="mf">10.98</span><span class="p">,</span><span class="mf">10.61</span><span class="p">,</span><span class="mf">10.48</span><span class="p">,</span><span class="mf">10.53</span><span class="p">,</span><span class="mf">11.07</span><span class="p">,</span> <span class="mf">10.61</span><span class="p">,</span><span class="mf">10.86</span><span class="p">,</span><span class="mf">10.34</span><span class="p">,</span><span class="mf">10.78</span><span class="p">,</span><span class="mf">10.80</span><span class="p">,</span><span class="mf">10.33</span><span class="p">,</span><span class="mf">10.44</span><span class="p">,</span><span class="mf">10.50</span><span class="p">,</span><span class="mf">10.75</span><span class="p">,</span><span class="mf">10.40</span><span class="p">,</span><span class="mf">10.40</span><span class="p">,</span><span class="mf">10.34</span><span class="p">,</span> <span class="mf">10.55</span><span class="p">,</span><span class="mf">10.46</span><span class="p">,</span><span class="mf">10.82</span><span class="p">,</span><span class="mf">10.91</span><span class="p">,</span><span class="mf">10.87</span><span class="p">,</span><span class="mf">10.67</span><span class="p">,</span><span class="mf">11.11</span><span class="p">,</span><span class="mf">10.88</span><span class="p">,</span><span class="mf">11.28</span><span class="p">,</span><span class="mf">11.27</span><span class="p">,</span><span class="mf">11.44</span><span class="p">,</span><span class="mf">11.52</span><span class="p">,</span> <span class="mf">12.10</span><span class="p">,</span><span class="mf">11.83</span><span class="p">,</span><span class="mf">12.62</span><span class="p">,</span><span class="mf">12.41</span><span class="p">,</span><span class="mf">12.43</span><span class="p">,</span><span class="mf">12.73</span><span class="p">,</span><span class="mf">13.01</span><span class="p">,</span><span class="mf">12.74</span><span class="p">,</span><span class="mf">12.73</span><span class="p">,</span><span class="mf">12.76</span><span class="p">,</span><span class="mf">12.92</span><span class="p">,</span><span class="mf">12.64</span><span class="p">,</span> <span class="mf">12.79</span><span class="p">,</span><span class="mf">13.05</span><span class="p">,</span><span class="mf">12.69</span><span class="p">,</span><span class="mf">13.01</span><span class="p">,</span><span class="mf">12.90</span><span class="p">,</span><span class="mf">13.12</span><span class="p">,</span><span class="mf">12.47</span><span class="p">,</span><span class="mf">12.47</span><span class="p">,</span><span class="mf">12.94</span><span class="p">,</span><span class="mf">13.10</span><span class="p">,</span><span class="mf">12.91</span><span class="p">,</span><span class="mf">13.39</span><span class="p">,</span> <span class="mf">13.13</span><span class="p">,</span><span class="mf">13.34</span><span class="p">,</span><span class="mf">13.34</span><span class="p">,</span><span class="mf">13.14</span><span class="p">,</span><span class="mf">13.49</span><span class="p">,</span><span class="mf">13.87</span><span class="p">,</span><span class="mf">13.39</span><span class="p">,</span><span class="mf">13.59</span><span class="p">,</span><span class="mf">13.27</span><span class="p">,</span><span class="mf">13.70</span><span class="p">,</span><span class="mf">13.20</span><span class="p">,</span><span class="mf">13.32</span><span class="p">,</span> <span class="mf">13.15</span><span class="p">,</span><span class="mf">13.30</span><span class="p">,</span><span class="mf">12.94</span><span class="p">,</span><span class="mf">13.29</span><span class="p">,</span><span class="mf">13.26</span><span class="p">,</span><span class="mf">13.08</span><span class="p">,</span><span class="mf">13.24</span><span class="p">,</span><span class="mf">13.31</span><span class="p">,</span><span class="mf">13.52</span><span class="p">,</span><span class="mf">13.02</span><span class="p">,</span><span class="mf">13.25</span><span class="p">,</span><span class="mf">13.12</span><span class="p">,</span> <span class="mf">13.26</span><span class="p">,</span><span class="mf">13.11</span><span class="p">,</span><span class="mf">13.30</span><span class="p">,</span><span class="mf">13.06</span><span class="p">,</span><span class="mf">13.32</span><span class="p">,</span><span class="mf">13.10</span><span class="p">,</span><span class="mf">13.27</span><span class="p">,</span><span class="mf">13.64</span><span class="p">,</span><span class="mf">13.58</span><span class="p">,</span><span class="mf">13.87</span><span class="p">,</span><span class="mf">13.53</span><span class="p">,</span><span class="mf">13.41</span><span class="p">,</span> <span class="mf">13.25</span><span class="p">,</span><span class="mf">13.50</span><span class="p">,</span><span class="mf">13.58</span><span class="p">,</span><span class="mf">13.51</span><span class="p">,</span><span class="mf">13.77</span><span class="p">,</span><span class="mf">13.40</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>These data sets have different characteristics and as such will demand
different modeling considerations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Modeling-global-trend-and-local-variation">
<a class="anchor" href="#Modeling-global-trend-and-local-variation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modeling global trend and local variation<a class="anchor-link" href="#Modeling-global-trend-and-local-variation"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The data used for this model is the
<a href="https://vincentarelbundock.github.io/Rdatasets/doc/datasets/BJsales.html">BJsales</a>
data set, which contains 150 sales observations. The data are bounded by
zero, as can be expected for sales data, and range between 200 and 300,
and there is very little variation between neighbouring points.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lgt_y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bj</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bj</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'BJ sales data'</span><span class="p">);</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A" width="379" height="266">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The first model we are building can be described as</p>
\begin{align*}
y_{t} &amp;\sim \text{T} (\nu,\mu_{t}, \sigma_{t}) \\
\sigma_{t} &amp;= \kappa \mu_{t}^{ \tau} + \xi   \\
\mu_{t} &amp;= \text{G}_{t} + \text{L}_{t} \\
\text{G}_{t} &amp;= g_{t-1} + \gamma g_{t-1}^{ \rho } \\
\text{L}_{t} &amp;= \lambda l_{t-1}  \\
g_{t} &amp;= \alpha y_{t} + ( 1- \alpha ) \text{G}_{t} \\
l_{t} &amp;= \beta  \left( g_{t}-g_{t-1} \right) + \left( 1- \beta  \right) l_{t-1}\\
\end{align*}
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This first model is already quite complex so let's spend a moment to digest it. We can visualise the model as follows</p>
<p><img src="https://github.com/riversdark/variation/blob/master/_notebooks/lgts0.jpeg?raw=1" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The variable $y$ in the above graph is shaded because it's the observed variable. One thing to keep in mind with Bayesian networks, is that they form directed acyclical graphs, that is, there shouldn't be circles anywhere in the model. We can check that this is indeed the case with the above model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To model the outcome we'll use the Student's T distribution, which has
three parameters: the degree of freedom $\nu$, the expected value $\mu$,
and the standard deviation $\sigma$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The degree of freedom determines how heavy tailed the distribution will
be: when the degree of freedom is one, the T distribution becomes the Cauchy
distribution, which is so heavy tailed that the mean and standard
deviation can not even be properly defined; when the degree of freedom
approaches infinity the distribution approaches Normal, with quite narrow
tails. Using the T distribution, we can account for heavy tail data when
the data are indeed heavy tailed, but it's also flexible enough for when
they are not. Since Ã  priori we don't have much information on the
degree of freedom, we'll put a wide prior on it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The expected value is where we'll put most of our modeling focus on. In
this first model we'll consider modeling it as the sum of a global trend
$\text{G}$ and a local variation $\text{L}$, with the global trend being
modeled as a smoothed exponential function of the hidden level $g$, and
the local variation as a dampened baseline variation $l$. Furthermore,
both the hidden level $g$ and the variation $l$ follow a smoothed
autoregressive process. For hidden level $g$, it's a weighted average
between the outcome, and the previous global
trend. For the baseline variation $l$ it's a weighted average between
the increment in hidden levels, and the previous baseline variation.
$\alpha$ and $\beta$ are the corresponding smoothing weights.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The standard deviation is modeled with another smoothed exponential
process of the expected value, so this is a heterogeneous model, with the
variance varying alongside the expected value.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Some of the modeling assumptions might look a bit arbitrary; and they indeed are. There are a thousand choices we can make about how each piece of the model is formulated, and sometimes we have to try different alternatives to see which works best for our data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because both the standard deviation $\sigma$ and the expected value
$\mu$ follow a smoothed exponential process, we have to guarantee the
expected value and the hidden level $g$ be always positive. This
assumption is reasonable when the data is also positive, as is the case
here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It's important to note that the models we are building might be too
complex for the data we are using. Here by exploratory analysis we can
see that the BJ sales data have a global trend, and there are clearly
local variations, but there doesn't seem to be many sudden changes in
the series, and as such the smoothed exponential process for the
standard deviation might not be strictly necessary. Still, we use a more
complex model to account for the <strong>potential</strong> heterogeneities in the
data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can chose some proper priors for the model parameters and code
the model in NumPyro. Of course, choosing priors is an integral part of
Bayesian modeling and it is no easy task, we need to consider our
prior knowledge carefully and gradually update that in model criticism.
The priors chosen for this model implementation take into account both
the model structure, and the general knowledge we have gained from
exploratory data analysis. In Bayesian analysis each model should be a
bespoke model for the specific data set we are modeling.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A closer look at the above graph and the model definition, we notice that for all variables at all time periods, their distributions are parameterised by other variables, with the exceptions of two variables: the first $g$ and the first $\ell$. So our model isn't complete: we need also choose the prior distributions for these two variables.</p>
<p>We can remind ourselves them $g$ and $\ell$ are the hidden global and local levels, respectively; and $g$ is defined as a weight average of the observed outcome value and the global trend; for these reasons it's not unreasonable to define the prior mean of $g$ to be the first observed outcome value. As for $\ell$, since it's the hidden level of the local variation, a prior centered at zero seems also reasonable.</p>
<p>And because we've used the first observation to
initialise the model, we'll model the observed data starting at the second.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">lgt</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

    <span class="n">xi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"xi"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"tau"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"kappa"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">lbda</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"lbda"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">l_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"l_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"L"</span><span class="p">,</span> <span class="n">lbda</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="n">L</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">xi</span> <span class="o">+</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">**</span> <span class="n">tau</span><span class="p">)</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">yt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">G</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"l"</span><span class="p">,</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">g_new</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">l_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">l_init</span><span class="p">),</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="n">future</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgt</span><span class="p">(</span><span class="n">lgt_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">future</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgt</span><span class="p">(</span><span class="n">lgt_y</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Observed data:
 [200.1 199.5 199.4 198.9 199. ]

Prior sampling:
[200.1  200.36 198.58 200.16 200.29]

Fixed data sampling:
[200.1 199.5 199.4 198.9 199. ]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the model specified we can draw prior samples from it. We can
generate prior samples without the observed data, because our model is
completely generative. We also write a simple helper function
<code>check_prior</code> to collect model variables and print out summary
statistics. The model variables are separated into two groups: global
variables, which determine the behaviour of the whole model; and local
variables which are specific to each observed outcome.</p>
<div class="highlight"><pre><span></span><span class="n">lgt_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="mi">139</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">check_prior</span><span class="p">(</span><span class="n">prior</span><span class="p">):</span>
    <span class="sd">"""Print out prior shapes,</span>
<span class="sd">    collect local and global variables into lists."""</span>
    <span class="n">lvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">gvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y_length</span> <span class="o">=</span> <span class="n">prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">prior</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">gvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">y_length</span><span class="p">:</span>
            <span class="n">lvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">"Global variables:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">gvs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">prior</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">Local variables:"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">lvs</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">prior</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">lvs</span><span class="p">,</span> <span class="n">gvs</span>

<span class="n">lgt_lvs</span><span class="p">,</span> <span class="n">lgt_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">)</span>
</pre></div>
<p>It's important to check the model prior predictions to make sure the
model indeed has the desired properties we intended for. We can use a
parallel plot in which each sample will be plotted as one single line
across different coordinates. While doing so, we'll also differentiate
the normal samples, that is that intended samples of our model, from
samples with extremely large outcomes, and also from samples with
numerical overflows. The samples with numerical overflow will be in red,
while those with extreme outcomes will be in blue.</p>
<p>Here we first write another helper function <code>concat_arrays</code> to
concatenate one or two dimensional arrays stored in a dict, as is the
case with our prior and posterior samples. And if the variable is
multivariate, we also break down its dimensions into separate variables.
This is to facilitate later visualisation.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">concat_arrays</span><span class="p">(</span><span class="n">dct</span><span class="p">,</span> <span class="n">vs</span><span class="p">):</span>
    <span class="n">nvs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vs</span><span class="p">:</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">dct</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">vplus</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
            <span class="n">nvs</span> <span class="o">+=</span> <span class="n">vplus</span>
            <span class="n">ds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">nvs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
            <span class="n">ds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span>

    <span class="n">ds</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">nvs</span><span class="p">,</span> <span class="n">ds</span>


<span class="n">nvs</span><span class="p">,</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">concat_arrays</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="p">[</span><span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'rho'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nvs</span><span class="p">),</span> <span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 1+1=2, (500, 2)</span>

<span class="n">nvs</span><span class="p">,</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">concat_arrays</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="p">[</span><span class="s1">'g'</span><span class="p">,</span> <span class="s1">'l'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nvs</span><span class="p">),</span> <span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 140+140=280, (500, 280)</span>

<span class="n">nvs</span><span class="p">,</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">concat_arrays</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="p">[</span><span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'l'</span><span class="p">,</span> <span class="s1">'rho'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nvs</span><span class="p">),</span> <span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># 1+140+1=142, (500, 142)</span>
</pre></div>
<p>And plot the global parameters of the model</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_global_params</span><span class="p">(</span><span class="n">dct</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># TODO add custom labels to different groups</span>

    <span class="n">nans</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">dct</span><span class="p">[</span><span class="n">v_ref</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">extremes</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dct</span><span class="p">[</span><span class="n">v_ref</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">val_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Number of samples with numerical overflows: '</span><span class="p">,</span> <span class="n">nans</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Number of samples with extreme values : '</span><span class="p">,</span> <span class="n">extremes</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">nvs</span><span class="p">,</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">concat_arrays</span><span class="p">(</span><span class="n">dct</span><span class="p">,</span> <span class="n">vs</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">nans</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">extremes</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">nans</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">extremes</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">nans</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">nvs</span><span class="p">)))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nvs</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>

<span class="n">compare_global_params</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="n">lgt_gvs</span><span class="p">)</span>
</pre></div>
<p>Because different variables have different scales, to get a better idea
of some certain variables we can limit the list of variables to be
plotted:</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="p">[</span><span class="s1">'rho'</span><span class="p">,</span> <span class="s1">'tau'</span><span class="p">,</span> <span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'kappa'</span><span class="p">,</span> <span class="s1">'lbda'</span><span class="p">])</span>
</pre></div>
<p>Even though we have consciously limited the prior distribution for
$\rho$ to favour smaller values, when its values are still relatively
large, we're likely to see samples with extreme outcomes. This is
understandable because $\rho$ decides how fast the exponential grows.</p>
<p>We can also plot the distribution of one single variable of interest:</p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">[</span><span class="s1">'rho'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">'probability'</span><span class="p">);</span>
</pre></div>
<p>This corresponds to the samples at coordinate <code>rho</code> in the previous
plot. A priori the expected value of $\rho$ is 0.2, but as we can see,
that are many samples will much bigger values. We can try to change the
priors for these parameters to limit their behavior, but the problem
here doesn't seem very serious so we'll leave them be for now.</p>
<p>We now plot the model outcome, and other local variables to check the
implications of the model priors.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_locals</span><span class="p">(</span><span class="n">dct</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'y'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="o">**</span><span class="n">plt_kws</span><span class="p">):</span>

    <span class="n">extremes</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">dct</span><span class="p">[</span><span class="n">v_ref</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">val_ref</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Number of samples with extreme values : '</span><span class="p">,</span> <span class="n">extremes</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="o">**</span><span class="n">plt_kws</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vs</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">dct</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">extremes</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
                <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">lgt_prior</span><span class="p">,</span> <span class="n">lgt_lvs</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>We can see that the outcome generally follow the smoothed exponential
process, with some cases the outcome growing faster than other cases.
Also notice that the local variation is roughly one to two orders of
magnitudes smaller than the global trend. This is intended because we
want to to able to properly identify them.</p>
<p>We can also generate prior samples conditioned on the observed data.
Since time series models are Markovian, i.e. path dependent, the
observed data will serve to limit some of the model parameters that
depend on the outcome to a region in the ambient space that is
compatible with the observed data.</p>
<div class="highlight"><pre><span></span><span class="n">lgt_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">[:</span><span class="mi">140</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">lgt_prior_fixed</span><span class="p">,</span> <span class="n">lgt_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'mu'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>This might be useful for understanding the general interactions between
the model and the data, and also for comparison with the posterior, to
see how much we have learned.</p>
<p>To test that our inference engine and the model works properly, We can
then chose one of our prior samples, feed it to our inference engine to
see if we can recover the parameters used to generate them. It's
important to realise that we are not always able to recover the
parameters, because as we have see in the prior check, there are some
highly irregular samples.</p>
<p>First chose one sample</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lgt_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>The sample seems ordinary enough, not one of the extreme ones. We then
run inference on it and compare the posterior samples with the real
parameter value that we used to generate the outcome.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgt</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">refs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vs</span><span class="p">:</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">prior</span><span class="p">[</span><span class="n">v</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">refs</span> <span class="o">+=</span> <span class="nb">list</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="n">d</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_numpyro</span><span class="p">(</span><span class="n">mcmc</span><span class="p">)</span>
    <span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="n">vs</span><span class="p">,</span> <span class="n">ref_val</span><span class="o">=</span><span class="n">refs</span><span class="p">,</span>
                      <span class="n">point_estimate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hdi_prob</span><span class="o">=</span><span class="s1">'hide'</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">));</span>

<span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">lgt_prior</span><span class="p">,</span> <span class="n">lgt_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
<p>The recovered parameters are well in the range of possibilities, but
there are large uncertainties around them. This is to be expected: when
building complex models, there is only so much we can learn from the
limited data. We move on to the inference on real data.</p>
<div class="highlight"><pre><span></span><span class="n">lgt_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgt</span><span class="p">,</span> <span class="n">target_accept_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">lgt_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">lgt_kernel</span><span class="p">,</span>
                <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lgt_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">lgt_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Now we can make predictions for future periods.</p>
<div class="highlight"><pre><span></span><span class="n">lgt_sample</span> <span class="o">=</span> <span class="n">lgt_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">lgt_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgt</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">lgt_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">[:</span><span class="mi">140</span><span class="p">],</span> <span class="mi">10</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgt_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgt_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>And plot the predictions. We'll plot the mean, along with 5th, 25th,
75th, and 95th prediction distribution percentiles.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_post</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">vs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrow</span><span class="p">,</span> <span class="n">ncol</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">v</span> <span class="o">==</span> <span class="s1">'y'</span><span class="p">:</span>
            <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>

        <span class="n">mean</span> <span class="o">=</span> <span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">p5</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">],</span> <span class="mi">5</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">p25</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">],</span> <span class="mi">25</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">p75</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">],</span> <span class="mi">75</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">p95</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">],</span> <span class="mi">95</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">post</span><span class="p">[</span><span class="n">v</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)),</span> <span class="n">p5</span><span class="p">,</span> <span class="n">p95</span><span class="p">,</span>
                         <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">)),</span> <span class="n">p25</span><span class="p">,</span> <span class="n">p75</span><span class="p">,</span>
                         <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>

<span class="n">plot_post</span><span class="p">(</span><span class="n">lgt_y</span><span class="p">,</span> <span class="n">lgt_post</span><span class="p">,</span> <span class="n">lgt_lvs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>We can see that the prediction is not that far off: we get the basic
trend right, and the 50% credible interval does cover the observed data.
But still, our model prediction is basically a linear extrapolation, the
model prediction is almost entirely determined by the global trend, and
the local variation in our model contributed little, if anything at all,
in modifying the global behaviour. This motivates us to add some new
information of locality to our model, and this is why we'll add a new
regression component in the next part.</p>
<p>Besides, the posterior of $\sigma$ is very small and almost constant,
this indicates that the smoothed exponential model for the standard
deviation is entirely redundant, we might as well just use a simple
homogeneous formulation.</p>
<p>Apart from visualising the posterior prediction, there are also many
different scores we can check. The first one is the <a href="https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error">symmetric mean
absolute percentage
error</a>,
which is based on the percentage error of the mean prediction. Notice
that the "mean" here refers to <strong>the mean of the point estimate</strong> across
all the observations, and for the point estimate itself, we can use the
mean, the median, or any other point estimate of the posterior.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_smape</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">200</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">truth</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pred</span> <span class="o">+</span> <span class="n">truth</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<p>And the mean absolute error. This time we will use the median of the
posterior prediction to calculate the point estimate.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_mae</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">truth</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<p>And the root mean squared error, using sample mean as point estimate.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_rmse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">truth</span><span class="p">)))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<p>And finally, the <a href="https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf">continuous ranked probability
score</a>.
The previous scores all use point estimates of the posterior, CRPS takes
the whole posterior into consideration.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_crps</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">truth</span><span class="p">):</span>
    <span class="c1"># ref: https://github.com/pyro-ppl/pyro/pull/2045</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">pred</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">pred</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="n">truth</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">crps_empirical</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">pred</span> <span class="o">-</span> <span class="n">truth</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span> \
        <span class="o">-</span> <span class="p">(</span><span class="n">diff</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_samples</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">crps_empirical</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<p>Let's see how well we have done.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_scores</span><span class="p">(</span><span class="n">post</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_test</span><span class="p">):</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">post</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][:,</span> <span class="o">-</span><span class="n">n_test</span><span class="p">:]</span>
    <span class="n">ytest</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="n">n_test</span><span class="p">:]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"sMAPE: </span><span class="si">{:.2f}</span><span class="s2">%, MAE: </span><span class="si">{:.2f}</span><span class="s2">, RMSE: </span><span class="si">{:.2f}</span><span class="s2">, CPRS: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">eval_smape</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">),</span>
        <span class="n">eval_mae</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">),</span>
        <span class="n">eval_rmse</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">),</span>
        <span class="n">eval_crps</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">ytest</span><span class="p">)))</span>

<span class="n">check_scores</span><span class="p">(</span><span class="n">lgt_post</span><span class="p">,</span> <span class="n">lgt_y</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
<p>Of course these scores are only useful when compared with performance
from other models. We can come back and compare them to other models in
the later parts of the post.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adding-a-regression-component-to-the-model">
<a class="anchor" href="#Adding-a-regression-component-to-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding a regression component to the model<a class="anchor-link" href="#Adding-a-regression-component-to-the-model"> </a>
</h2>
<p>As we've seen before, the model prediction from the previous model is
far from satisfactory. Luckily with the <code>BJsales</code> data set we have
another accompanying lead indicator which we can integrate into our
model as a regression component. Also, since the smoothed exponential
process for the standard deviation doesn't contributed much, we'll
remove this model component, and directly put a prior on $\sigma$.</p>
<p>The lead indicator</p>
<div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bj_lead</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'BJ sales lead indicator'</span><span class="p">);</span>
</pre></div>
<p>we can see that the indicator variable follows a similar progression
pattern as that of the sales data, so it should offer us useful
information to improve the model prediction.</p>
<p>We'll use the lead indicator, lagged 3 and 4 periods, as predictors. The
total length of the data, since we have to remove the first 4 data
points for lack of predictors, becomes 146. We'll use the last 10 period
for prediction as before. The regression component of the model will
also have an intercept.</p>
<div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">146</span><span class="p">])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">bj_lead</span><span class="p">[:</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">bj_lead</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
<span class="n">lgtr_x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lgtr_y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bj</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Predictors shape:'</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">Outcome shape:'</span><span class="p">,</span> <span class="n">lgtr_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>We change the expected value of the model so that it now consists of
three components: a global trend, a local variation, and a local
regression component.</p>
<p>Also, with increasing components for the expected value, it's getting
more difficult to assign a proper prior to the hidden level $g$. For
this reason we give the starting level a wider Cauchy prior, and limit
its value to be positive.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lgtr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">lbda</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"lbda"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">plate</span><span class="p">(</span><span class="s2">"D"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"eta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedCauchy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">l_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"l_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">xt</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"L"</span><span class="p">,</span> <span class="n">lbda</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"R"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">eta</span><span class="p">))</span>
        <span class="c1"># LR = deterministic("LR", L+R)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="n">L</span> <span class="o">+</span> <span class="n">R</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">yt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">G</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"l"</span><span class="p">,</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">g_new</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">l_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">l_init</span><span class="p">),</span>  <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:"</span><span class="p">,</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgtr</span><span class="p">(</span><span class="n">lgtr_x</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgtr</span><span class="p">(</span><span class="n">lgtr_x</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
<p>Sample from the prior</p>
<div class="highlight"><pre><span></span><span class="n">lgtr_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtr</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lgtr_lvs</span><span class="p">,</span> <span class="n">lgtr_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">lgtr_prior</span><span class="p">)</span>
</pre></div>
<p>First let's take a look at our truncated Cauchy prior for the initial
hidden level</p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">lgtr_prior</span><span class="p">[</span><span class="s1">'g_init'</span><span class="p">]);</span>
</pre></div>
<p>We can see the bulk of the samples are around 200, but the samples have
quite a wide range, from zero to a few thousands, so it should be
flexible enough for our model.</p>
<p>And check for global parameters</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">lgtr_prior</span><span class="p">,</span> <span class="p">[</span><span class="s1">'eta'</span><span class="p">,</span> <span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'l_init'</span><span class="p">,</span> <span class="s1">'sigma'</span><span class="p">,</span> <span class="p">])</span>
</pre></div>
<p>We don't have numerical overflows, this is good. We do have more samples
with very large outcomes, but this is to be expected, because we are
introducing more model components to the model, and this expands our
modeling space and consequently the outcome range. What matters here, is
that there are no clear patterns to the extreme outcomes, which implies
that the extreme outcomes are the natural result of expanding the model
space, not that of some malfunctionaling model component.</p>
<p>And the prior prediction for local variables</p>
<div class="highlight"><pre><span></span><span class="n">plot_locals</span><span class="p">(</span><span class="n">lgtr_prior</span><span class="p">,</span> <span class="n">lgtr_lvs</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Again, we can clearly see the effect of the smoothed exponential model.
We can compare it with prior predictions conditioning on the data</p>
<div class="highlight"><pre><span></span><span class="n">lgtr_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtr</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">lgtr_prior_fixed</span><span class="p">,</span> <span class="n">lgtr_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'mu'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">nrows</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>The introduction of the regression component makes the relationship
between the hidden level $l$ and the model outcome $y$ less prominent.
This implies that the current model has the <strong>potential</strong> to improve on
the previous model, since the model now has more flexibility introduced
by the R component. However, we should also be aware that if the
regression part does not explain the outcome very well, it can also
potentially reduce the overall predictive power of the model, as it
obscures the previously dominating relationship between the outcome and
the hidden level.</p>
<p>Like before, now we chose one sample from the prior predictive samples
and do parameter recovery.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">14</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lgtr_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>And inference.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgtr</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">lgtr_prior</span><span class="p">,</span> <span class="n">lgtr_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
<p>We have been able to recover most of the parameters. The posterior for
$\nu$ is quite far off, but considering the model and the data used,
this is also understandable: this specific sample hardly has any
fluctuation at all, there is no way we can learn much about the true
degree of freedom.</p>
<p>Next we do inference on the real data.</p>
<div class="highlight"><pre><span></span><span class="n">lgtr_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgtr</span><span class="p">,</span> <span class="n">target_accept_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">lgtr_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">lgtr_kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                 <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lgtr_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>
<span class="n">lgtr_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Now we can make predictions for future periods. If we feed the
<code>Predictive</code> function with more predictors than outcomes, the function
will automatically make prediction for future periods.</p>
<div class="highlight"><pre><span></span><span class="n">lgtr_sample</span> <span class="o">=</span> <span class="n">lgtr_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">lgtr_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtr</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">lgtr_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtr_x</span><span class="p">,</span> <span class="n">lgtr_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgtr_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgtr_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>Let's look at the model prediction.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">lgtr_y</span><span class="p">,</span> <span class="n">lgtr_post</span><span class="p">,</span> <span class="n">lgtr_lvs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>This time the model prediction follows much closer to the local
variations, we have made significant improvement to the model's
predictive power. The reason for this improvement, as we can see from
the above plot, is that we now have two model components, R and L, that
contribute very meaningfully to the prediction. In the previous model,
although the local variation component has been able to capture some
local change, but the prediction for future periods is outright flat,
which means it hardly alters the global trend at all. In this model, on
the contrary, there is some uncertainty around the global trend G all
along the time series, and this uncertainty permits the other model
components, R and L, to make changes to the global trend when the data
demands it, and to consequently change the overall mean and ultimately
the outcome.</p>
<p>We can also check the posterior distribution of $\sigma$:</p>
<div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">lgtr_sample</span><span class="p">[</span><span class="s1">'sigma'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">);</span>
</pre></div>
<p>Indeed the standard deviation is very small, and with little variation.
We can also check the scores.</p>
<div class="highlight"><pre><span></span><span class="n">check_scores</span><span class="p">(</span><span class="n">lgtr_post</span><span class="p">,</span> <span class="n">lgtr_y</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
<p>All the scores have significantly improved (that is, reduced), compared
to the previous model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-a-different-variation-modeling-approach">
<a class="anchor" href="#Using-a-different-variation-modeling-approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using a different variation modeling approach<a class="anchor-link" href="#Using-a-different-variation-modeling-approach"> </a>
</h2>
<p>From the previous two models we can see that with the BJsales data set,
the expected value does not affect the outcome variation very much, so
the exponentially smoothed model is not very helpful.</p>
<p>Here we propose another model for the variation. In this model, the
variation is a linear function of another hidden variable $w$, which
itself follows an autoregressive process as the smoothed average between
its previous value and the current absolute error, just like $l$ and
$g$.</p>
<div class="highlight"><pre><span></span><span class="n">lgtrs_x</span> <span class="o">=</span> <span class="n">lgtr_x</span>
<span class="n">lgtrs_y</span> <span class="o">=</span> <span class="n">lgtr_y</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Predictors shape:'</span><span class="p">,</span> <span class="n">lgtrs_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">Outcome shape:'</span><span class="p">,</span> <span class="n">lgtrs_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>The full model can be written down as:</p>
<p>And we update our model</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lgtrs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

    <span class="n">xi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"xi"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"kappa"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">lbda</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"lbda"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">zeta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"zeta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">plate</span><span class="p">(</span><span class="s2">"D"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"eta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedCauchy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">l_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"l_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">w_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"w_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">xt</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"L"</span><span class="p">,</span> <span class="n">lbda</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"R"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">eta</span><span class="p">))</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="n">L</span> <span class="o">+</span> <span class="n">R</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">xi</span> <span class="o">+</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">yt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">G</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"l"</span><span class="p">,</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">g_new</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">l</span><span class="p">)</span>
        <span class="n">w_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"w"</span><span class="p">,</span> <span class="n">zeta</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">yt</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">zeta</span><span class="p">)</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">l_new</span><span class="p">,</span> <span class="n">w_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">l_init</span><span class="p">,</span> <span class="n">w_init</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:"</span><span class="p">,</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="mi">4</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">(</span><span class="n">lgtrs_x</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">(</span><span class="n">lgtrs_x</span><span class="p">[:</span><span class="mi">4</span><span class="p">],</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>
<p>Sample from the prior model</p>
<div class="highlight"><pre><span></span><span class="n">lgtrs_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtrs_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>

<span class="n">lgtrs_lvs</span><span class="p">,</span> <span class="n">lgtrs_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">lgtrs_prior</span><span class="p">)</span>
</pre></div>
<p>Check global parameters</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">lgtrs_prior</span><span class="p">,</span>
                      <span class="p">[</span><span class="s1">'eta'</span><span class="p">,</span> <span class="s1">'kappa'</span><span class="p">,</span> <span class="s1">'l_init'</span><span class="p">,</span> <span class="s1">'w_init'</span><span class="p">,</span> <span class="s1">'xi'</span><span class="p">],</span>
                      <span class="n">val_ref</span><span class="o">=</span><span class="mi">50000</span><span class="p">)</span>
</pre></div>
<p>And local variables</p>
<div class="highlight"><pre><span></span><span class="n">plot_locals</span><span class="p">(</span><span class="n">lgtrs_prior</span><span class="p">,</span> <span class="n">lgtrs_lvs</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>The outcome range has greatly increased, we now have almost half the
samples having outcomes greater than 50000. This is way bigger than the
range of data we'd expect from the real world. If a store has regular
sales between 200 and 300 per day, we can be sure that that store won't
hold an inventory of 50000. If they do, well, statistical modeling of
their sales figure should be the last of their concern.</p>
<p>Do we need to revise the model before we carry on doing inference? That
depends. If we have some extra information readily available that we can
use to make our priors more precise, that will certainly help. But this
is above all a cost-benefit tradeoff. Does the improvement we bring to
the model worth the effort we are putting in? It's possible that the
information in the data likelihood will completely overwhelm the
information in the prior, and if that is the case the extra effort spent
in improving the priors wouldn't really worth it. Again, what's
important is, Ã  priori, not to limit our model to regions of the ambient
space not covering the observed data. This doesn't seem to be the case
here, so we'll move on.</p>
<p>Prior prediction conditioned on data</p>
<div class="highlight"><pre><span></span><span class="n">lgtrs_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtrs_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">lgtrs_prior_fixed</span><span class="p">,</span> <span class="n">lgtrs_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'mu'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
            <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Like before, now we chose one sample from the prior predictive samples
and do parameter recovery on it.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">74</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">lgtrs_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>This does not look like our observed data at all. And inference.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtrs_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">lgtrs_prior</span><span class="p">,</span> <span class="n">lgtrs_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
<p>Although the data look quite different from the observed, we are still
able to recover the parameters reasonably well, this should give us some
confidence in our model. Next we do inference on the real data.</p>
<div class="highlight"><pre><span></span><span class="n">lgtrs_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">,</span> <span class="n">target_accept_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">lgtrs_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">lgtrs_kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                  <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">lgtrs_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">lgtrs_x</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">],</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>
<span class="n">lgtrs_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Prediction</p>
<div class="highlight"><pre><span></span><span class="n">lgtrs_sample</span> <span class="o">=</span> <span class="n">lgtrs_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">lgtrs_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">lgtrs</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">lgtrs_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">lgtrs_x</span><span class="p">,</span> <span class="n">lgtrs_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgtrs_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">lgtrs_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>and visualisation</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">lgtrs_y</span><span class="p">,</span> <span class="n">lgtrs_post</span><span class="p">,</span> <span class="n">lgtrs_lvs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
<p>This time we indeed have a much more precise estimation of the standard
deviation $\sigma$, we can see that $\sigma$ increases when the time
series has a sudden change of global trend. But it is very small and for
the prediction of future periods, the standard deviation returned to a
flat prediction like in previous models.</p>
<p>Compare the scores.</p>
<div class="highlight"><pre><span></span><span class="n">check_scores</span><span class="p">(</span><span class="n">lgtrs_post</span><span class="p">,</span> <span class="n">lgtrs_y</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
<p>The scores are slightly better than the previous model.</p>
<p>In the last three models, we have used three different methods to model
the outcome variance, and two different methods for the hidden level.
This flexibility is the most important feature of our Bayesian modeling
approach: we take different components capable of capturing different
features, and freely assemble them to fit the need of a specific data
set we want to model.</p>
<p>In the previous models, to capture the local variation, we first used a
Markov process, in which we modeled the local variation as the change in
global trend; then, since we also have some predictors corresponding to
each time period, we also added a regression component. But we didn't
introduce seasonality to the model, because there doesn't seem to be any
such feature present in this specific data set. However, if the data are
indeed seasonal, we can also add new components to address this.</p>
<p>Still, seasonality can be represented in infinitely many different ways,
and in our modeling framework, there remains the question of how
seasonality should be incorporated into it. Should it be an additional
component, like the regression component we have just added, to capture
extra local variation, or should it be an enhancing factor, that impacts
that outcome by changing the global trend? In the next section we first
introduce a multiplicative seasonality, which affects the outcome by
increasing or decreasing the global trend, and in the section that
follows, we'll introduce an additive seasonality as an independent
component.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Adding-multiplicative-seasonality-to-the-model">
<a class="anchor" href="#Adding-multiplicative-seasonality-to-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adding multiplicative seasonality to the model<a class="anchor-link" href="#Adding-multiplicative-seasonality-to-the-model"> </a>
</h2>
<p>For this model we'll use the monthly air Passengers data, which contains
144 observations of passenger counts. We'll use the first 120 points for
modeling and the last 24 for prediction.</p>
<p>Although the passenger counts are clearly integers, the outcome of the
Student's T distribution is real number. Later we might try another
heavy tailed integer valued distribution, such as Negative Binomial, to
better fit the observation.</p>
<div class="highlight"><pre><span></span><span class="n">sgt_y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">air_passengers</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'float64'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">air_passengers</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Monthly Air passengers'</span><span class="p">);</span>
</pre></div>
<p>In this model we'll replace the local trend with a seasonal effect.
Specifically, we'll start with a multiplicative model in this section
and explore the additive one in the next.</p>
<p>This model assumes the number of periods for seasonality is already
known and thus not inferred from the data. This works with datasets with
natural seasonality, which is the case here with monthly data.</p>
<p>Because this is a multiplicative model, when computing the global trend
and seasonality, we have to remove from the outcome the respective
effects of each other, that's why we have the divisions in the smoothed
average process.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgt</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

    <span class="n">xi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"xi"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"tau"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"kappa"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedCauchy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">plate</span><span class="p">(</span><span class="s2">"T"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">T</span><span class="p">):</span>
        <span class="n">s_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"s_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">xi</span> <span class="o">+</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">**</span> <span class="n">tau</span><span class="p">)</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span><span class="o">/</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">G</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
        <span class="n">su</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"su"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span><span class="o">/</span><span class="n">G</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.1</span><span class="p">))</span>
        <span class="n">s_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"s"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">su</span><span class="p">[</span><span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">s_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">s_init</span><span class="p">),</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="n">future</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:"</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgt</span><span class="p">(</span><span class="n">sgt_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">8</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgt</span><span class="p">(</span><span class="n">sgt_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
<p>Sample from prior</p>
<div class="highlight"><pre><span></span><span class="n">sgt_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">119</span><span class="p">)</span>

<span class="n">sgt_lvs</span><span class="p">,</span> <span class="n">sgt_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">sgt_prior</span><span class="p">)</span>
<span class="n">sgt_lvs</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">'s'</span><span class="p">)</span>
</pre></div>
<p>Check global parameters</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">sgt_prior</span><span class="p">,</span>
                      <span class="p">[</span><span class="s1">'alpha'</span><span class="p">,</span> <span class="s1">'beta'</span><span class="p">,</span> <span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'kappa'</span><span class="p">,</span> <span class="s1">'rho'</span><span class="p">,</span> <span class="s1">'tau'</span><span class="p">],</span>
                      <span class="n">val_ref</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
<p>And local variables</p>
<div class="highlight"><pre><span></span><span class="n">plot_locals</span><span class="p">(</span><span class="n">sgt_prior</span><span class="p">,</span> <span class="n">sgt_lvs</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Prior prediction conditioned on data</p>
<div class="highlight"><pre><span></span><span class="n">sgt_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgt</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">sgt_prior_fixed</span><span class="p">,</span> <span class="n">sgt_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'sigma'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Like before, now we chose one sample from the prior predictive samples
and do parameter recovery on it.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">34</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sgt_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>And inference.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgt</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">sgt_prior</span><span class="p">,</span> <span class="n">sgt_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
<p>This time, let's also look at the prediction.</p>
<div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgt</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">samples</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>And visualisation.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">sgt_lvs</span><span class="p">)</span>
</pre></div>
<p>This looks passable enough. Next we do inference on the real data.</p>
<div class="highlight"><pre><span></span><span class="n">sgt_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgt</span><span class="p">)</span>
<span class="n">sgt_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">sgt_kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sgt_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">sgt_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Now we can make predictions for future periods.</p>
<div class="highlight"><pre><span></span><span class="n">sgt_sample</span> <span class="o">=</span> <span class="n">sgt_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">sgt_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgt</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">sgt_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgt_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgt_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>Let's look at the model prediction.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">sgt_y</span><span class="p">,</span> <span class="n">sgt_post</span><span class="p">,</span> <span class="n">sgt_lvs</span><span class="p">)</span>
</pre></div>
<p>The model has been able to capture the growing global trend, and the
seasonality present in it. However, looking at the prediction for future
periods, it looks clear that the model has been systematically
underestimating the outcome.</p>
<p>We have saw similar problem before, in the first LGT model. Back then,
the model is also underestimating, and because we have another predictor
variable, we have been able to use it to compensate for some extra local
information. With this data set, however, we don't have any predictor
available.</p>
<p>One way we can get around this, is to simply use lagged outcomes to
construct a predictor matrix, and add a regression component to the
model as before. However, there is another way to make use of local
information, and that is using a moving window of outcomes. In a later
model, we'll use this moving window information to compute the global
trend. But first, we'll return to, as promised, an additive approach to
modeling seasonality.</p>
<p>Finally, look at the scores.</p>
<div class="highlight"><pre><span></span><span class="n">check_scores</span><span class="p">(</span><span class="n">sgt_post</span><span class="p">,</span> <span class="n">sgt_y</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-additive-seasonality">
<a class="anchor" href="#Using-additive-seasonality" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using additive seasonality<a class="anchor-link" href="#Using-additive-seasonality"> </a>
</h2>
<p>Next we replace the multiplicative seasonality with an additive one. For
the multiplicative seasonality, we assume it affects the outcome by
enhancing or decreasing the global trend, an amplifying factor, so it
naturally only takes positive values. But for the additive seasonality,
we assume it adds or deducts from the global trend, so it can be both
positive and negative.</p>
<div class="highlight"><pre><span></span><span class="n">sgta_y</span> <span class="o">=</span> <span class="n">sgt_y</span>
</pre></div>
<p>The model can be represented as:</p>
<p>We'll put a relatively wide prior on the seasonality effect. And because
updating the seasonality effect needs the outcome before, we need to
keep at least T periods of outcomes in memory, when doing the update.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgta</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

    <span class="n">xi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"xi"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"tau"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"kappa"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedCauchy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">plate</span><span class="p">(</span><span class="s2">"T"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">T</span><span class="p">):</span>
        <span class="n">s_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"s_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">xi</span> <span class="o">+</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">**</span> <span class="n">tau</span><span class="p">)</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span><span class="o">-</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
        <span class="n">su</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"su"</span><span class="p">,</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span><span class="o">-</span><span class="n">G</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">s_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"s"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">su</span><span class="p">[</span><span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">s_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">s_init</span><span class="p">),</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="n">future</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:"</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgta</span><span class="p">(</span><span class="n">sgta_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">8</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgta</span><span class="p">(</span><span class="n">sgta_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
<p>Sample from prior</p>
<div class="highlight"><pre><span></span><span class="n">sgta_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgta</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">119</span><span class="p">)</span>

<span class="n">sgta_lvs</span><span class="p">,</span> <span class="n">sgta_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">sgta_prior</span><span class="p">)</span>
<span class="n">sgta_lvs</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">'s'</span><span class="p">)</span>
</pre></div>
<p>Check global parameters</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">sgta_prior</span><span class="p">,</span>
                      <span class="p">[</span><span class="s1">'alpha'</span><span class="p">,</span> <span class="s1">'beta'</span><span class="p">,</span> <span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'kappa'</span><span class="p">,</span> <span class="s1">'rho'</span><span class="p">,</span> <span class="s1">'tau'</span><span class="p">],</span>
                      <span class="n">val_ref</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
<p>Nothing special. And local variables</p>
<div class="highlight"><pre><span></span><span class="n">plot_locals</span><span class="p">(</span><span class="n">sgta_prior</span><span class="p">,</span> <span class="n">sgta_lvs</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Prior prediction conditioned on data</p>
<div class="highlight"><pre><span></span><span class="n">sgta_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgta</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">sgta_prior_fixed</span><span class="p">,</span> <span class="n">sgta_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'mu'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>This prior predictive distribution, conditioned on data, looks a bit
problematic because as we can see, the seasonality is still present in
the global trend, which defeats our intention of separating the two.
Later we may want to find better updating mechanisms for them.</p>
<p>Like before, now we chose one sample from the prior predictive samples
and do parameter recovery on it.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">24</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sgta_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>And inference.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgta</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">sgta_prior</span><span class="p">,</span> <span class="n">sgta_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
<p>Seems passable, though certainly not very precise, and there are a few
initial seasonality values a bit far from true value. Let's also look at
the prediction.</p>
<div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgta</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">samples</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>And visualisation.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">sgta_lvs</span><span class="p">)</span>
</pre></div>
<p>The prediction looks alright but there is a visible increasing trend in
the seasonality component, this again confirms the problem we've spotted
in the prior predictive check, that in this model the global trend and
seasonality are not completely separated. Next we do inference on the
real data.</p>
<p>Next we do inference on the real data.</p>
<div class="highlight"><pre><span></span><span class="n">sgta_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgta</span><span class="p">,</span> <span class="n">target_accept_prob</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">sgta_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">sgta_kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sgta_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">sgta_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>And predictions.</p>
<div class="highlight"><pre><span></span><span class="n">sgta_sample</span> <span class="o">=</span> <span class="n">sgta_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">sgta_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgta</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">sgta_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgta_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgta_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>Let's look at the model prediction.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">sgta_y</span><span class="p">,</span> <span class="n">sgta_post</span><span class="p">,</span> <span class="n">sgta_lvs</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>Compared to the previous multiplicative seasonality, although the
predictions for the individual components are quite different, but the
final predictions for the outcome are quite similar. And as a model
defect, we can see that both in this and the previous model, we are
slightly underestimating the outcome.</p>
<p>Let's also check the scores.</p>
<div class="highlight"><pre><span></span><span class="n">check_scores</span><span class="p">(</span><span class="n">sgta_post</span><span class="p">,</span> <span class="n">sgta_y</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
</pre></div>
<p>The scores are also similar to the previous model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Calculating-hidden-level-with-moving-average">
<a class="anchor" href="#Calculating-hidden-level-with-moving-average" aria-hidden="true"><span class="octicon octicon-link"></span></a>Calculating hidden level with moving average<a class="anchor-link" href="#Calculating-hidden-level-with-moving-average"> </a>
</h2>
<p>There are, of course, infinite ways to model the hidden level in the
time series. Up to this point we have been using the current outcome to
calculate a weighted average, but we can also use a moving window of
past outcomes to do the calculation. In the previous models the outcome
seems to be underestimated, so we also have to improve a little bit on
this issue, by using more local information. Because the model is
seasonal, the natural choice for window width is the seasonality.</p>
<div class="highlight"><pre><span></span><span class="n">sgtm_y</span> <span class="o">=</span> <span class="n">sgt_y</span>
</pre></div>
<p>We'll continue to use the additive seasonality so the only change lies
in how we update the hidden level:</p>
<p>The moving average is computed with the outcome of the previous T
periods, but there are less than T previous outcomes for the first T
periods, so for these first T periods we'll calculate the moving average
with all the outcomes available. When coding the model, it's easier to
keep track of the moving window rather than the moving average, and
that's how we are going to code our model.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sgtm</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"nu"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

    <span class="n">xi</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"xi"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"tau"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">kappa</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"kappa"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"gamma"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"rho"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"alpha"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"beta"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">g_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"g_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">TruncatedCauchy</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">with</span> <span class="n">plate</span><span class="p">(</span><span class="s2">"T"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">T</span><span class="p">):</span>
        <span class="n">s_init</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"s_init"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">Cauchy</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transition_fn</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">g</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">mw</span> <span class="o">=</span> <span class="n">carry</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="n">g</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">g</span> <span class="o">**</span> <span class="n">rho</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"mu"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"sigma"</span><span class="p">,</span> <span class="n">xi</span> <span class="o">+</span> <span class="n">kappa</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">**</span> <span class="n">tau</span><span class="p">)</span>

        <span class="n">yt</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>

        <span class="n">mw_new</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">mw</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">yt</span><span class="p">[</span><span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">ma</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">t</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span> <span class="n">mw_new</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">mw_new</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="n">T</span><span class="p">)</span>
        <span class="n">g_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">ma</span><span class="o">-</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">G</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="n">su</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"su"</span><span class="p">,</span> <span class="n">beta</span> <span class="o">*</span> <span class="p">(</span><span class="n">yt</span><span class="o">-</span><span class="n">G</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">s_new</span> <span class="o">=</span> <span class="n">deterministic</span><span class="p">(</span><span class="s2">"s"</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">su</span><span class="p">[</span><span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">g_new</span><span class="p">,</span> <span class="n">s_new</span><span class="p">,</span> <span class="n">mw_new</span><span class="p">),</span> <span class="n">yt</span>

    <span class="k">with</span> <span class="n">condition</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s1">'y'</span><span class="p">:</span><span class="n">y</span><span class="p">}):</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">transition_fn</span><span class="p">,</span> <span class="p">(</span><span class="n">g_init</span><span class="p">,</span> <span class="n">s_init</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)),</span>  <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">+</span><span class="n">future</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">ys</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Observed data:"</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">])</span>

<span class="k">with</span> <span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Prior sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgtm</span><span class="p">(</span><span class="n">sgtm_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">8</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Fixed data sampling:"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">sgtm</span><span class="p">(</span><span class="n">sgtm_y</span><span class="p">[:</span><span class="mi">9</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
<p>Sample from prior</p>
<div class="highlight"><pre><span></span><span class="n">sgtm_prior</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgtm</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">119</span><span class="p">)</span>

<span class="n">sgtm_lvs</span><span class="p">,</span> <span class="n">sgtm_gvs</span> <span class="o">=</span> <span class="n">check_prior</span><span class="p">(</span><span class="n">sgtm_prior</span><span class="p">)</span>
<span class="n">sgtm_lvs</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">'s'</span><span class="p">)</span>
</pre></div>
<p>Check global parameters</p>
<div class="highlight"><pre><span></span><span class="n">compare_global_params</span><span class="p">(</span><span class="n">sgtm_prior</span><span class="p">,</span>
                      <span class="p">[</span><span class="s1">'alpha'</span><span class="p">,</span> <span class="s1">'beta'</span><span class="p">,</span> <span class="s1">'gamma'</span><span class="p">,</span> <span class="s1">'kappa'</span><span class="p">,</span> <span class="s1">'rho'</span><span class="p">,</span> <span class="s1">'tau'</span><span class="p">],</span>
                      <span class="n">val_ref</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
<p>And local variables. We limit the range of outcome values to focus on
the region most useful to our inference.</p>
<div class="highlight"><pre><span></span><span class="n">plot_locals</span><span class="p">(</span><span class="n">sgtm_prior</span><span class="p">,</span> <span class="n">sgtm_lvs</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Prior prediction conditioned on data</p>
<div class="highlight"><pre><span></span><span class="n">sgtm_prior_fixed</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgtm</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plot_locals</span><span class="p">(</span><span class="n">sgtm_prior_fixed</span><span class="p">,</span> <span class="n">sgtm_lvs</span><span class="p">,</span> <span class="n">v_ref</span><span class="o">=</span><span class="s1">'mu'</span><span class="p">,</span> <span class="n">val_ref</span><span class="o">=</span><span class="mi">600</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
<p>Like before, now we chose one sample from the prior predictive samples
and do parameter recovery on it.</p>
<div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">34</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sgtm_prior</span><span class="p">[</span><span class="s1">'y'</span><span class="p">][</span><span class="n">n</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Fake outcome no. </span><span class="si">{}</span><span class="s1"> from prior'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>
</pre></div>
<p>And inference.</p>
<div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgtm</span><span class="p">)</span>
<span class="n">mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Let's visualise the parameter posterior.</p>
<div class="highlight"><pre><span></span><span class="n">check_inference</span><span class="p">(</span><span class="n">mcmc</span><span class="p">,</span> <span class="n">sgtm_prior</span><span class="p">,</span> <span class="n">sgtm_gvs</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
<p>And the prediction.</p>
<div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="n">mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgtm</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">samples</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>And visualisation.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">sgtm_lvs</span><span class="p">)</span>
</pre></div>
<p>This looks passable enough. Next we do inference on the real data.</p>
<div class="highlight"><pre><span></span><span class="n">sgtm_kernel</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">sgtm</span><span class="p">)</span>
<span class="n">sgtm_mcmc</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">sgtm_kernel</span><span class="p">,</span> <span class="n">num_warmup</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">num_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">sgtm_mcmc</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">sgtm_mcmc</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
<p>Now we can make predictions for future periods.</p>
<div class="highlight"><pre><span></span><span class="n">sgtm_sample</span> <span class="o">=</span> <span class="n">sgtm_mcmc</span><span class="o">.</span><span class="n">get_samples</span><span class="p">()</span>
<span class="n">sgtm_post</span> <span class="o">=</span> <span class="n">Predictive</span><span class="p">(</span><span class="n">sgtm</span><span class="p">,</span> <span class="n">posterior_samples</span><span class="o">=</span><span class="n">sgtm_sample</span><span class="p">)(</span><span class="n">rng</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">[:</span><span class="o">-</span><span class="mi">24</span><span class="p">],</span> <span class="n">T</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">future</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"posterior samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgtm_sample</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n\n</span><span class="s2">posterior Prediction samples:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">sgtm_post</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">'; '</span><span class="p">)</span>
</pre></div>
<p>Let's look at the model prediction.</p>
<div class="highlight"><pre><span></span><span class="n">plot_post</span><span class="p">(</span><span class="n">sgtm_y</span><span class="p">,</span> <span class="n">sgtm_post</span><span class="p">,</span> <span class="n">sgtm_lvs</span><span class="p">,</span> <span class="n">ncol</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>Here the prediction is slightly better than before, the underestimation
problem seems abatted, but we're spotting another problem. The global
trend still has a very strong seasonality effect built in it. Upon some
reflection it's not difficult to understand why: we're now using the
moving average to calculate the global trend, and since the moving
average uses all the outcome for one cycle, the seasonality is clearly
present in it.</p>
<p>And finally the scores.</p>
<div class="highlight"><pre><span></span><span class="n">check_scores</span><span class="p">(</span><span class="n">sgtm_post</span><span class="p">,</span> <span class="n">sgtm_y</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>In this post we studied some time series models for global trend, local
variation, and seasonality. Our goal is not to build the perfect model;
rather, our goal is to build models using composable components, observe
their effects, and make changes when necessary, so that we can model the
specific characteristics in each data set.</p>
<p>The models are all based on a smoothed exponential process, which is
quite flexible in its wide range of behaviours, but sometimes when
modeling data with strong pattern this flexibility might not be
necessary.</p>
<p>The original Rlgt package also includes a double seasonality model, and
a seasonality modeled by an extra exponentially smoothed process, these
models are not implemented here, because they don't enhance the
performance, and induce more identifiability issues.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="riversdark/variation"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/variation/data/2021/05/18/lgts.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/variation/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/variation/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/variation/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Experimentation in Statistical Modelling and Deep Learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/riversdark" target="_blank" title="riversdark"><svg class="svg-icon grey"><use xlink:href="/variation/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/OlivierMa2016" target="_blank" title="OlivierMa2016"><svg class="svg-icon grey"><use xlink:href="/variation/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
