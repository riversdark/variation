{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build, criticise, and expand time series models with NumPyro\n",
    "> Bayesian modeling of global trend, local variation, seasonality, and heterogeneity in time series\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302dae51",
   "metadata": {},
   "source": [
    "\n",
    "In this post we'll implement a time series framework based on a flexible\n",
    "smoothed exponential process. The framework can be used to model global\n",
    "trend, local variation, seasonality and other features that are\n",
    "essential for flexible time series modeling.\n",
    "\n",
    "This post also covers several other features under this general\n",
    "framework, notably:\n",
    "\n",
    "-   modeling over-dispersed observations with Student-T distribution\n",
    "-   explicit and extensive modeling of the variation to capture data\n",
    "    heteroscedasticity\n",
    "-   adding regression components when necessary\n",
    "\n",
    "Some other general but important features include:\n",
    "\n",
    "-   extensive usage of visualisation for model criticism\n",
    "-   prior and posterior simulation for model inspection\n",
    "-   and a unified prediction framework.\n",
    "\n",
    "Last but maybe most importantly, apart from the specific features and\n",
    "techniques, the post gives a general procedure for Bayesian model\n",
    "building and criticism, which should be useful for building any kind of\n",
    "model for any kind of data.\n",
    "\n",
    "This is mostly a port of the R package [Rlgt: Bayesian Exponential Smoothing Models with Trend Modifications](https://cran.r-project.org/web/packages/Rlgt/index.html), although the implementation details might differ here and there. One of the models has already been [ported to Numpyro](http://num.pyro.ai/en/stable/tutorials/time_series_forecasting.html) in one of the Numpyro tutorials, so that specific model is not covered here.\n",
    "\n",
    "Different options for modeling trends and seasonalities have been\n",
    "considered in this post, to understand their varying capabilities and\n",
    "characteristics, and to prepare an arsenal for building bespoke models\n",
    "for future data sets with different characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713ff1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette='Set2')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import lax, nn\n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "jnp.set_printoptions(precision=2)\n",
    "rng = random.PRNGKey(123)\n",
    "\n",
    "import numpyro\n",
    "numpyro.set_host_device_count(4)\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.contrib.control_flow import scan\n",
    "from numpyro.diagnostics import autocorrelation, hpdi\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from numpyro import deterministic, sample\n",
    "from numpyro.handlers import seed, condition, plate\n",
    "\n",
    "import daft\n",
    "\n",
    "from rdatasets import data, descr, summary\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a741d7ed",
   "metadata": {},
   "source": [
    "\n",
    "We are going to use two data sets, the BJ sales data and the monthly\n",
    "airline passenger data set. For the BJ sales data set we have an extra\n",
    "lead indicator, which can be used to build a regression component to the\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d01313",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "bj = data(\"BJsales\")['value']\n",
    "air_passengers = data(\"AirPassengers\")['value']\n",
    "bj_lead = jnp.array(\n",
    "    [10.01,10.07,10.32, 9.75,10.33,10.13,10.36,10.32,10.13,10.16,10.58,10.62, 10.86,11.20,10.74,10.56,10.48,10.77,11.33,10.96,11.16,11.70,11.39,11.42, 11.94,11.24,11.59,10.96,11.40,11.02,11.01,11.23,11.33,10.83,10.84,11.14, 10.38,10.90,11.05,11.11,11.01,11.22,11.21,11.91,11.69,10.93,10.99,11.01, 10.84,10.76,10.77,10.88,10.49,10.50,11.00,10.98,10.61,10.48,10.53,11.07, 10.61,10.86,10.34,10.78,10.80,10.33,10.44,10.50,10.75,10.40,10.40,10.34, 10.55,10.46,10.82,10.91,10.87,10.67,11.11,10.88,11.28,11.27,11.44,11.52, 12.10,11.83,12.62,12.41,12.43,12.73,13.01,12.74,12.73,12.76,12.92,12.64, 12.79,13.05,12.69,13.01,12.90,13.12,12.47,12.47,12.94,13.10,12.91,13.39, 13.13,13.34,13.34,13.14,13.49,13.87,13.39,13.59,13.27,13.70,13.20,13.32, 13.15,13.30,12.94,13.29,13.26,13.08,13.24,13.31,13.52,13.02,13.25,13.12, 13.26,13.11,13.30,13.06,13.32,13.10,13.27,13.64,13.58,13.87,13.53,13.41, 13.25,13.50,13.58,13.51,13.77,13.40])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698b863",
   "metadata": {},
   "source": [
    "\n",
    "These data sets have different characteristics and as such will demand\n",
    "different modeling considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065112b",
   "metadata": {},
   "source": [
    "\n",
    "## Modeling global trend and local variation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d3679",
   "metadata": {},
   "source": [
    "\n",
    "The data used for this model is the\n",
    "[BJsales](https://vincentarelbundock.github.io/Rdatasets/doc/datasets/BJsales.html)\n",
    "data set, which contains 150 sales observations. The data are bounded by\n",
    "zero, as can be expected for sales data, and range between 200 and 300,\n",
    "and there is very little variation between neighbouring points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1734d60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 266,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lgt_y = jnp.array(bj)\n",
    "\n",
    "plt.plot(bj, '.')\n",
    "plt.title('BJ sales data');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c71e89",
   "metadata": {},
   "source": [
    "\n",
    "The first model we are building can be described as\n",
    "\n",
    "\\begin{align*}\n",
    "y_{t} &\\sim \\text{T} (\\nu,\\mu_{t}, \\sigma_{t}) \\\\\n",
    "\\sigma_{t} &= \\kappa \\mu_{t}^{ \\tau} + \\xi   \\\\\n",
    "\\mu_{t} &= \\text{G}_{t} + \\text{L}_{t} \\\\\n",
    "\\text{G}_{t} &= g_{t-1} + \\gamma g_{t-1}^{ \\rho } \\\\\n",
    "\\text{L}_{t} &= \\lambda l_{t-1}  \\\\\n",
    "g_{t} &= \\alpha y_{t} + ( 1- \\alpha ) \\text{G}_{t} \\\\\n",
    "l_{t} &= \\beta  \\left( g_{t}-g_{t-1} \\right) + \\left( 1- \\beta  \\right) l_{t-1}\\\\\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549efdfe",
   "metadata": {},
   "source": [
    "This first model is already quite complex so let's spend a moment to digest it. We can visualise the model as follows\n",
    "\n",
    "![](lgts0.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba07ac",
   "metadata": {},
   "source": [
    "The variable $y$ in the above graph is shaded because it's the observed variable. One thing to keep in mind with Bayesian networks, is that they form directed acyclical graphs, that is, there shouldn't be circles anywhere in the model. We can check that this is indeed the case with the above model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0e5c1",
   "metadata": {},
   "source": [
    "\n",
    "To model the outcome we'll use the Student's T distribution, which has\n",
    "three parameters: the degree of freedom $\\nu$, the expected value $\\mu$,\n",
    "and the standard deviation $\\sigma$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b4479",
   "metadata": {},
   "source": [
    "\n",
    "The degree of freedom determines how heavy tailed the distribution will\n",
    "be: when the degree of freedom is one, the T distribution becomes the Cauchy\n",
    "distribution, which is so heavy tailed that the mean and standard\n",
    "deviation can not even be properly defined; when the degree of freedom\n",
    "approaches infinity the distribution approaches Normal, with quite narrow\n",
    "tails. Using the T distribution, we can account for heavy tail data when\n",
    "the data are indeed heavy tailed, but it's also flexible enough for when\n",
    "they are not. Since à priori we don't have much information on the\n",
    "degree of freedom, we'll put a wide prior on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e19d51",
   "metadata": {},
   "source": [
    "\n",
    "The expected value is where we'll put most of our modeling focus on. In\n",
    "this first model we'll consider modeling it as the sum of a global trend\n",
    "$\\text{G}$ and a local variation $\\text{L}$, with the global trend being\n",
    "modeled as a smoothed exponential function of the hidden level $g$, and\n",
    "the local variation as a dampened baseline variation $l$. Furthermore,\n",
    "both the hidden level $g$ and the variation $l$ follow a smoothed\n",
    "autoregressive process. For hidden level $g$, it's a weighted average\n",
    "between the outcome, and the previous global\n",
    "trend. For the baseline variation $l$ it's a weighted average between\n",
    "the increment in hidden levels, and the previous baseline variation.\n",
    "$\\alpha$ and $\\beta$ are the corresponding smoothing weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5300424",
   "metadata": {},
   "source": [
    "\n",
    "The standard deviation is modeled with another smoothed exponential\n",
    "process of the expected value, so this is a heterogeneous model, with the\n",
    "variance varying alongside the expected value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5b927",
   "metadata": {},
   "source": [
    "Some of the modeling assumptions might look a bit arbitrary; and they indeed are. There are a thousand choices we can make about how each piece of the model is formulated, and sometimes we have to try different alternatives to see which works best for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778a07cc",
   "metadata": {},
   "source": [
    "\n",
    "Because both the standard deviation $\\sigma$ and the expected value\n",
    "$\\mu$ follow a smoothed exponential process, we have to guarantee the\n",
    "expected value and the hidden level $g$ be always positive. This\n",
    "assumption is reasonable when the data is also positive, as is the case\n",
    "here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c72857",
   "metadata": {},
   "source": [
    "\n",
    "It's important to note that the models we are building might be too\n",
    "complex for the data we are using. Here by exploratory analysis we can\n",
    "see that the BJ sales data have a global trend, and there are clearly\n",
    "local variations, but there doesn't seem to be many sudden changes in\n",
    "the series, and as such the smoothed exponential process for the\n",
    "standard deviation might not be strictly necessary. Still, we use a more\n",
    "complex model to account for the **potential** heterogeneities in the\n",
    "data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5ff50",
   "metadata": {},
   "source": [
    "\n",
    "Now we can chose some proper priors for the model parameters and code\n",
    "the model in NumPyro. Of course, choosing priors is an integral part of\n",
    "Bayesian modeling and it is no easy task, we need to consider our\n",
    "prior knowledge carefully and gradually update that in model criticism.\n",
    "The priors chosen for this model implementation take into account both\n",
    "the model structure, and the general knowledge we have gained from\n",
    "exploratory data analysis. In Bayesian analysis each model should be a\n",
    "bespoke model for the specific data set we are modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d2002",
   "metadata": {},
   "source": [
    "Because this is a time series model, we use the first observation to\n",
    "initialise the model. Specifically, we'll use it to indicate the\n",
    "starting whereabouts of the hidden level $g$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c4a3f",
   "metadata": {},
   "source": [
    "\n",
    "``` python\n",
    "def lgt(y, future=3):\n",
    "    N = y.shape[0]\n",
    "    nu = sample(\"nu\", dist.Uniform(1, 50))\n",
    "\n",
    "    xi = sample(\"xi\", dist.HalfNormal(2))\n",
    "    tau = sample(\"tau\", dist.Beta(1, 4))\n",
    "    kappa = sample(\"kappa\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    lbda = sample(\"lbda\", dist.Beta(2, 2))\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.Normal(y[0], 5))\n",
    "    l_init = sample(\"l_init\", dist.Normal(0, 10))\n",
    "\n",
    "    def transition_fn(carry, t):\n",
    "        g, l = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        L = deterministic(\"L\", lbda * l)\n",
    "        mu = deterministic(\"mu\", jnp.clip(G + L, 0))\n",
    "        sigma = deterministic(\"sigma\", xi + kappa * mu ** tau)\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * yt + (1 - alpha) * G, 0))\n",
    "        l_new = deterministic(\"l\", beta * (g_new - g) + (1 - beta) * l)\n",
    "\n",
    "        return (g_new, l_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, l_init),  jnp.arange(0, N+future))\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", lgt_y[:4])\n",
    "\n",
    "with seed(rng_seed=3):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(lgt(lgt_y[:1], future=3))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(lgt(lgt_y[:4], future=0))\n",
    "```\n",
    "\n",
    "With the model specified we can draw prior samples from it. We can\n",
    "generate prior samples without the observed data, because our model is\n",
    "completely generative. We also write a simple helper function\n",
    "`check_prior` to collect model variables and print out summary\n",
    "statistics. The model variables are separated into two groups: global\n",
    "variables, which determine the behaviour of the whole model; and local\n",
    "variables which are specific to each observed outcome.\n",
    "\n",
    "``` python\n",
    "lgt_prior = Predictive(lgt, num_samples=500)(rng, lgt_y[:1], 139)\n",
    "\n",
    "def check_prior(prior):\n",
    "    \"\"\"Print out prior shapes,\n",
    "    collect local and global variables into lists.\"\"\"\n",
    "    lvs = []\n",
    "    gvs = []\n",
    "    y_length = prior['y'].shape[-1]\n",
    "    for k, v in prior.items():\n",
    "        if v.ndim == 1:\n",
    "            gvs.append(k)\n",
    "        elif v.shape[1] == y_length:\n",
    "            lvs.append(k)\n",
    "        else:\n",
    "            gvs.append(k)\n",
    "\n",
    "    print(\"Global variables:\")\n",
    "    for k in gvs:\n",
    "        print(k, prior[k].shape, end='; ')\n",
    "    print(\"\\n\\nLocal variables:\")\n",
    "    for k in lvs:\n",
    "        print(k, prior[k].shape, end='; ')\n",
    "\n",
    "    return lvs, gvs\n",
    "\n",
    "lgt_lvs, lgt_gvs = check_prior(lgt_prior)\n",
    "```\n",
    "\n",
    "It's important to check the model prior predictions to make sure the\n",
    "model indeed has the desired properties we intended for. We can use a\n",
    "parallel plot in which each sample will be plotted as one single line\n",
    "across different coordinates. While doing so, we'll also differentiate\n",
    "the normal samples, that is that intended samples of our model, from\n",
    "samples with extremely large outcomes, and also from samples with\n",
    "numerical overflows. The samples with numerical overflow will be in red,\n",
    "while those with extreme outcomes will be in blue.\n",
    "\n",
    "Here we first write another helper function `concat_arrays` to\n",
    "concatenate one or two dimensional arrays stored in a dict, as is the\n",
    "case with our prior and posterior samples. And if the variable is\n",
    "multivariate, we also break down its dimensions into separate variables.\n",
    "This is to facilitate later visualisation.\n",
    "\n",
    "``` python\n",
    "def concat_arrays(dct, vs):\n",
    "    nvs = []\n",
    "    ds = []\n",
    "    for v in vs:\n",
    "        d = dct[v]\n",
    "        if d.ndim == 2:\n",
    "            vplus = [v+str(i) for i in range(d.shape[-1])]\n",
    "            nvs += vplus\n",
    "            ds.append(d)\n",
    "        else:\n",
    "            nvs.append(v)\n",
    "            ds.append(d[:, None])\n",
    "\n",
    "    ds = jnp.concatenate(ds, axis=1)\n",
    "\n",
    "    return nvs, ds\n",
    "\n",
    "\n",
    "nvs, ds = concat_arrays(lgt_prior, ['gamma', 'rho'])\n",
    "print(len(nvs), ds.shape)  # 1+1=2, (500, 2)\n",
    "\n",
    "nvs, ds = concat_arrays(lgt_prior, ['g', 'l'])\n",
    "print(len(nvs), ds.shape)  # 140+140=280, (500, 280)\n",
    "\n",
    "nvs, ds = concat_arrays(lgt_prior, ['gamma', 'l', 'rho'])\n",
    "print(len(nvs), ds.shape)  # 1+140+1=142, (500, 142)\n",
    "```\n",
    "\n",
    "And plot the global parameters of the model\n",
    "\n",
    "``` python\n",
    "def compare_global_params(dct, vs, v_ref='y', val_ref=10000):\n",
    "    # TODO add custom labels to different groups\n",
    "\n",
    "    nans = jnp.any(jnp.isnan(dct[v_ref]), axis=1)\n",
    "    extremes = jnp.any(dct[v_ref] > val_ref, axis=1)\n",
    "    print('Number of samples with numerical overflows: ', nans.sum())\n",
    "    print('Number of samples with extreme values : ', extremes.sum())\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, sharex=True, sharey=True)\n",
    "    nvs, ds = concat_arrays(dct, vs)\n",
    "\n",
    "    for i in range(ds.shape[0]):\n",
    "        if not nans[i]:\n",
    "            if not extremes[i]:\n",
    "                ax.plot(ds[i], color=colors[0], alpha=0.1, lw=1)\n",
    "\n",
    "    for i in range(ds.shape[0]):\n",
    "        if not nans[i]:\n",
    "            if extremes[i]:\n",
    "                ax.plot(ds[i], color='r', alpha=0.2, lw=1)\n",
    "\n",
    "    for i in range(ds.shape[0]):\n",
    "        if nans[i]:\n",
    "            ax.plot(ds[i], color='k', alpha=0.2, lw=1)\n",
    "\n",
    "    ax.set_xticks(range(len(nvs)))\n",
    "    ax.set_xticklabels(nvs, rotation=30)\n",
    "    plt.tight_layout();\n",
    "\n",
    "compare_global_params(lgt_prior, lgt_gvs)\n",
    "```\n",
    "\n",
    "Because different variables have different scales, to get a better idea\n",
    "of some certain variables we can limit the list of variables to be\n",
    "plotted:\n",
    "\n",
    "``` python\n",
    "compare_global_params(lgt_prior, ['rho', 'tau', 'gamma', 'kappa', 'lbda'])\n",
    "```\n",
    "\n",
    "Even though we have consciously limited the prior distribution for\n",
    "$\\rho$ to favour smaller values, when its values are still relatively\n",
    "large, we're likely to see samples with extreme outcomes. This is\n",
    "understandable because $\\rho$ decides how fast the exponential grows.\n",
    "\n",
    "We can also plot the distribution of one single variable of interest:\n",
    "\n",
    "``` python\n",
    "sns.histplot(lgt_prior['rho'], bins=30, stat='probability');\n",
    "```\n",
    "\n",
    "This corresponds to the samples at coordinate `rho` in the previous\n",
    "plot. A priori the expected value of $\\rho$ is 0.2, but as we can see,\n",
    "that are many samples will much bigger values. We can try to change the\n",
    "priors for these parameters to limit their behavior, but the problem\n",
    "here doesn't seem very serious so we'll leave them be for now.\n",
    "\n",
    "We now plot the model outcome, and other local variables to check the\n",
    "implications of the model priors.\n",
    "\n",
    "``` python\n",
    "def plot_locals(dct, vs, v_ref='y', val_ref=5000, **plt_kws):\n",
    "\n",
    "    extremes = jnp.any(dct[v_ref] > val_ref, axis=1)\n",
    "    print('Number of samples with extreme values : ', extremes.sum())\n",
    "\n",
    "    fig, axes = plt.subplots(**plt_kws)\n",
    "    axes = axes.flatten()\n",
    "    for i, v in enumerate(vs):\n",
    "        data = dct[v]\n",
    "        for k in range(data.shape[0]):\n",
    "            if not extremes[k]:\n",
    "                axes[i].plot(data[k], color=colors[0], alpha=0.2)\n",
    "        axes[i].set_title(v)\n",
    "    plt.tight_layout();\n",
    "\n",
    "plot_locals(lgt_prior, lgt_lvs, nrows=4, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "We can see that the outcome generally follow the smoothed exponential\n",
    "process, with some cases the outcome growing faster than other cases.\n",
    "Also notice that the local variation is roughly one to two orders of\n",
    "magnitudes smaller than the global trend. This is intended because we\n",
    "want to to able to properly identify them.\n",
    "\n",
    "We can also generate prior samples conditioned on the observed data.\n",
    "Since time series models are Markovian, i.e. path dependent, the\n",
    "observed data will serve to limit some of the model parameters that\n",
    "depend on the outcome to a region in the ambient space that is\n",
    "compatible with the observed data.\n",
    "\n",
    "``` python\n",
    "lgt_prior_fixed = Predictive(lgt, num_samples=500)(rng, lgt_y[:140], 0)\n",
    "\n",
    "plot_locals(lgt_prior_fixed, lgt_lvs, v_ref='mu', val_ref=500,\n",
    "            nrows=4, ncols=2, figsize=(8,8))\n",
    "```\n",
    "\n",
    "This might be useful for understanding the general interactions between\n",
    "the model and the data, and also for comparison with the posterior, to\n",
    "see how much we have learned.\n",
    "\n",
    "To test that our inference engine and the model works properly, We can\n",
    "then chose one of our prior samples, feed it to our inference engine to\n",
    "see if we can recover the parameters used to generate them. It's\n",
    "important to realise that we are not always able to recover the\n",
    "parameters, because as we have see in the prior check, there are some\n",
    "highly irregular samples.\n",
    "\n",
    "First chose one sample\n",
    "\n",
    "``` python\n",
    "n = 14\n",
    "y = lgt_prior['y'][n]\n",
    "\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "The sample seems ordinary enough, not one of the extreme ones. We then\n",
    "run inference on it and compare the posterior samples with the real\n",
    "parameter value that we used to generate the outcome.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(lgt)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, y, future=0)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "def check_inference(mcmc, prior, vs, n):\n",
    "    refs = []\n",
    "    for v in vs:\n",
    "        val = prior[v][n]\n",
    "        if val.ndim == 0:\n",
    "            refs.append(val.item())\n",
    "        else:\n",
    "            refs += list(val)\n",
    "\n",
    "    d = az.from_numpyro(mcmc)\n",
    "    az.plot_posterior(d, var_names=vs, ref_val=refs,\n",
    "                      point_estimate=None, hdi_prob='hide', figsize=(8, 8));\n",
    "\n",
    "check_inference(mcmc, lgt_prior, lgt_gvs, n=14)\n",
    "```\n",
    "\n",
    "The recovered parameters are well in the range of possibilities, but\n",
    "there are large uncertainties around them. This is to be expected: when\n",
    "building complex models, there is only so much we can learn from the\n",
    "limited data. We move on to the inference on real data.\n",
    "\n",
    "``` python\n",
    "lgt_kernel = NUTS(lgt, target_accept_prob=0.95)\n",
    "lgt_mcmc = MCMC(lgt_kernel,\n",
    "                num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False)\n",
    "lgt_mcmc.run(rng, lgt_y[:-10], future=0)\n",
    "lgt_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Now we can make predictions for future periods.\n",
    "\n",
    "``` python\n",
    "lgt_sample = lgt_mcmc.get_samples()\n",
    "lgt_post = Predictive(lgt, posterior_samples=lgt_sample)(rng, lgt_y[:140], 10)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in lgt_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in lgt_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "And plot the predictions. We'll plot the mean, along with 5th, 25th,\n",
    "75th, and 95th prediction distribution percentiles.\n",
    "\n",
    "``` python\n",
    "def plot_post(y, post, vs, nrow=3, ncol=2, size=(8, 8)):\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=size)\n",
    "    axes = axes.flatten()\n",
    "    for i, v in enumerate(vs):\n",
    "        if v == 'y':\n",
    "            axes[i].plot(y, color=colors[0], lw=1);\n",
    "\n",
    "        mean = post[v].mean(0)\n",
    "        p5 = jnp.percentile(post[v], 5, axis=0)\n",
    "        p25 = jnp.percentile(post[v], 25, axis=0)\n",
    "        p75 = jnp.percentile(post[v], 75, axis=0)\n",
    "        p95 = jnp.percentile(post[v], 95, axis=0)\n",
    "        sd = post[v].std(0)\n",
    "        axes[i].fill_between(range(len(mean)), p5, p95,\n",
    "                         color=colors[4], alpha=0.2)\n",
    "        axes[i].fill_between(range(len(mean)), p25, p75,\n",
    "                         color=colors[1], alpha=0.4)\n",
    "        axes[i].plot(mean, color=colors[2], lw=1)\n",
    "        axes[i].set_title(v)\n",
    "    plt.tight_layout();\n",
    "\n",
    "plot_post(lgt_y, lgt_post, lgt_lvs, nrow=4, ncol=2)\n",
    "```\n",
    "\n",
    "We can see that the prediction is not that far off: we get the basic\n",
    "trend right, and the 50% credible interval does cover the observed data.\n",
    "But still, our model prediction is basically a linear extrapolation, the\n",
    "model prediction is almost entirely determined by the global trend, and\n",
    "the local variation in our model contributed little, if anything at all,\n",
    "in modifying the global behaviour. This motivates us to add some new\n",
    "information of locality to our model, and this is why we'll add a new\n",
    "regression component in the next part.\n",
    "\n",
    "Besides, the posterior of $\\sigma$ is very small and almost constant,\n",
    "this indicates that the smoothed exponential model for the standard\n",
    "deviation is entirely redundant, we might as well just use a simple\n",
    "homogeneous formulation.\n",
    "\n",
    "Apart from visualising the posterior prediction, there are also many\n",
    "different scores we can check. The first one is the [symmetric mean\n",
    "absolute percentage\n",
    "error](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error),\n",
    "which is based on the percentage error of the mean prediction. Notice\n",
    "that the \"mean\" here refers to **the mean of the point estimate** across\n",
    "all the observations, and for the point estimate itself, we can use the\n",
    "mean, the median, or any other point estimate of the posterior.\n",
    "\n",
    "``` python\n",
    "def eval_smape(pred, truth):\n",
    "    pred = jnp.mean(pred, 0)\n",
    "    return 200 * jnp.mean(jnp.abs(pred - truth) / (pred + truth)).item()\n",
    "```\n",
    "\n",
    "And the mean absolute error. This time we will use the median of the\n",
    "posterior prediction to calculate the point estimate.\n",
    "\n",
    "``` python\n",
    "def eval_mae(pred, truth):\n",
    "    pred = jnp.median(pred, 0)\n",
    "    return jnp.mean(jnp.abs(pred - truth)).item()\n",
    "```\n",
    "\n",
    "And the root mean squared error, using sample mean as point estimate.\n",
    "\n",
    "``` python\n",
    "def eval_rmse(pred, truth):\n",
    "    pred = jnp.mean(pred, 0)\n",
    "    return jnp.sqrt(jnp.mean(jnp.square(pred - truth))).item()\n",
    "```\n",
    "\n",
    "And finally, the [continuous ranked probability\n",
    "score](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf).\n",
    "The previous scores all use point estimates of the posterior, CRPS takes\n",
    "the whole posterior into consideration.\n",
    "\n",
    "``` python\n",
    "def eval_crps(pred, truth):\n",
    "    # ref: https://github.com/pyro-ppl/pyro/pull/2045\n",
    "    num_samples = pred.shape[0]\n",
    "    pred = jnp.sort(pred, axis=0)\n",
    "    diff = pred[1:] - pred[:-1]\n",
    "    weight = jnp.arange(1, num_samples) * jnp.arange(num_samples - 1, 0, -1)\n",
    "    weight = weight.reshape(weight.shape + (1,) * truth.ndim)\n",
    "    crps_empirical = jnp.mean(jnp.abs(pred - truth), 0) \\\n",
    "        - (diff * weight).sum(axis=0) / num_samples ** 2\n",
    "    return jnp.mean(crps_empirical).item()\n",
    "```\n",
    "\n",
    "Let's see how well we have done.\n",
    "\n",
    "``` python\n",
    "def check_scores(post, y, n_test):\n",
    "    pred = post['y'][:, -n_test:]\n",
    "    ytest = y[-n_test:]\n",
    "    print(\"sMAPE: {:.2f}%, MAE: {:.2f}, RMSE: {:.2f}, CPRS: {:.2f}\".format(\n",
    "        eval_smape(pred, ytest),\n",
    "        eval_mae(pred, ytest),\n",
    "        eval_rmse(pred, ytest),\n",
    "        eval_crps(pred, ytest)))\n",
    "\n",
    "check_scores(lgt_post, lgt_y, 10)\n",
    "```\n",
    "\n",
    "Of course these scores are only useful when compared with performance\n",
    "from other models. We can come back and compare them to other models in\n",
    "the later parts of the post.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008178e",
   "metadata": {},
   "source": [
    "## Adding a regression component to the model\n",
    "\n",
    "As we've seen before, the model prediction from the previous model is\n",
    "far from satisfactory. Luckily with the `BJsales` data set we have\n",
    "another accompanying lead indicator which we can integrate into our\n",
    "model as a regression component. Also, since the smoothed exponential\n",
    "process for the standard deviation doesn't contributed much, we'll\n",
    "remove this model component, and directly put a prior on $\\sigma$.\n",
    "\n",
    "The lead indicator\n",
    "\n",
    "``` python\n",
    "plt.plot(bj_lead, '.')\n",
    "plt.title('BJ sales lead indicator');\n",
    "```\n",
    "\n",
    "we can see that the indicator variable follows a similar progression\n",
    "pattern as that of the sales data, so it should offer us useful\n",
    "information to improve the model prediction.\n",
    "\n",
    "We'll use the lead indicator, lagged 3 and 4 periods, as predictors. The\n",
    "total length of the data, since we have to remove the first 4 data\n",
    "points for lack of predictors, becomes 146. We'll use the last 10 period\n",
    "for prediction as before. The regression component of the model will\n",
    "also have an intercept.\n",
    "\n",
    "``` python\n",
    "x0 = jnp.ones([146])\n",
    "x1 = bj_lead[:-4]\n",
    "x2 = bj_lead[1:-3]\n",
    "lgtr_x = jnp.stack((x0, x1, x2), axis=-1)\n",
    "\n",
    "lgtr_y = jnp.array(bj[4:])\n",
    "print('Predictors shape:', lgtr_x.shape, '\\nOutcome shape:', lgtr_y.shape)\n",
    "```\n",
    "\n",
    "We change the expected value of the model so that it now consists of\n",
    "three components: a global trend, a local variation, and a local\n",
    "regression component.\n",
    "\n",
    "Also, with increasing components for the expected value, it's getting\n",
    "more difficult to assign a proper prior to the hidden level $g$. For\n",
    "this reason we give the starting level a wider Cauchy prior, and limit\n",
    "its value to be positive.\n",
    "\n",
    "``` python\n",
    "def lgtr(x, y):\n",
    "    nu = sample(\"nu\", dist.Uniform(2, 20))\n",
    "    sigma = sample(\"sigma\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    lbda = sample(\"lbda\", dist.Beta(2, 2))\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "\n",
    "    with plate(\"D\", size=x.shape[1]):\n",
    "        eta = sample(\"eta\", dist.Normal(0, 5))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.TruncatedCauchy(y[0], 10, low=0))\n",
    "    l_init = sample(\"l_init\", dist.Normal(0, 10))\n",
    "\n",
    "    def transition_fn(carry, xt):\n",
    "        g, l = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        L = deterministic(\"L\", lbda * l)\n",
    "        R = deterministic(\"R\", jnp.dot(xt, eta))\n",
    "        # LR = deterministic(\"LR\", L+R)\n",
    "        mu = deterministic(\"mu\", jnp.clip(G + L + R, 0))\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * yt + (1 - alpha) * G, 0))\n",
    "        l_new = deterministic(\"l\", beta * (g_new - g) + (1 - beta) * l)\n",
    "\n",
    "        return (g_new, l_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, l_init),  x)\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", lgtr_y[:4])\n",
    "\n",
    "with seed(rng_seed=3):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(lgtr(lgtr_x[:4], lgtr_y[:1]))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(lgtr(lgtr_x[:4], lgtr_y[:4]))\n",
    "```\n",
    "\n",
    "Sample from the prior\n",
    "\n",
    "``` python\n",
    "lgtr_prior = Predictive(lgtr, num_samples=500)(rng, lgtr_x[:-10], lgtr_y[:1])\n",
    "\n",
    "lgtr_lvs, lgtr_gvs = check_prior(lgtr_prior)\n",
    "```\n",
    "\n",
    "First let's take a look at our truncated Cauchy prior for the initial\n",
    "hidden level\n",
    "\n",
    "``` python\n",
    "sns.histplot(lgtr_prior['g_init']);\n",
    "```\n",
    "\n",
    "We can see the bulk of the samples are around 200, but the samples have\n",
    "quite a wide range, from zero to a few thousands, so it should be\n",
    "flexible enough for our model.\n",
    "\n",
    "And check for global parameters\n",
    "\n",
    "``` python\n",
    "compare_global_params(lgtr_prior, ['eta', 'gamma', 'l_init', 'sigma', ])\n",
    "```\n",
    "\n",
    "We don't have numerical overflows, this is good. We do have more samples\n",
    "with very large outcomes, but this is to be expected, because we are\n",
    "introducing more model components to the model, and this expands our\n",
    "modeling space and consequently the outcome range. What matters here, is\n",
    "that there are no clear patterns to the extreme outcomes, which implies\n",
    "that the extreme outcomes are the natural result of expanding the model\n",
    "space, not that of some malfunctionaling model component.\n",
    "\n",
    "And the prior prediction for local variables\n",
    "\n",
    "``` python\n",
    "plot_locals(lgtr_prior, lgtr_lvs, val_ref=10000,\n",
    "            nrows=4, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "Again, we can clearly see the effect of the smoothed exponential model.\n",
    "We can compare it with prior predictions conditioning on the data\n",
    "\n",
    "``` python\n",
    "lgtr_prior_fixed = Predictive(lgtr, num_samples=500)(rng, lgtr_x[:-10], lgtr_y[:-10])\n",
    "\n",
    "plot_locals(lgtr_prior_fixed, lgtr_lvs, v_ref='mu', val_ref=500,\n",
    "            nrows=4, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "The introduction of the regression component makes the relationship\n",
    "between the hidden level $l$ and the model outcome $y$ less prominent.\n",
    "This implies that the current model has the **potential** to improve on\n",
    "the previous model, since the model now has more flexibility introduced\n",
    "by the R component. However, we should also be aware that if the\n",
    "regression part does not explain the outcome very well, it can also\n",
    "potentially reduce the overall predictive power of the model, as it\n",
    "obscures the previously dominating relationship between the outcome and\n",
    "the hidden level.\n",
    "\n",
    "Like before, now we chose one sample from the prior predictive samples\n",
    "and do parameter recovery.\n",
    "\n",
    "``` python\n",
    "n = 14\n",
    "y = lgtr_prior['y'][n]\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "And inference.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(lgtr)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, lgtr_x[:-10], y)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "check_inference(mcmc, lgtr_prior, lgtr_gvs, n=14)\n",
    "```\n",
    "\n",
    "We have been able to recover most of the parameters. The posterior for\n",
    "$\\nu$ is quite far off, but considering the model and the data used,\n",
    "this is also understandable: this specific sample hardly has any\n",
    "fluctuation at all, there is no way we can learn much about the true\n",
    "degree of freedom.\n",
    "\n",
    "Next we do inference on the real data.\n",
    "\n",
    "``` python\n",
    "lgtr_kernel = NUTS(lgtr, target_accept_prob=0.95)\n",
    "lgtr_mcmc = MCMC(lgtr_kernel, num_warmup=2000, num_samples=2000,\n",
    "                 num_chains=4, progress_bar=False)\n",
    "lgtr_mcmc.run(rng, lgtr_x[:-10], lgtr_y[:-10])\n",
    "lgtr_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Now we can make predictions for future periods. If we feed the\n",
    "`Predictive` function with more predictors than outcomes, the function\n",
    "will automatically make prediction for future periods.\n",
    "\n",
    "``` python\n",
    "lgtr_sample = lgtr_mcmc.get_samples()\n",
    "lgtr_post = Predictive(lgtr, posterior_samples=lgtr_sample)(rng, lgtr_x, lgtr_y[:-10])\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in lgtr_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in lgtr_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "Let's look at the model prediction.\n",
    "\n",
    "``` python\n",
    "plot_post(lgtr_y, lgtr_post, lgtr_lvs, nrow=4, ncol=2)\n",
    "```\n",
    "\n",
    "This time the model prediction follows much closer to the local\n",
    "variations, we have made significant improvement to the model's\n",
    "predictive power. The reason for this improvement, as we can see from\n",
    "the above plot, is that we now have two model components, R and L, that\n",
    "contribute very meaningfully to the prediction. In the previous model,\n",
    "although the local variation component has been able to capture some\n",
    "local change, but the prediction for future periods is outright flat,\n",
    "which means it hardly alters the global trend at all. In this model, on\n",
    "the contrary, there is some uncertainty around the global trend G all\n",
    "along the time series, and this uncertainty permits the other model\n",
    "components, R and L, to make changes to the global trend when the data\n",
    "demands it, and to consequently change the overall mean and ultimately\n",
    "the outcome.\n",
    "\n",
    "We can also check the posterior distribution of $\\sigma$:\n",
    "\n",
    "``` python\n",
    "sns.histplot(lgtr_sample['sigma'], bins=30);\n",
    "```\n",
    "\n",
    "Indeed the standard deviation is very small, and with little variation.\n",
    "We can also check the scores.\n",
    "\n",
    "``` python\n",
    "check_scores(lgtr_post, lgtr_y, 10)\n",
    "```\n",
    "\n",
    "All the scores have significantly improved (that is, reduced), compared\n",
    "to the previous model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815341b",
   "metadata": {},
   "source": [
    "## Using a different variation modeling approach\n",
    "\n",
    "From the previous two models we can see that with the BJsales data set,\n",
    "the expected value does not affect the outcome variation very much, so\n",
    "the exponentially smoothed model is not very helpful.\n",
    "\n",
    "Here we propose another model for the variation. In this model, the\n",
    "variation is a linear function of another hidden variable $w$, which\n",
    "itself follows an autoregressive process as the smoothed average between\n",
    "its previous value and the current absolute error, just like $l$ and\n",
    "$g$.\n",
    "\n",
    "``` python\n",
    "lgtrs_x = lgtr_x\n",
    "lgtrs_y = lgtr_y\n",
    "print('Predictors shape:', lgtrs_x.shape, '\\nOutcome shape:', lgtrs_y.shape)\n",
    "```\n",
    "\n",
    "The full model can be written down as:\n",
    "\n",
    "And we update our model\n",
    "\n",
    "``` python\n",
    "def lgtrs(x, y):\n",
    "    nu = sample(\"nu\", dist.Uniform(1, 50))\n",
    "\n",
    "    xi = sample(\"xi\", dist.HalfNormal(2))\n",
    "    kappa = sample(\"kappa\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    lbda = sample(\"lbda\", dist.Beta(2, 2))\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "    zeta = sample(\"zeta\", dist.Beta(2, 2))\n",
    "\n",
    "    with plate(\"D\", size=x.shape[1]):\n",
    "        eta = sample(\"eta\", dist.Normal(0, 5))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.TruncatedCauchy(y[0], 10, low=0))\n",
    "    l_init = sample(\"l_init\", dist.Normal(0, 10))\n",
    "    w_init = sample(\"w_init\", dist.HalfNormal(5))\n",
    "\n",
    "    def transition_fn(carry, xt):\n",
    "        g, l, w = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        L = deterministic(\"L\", lbda * l)\n",
    "        R = deterministic(\"R\", jnp.dot(xt, eta))\n",
    "        mu = deterministic(\"mu\", jnp.clip(G + L + R, 0))\n",
    "        sigma = deterministic(\"sigma\", xi + kappa * w)\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * yt + (1 - alpha) * G, 0))\n",
    "        l_new = deterministic(\"l\", beta * (g_new - g) + (1 - beta) * l)\n",
    "        w_new = deterministic(\"w\", zeta * jnp.abs(yt - mu) + (1 - zeta) * w)\n",
    "\n",
    "        return (g_new, l_new, w_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, l_init, w_init), x)\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", lgtrs_y[:4])\n",
    "\n",
    "with seed(rng_seed=5):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(lgtrs(lgtrs_x[:4], lgtrs_y[:1]))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(lgtrs(lgtrs_x[:4], lgtrs_y[:4]))\n",
    "```\n",
    "\n",
    "Sample from the prior model\n",
    "\n",
    "``` python\n",
    "lgtrs_prior = Predictive(lgtrs, num_samples=500)(rng, lgtrs_x[:-10], lgtrs_y[:1])\n",
    "\n",
    "lgtrs_lvs, lgtrs_gvs = check_prior(lgtrs_prior)\n",
    "```\n",
    "\n",
    "Check global parameters\n",
    "\n",
    "``` python\n",
    "compare_global_params(lgtrs_prior,\n",
    "                      ['eta', 'kappa', 'l_init', 'w_init', 'xi'],\n",
    "                      val_ref=50000)\n",
    "```\n",
    "\n",
    "And local variables\n",
    "\n",
    "``` python\n",
    "plot_locals(lgtrs_prior, lgtrs_lvs, val_ref=50000, nrows=3, ncols=3, figsize=(8,8))\n",
    "```\n",
    "\n",
    "The outcome range has greatly increased, we now have almost half the\n",
    "samples having outcomes greater than 50000. This is way bigger than the\n",
    "range of data we'd expect from the real world. If a store has regular\n",
    "sales between 200 and 300 per day, we can be sure that that store won't\n",
    "hold an inventory of 50000. If they do, well, statistical modeling of\n",
    "their sales figure should be the last of their concern.\n",
    "\n",
    "Do we need to revise the model before we carry on doing inference? That\n",
    "depends. If we have some extra information readily available that we can\n",
    "use to make our priors more precise, that will certainly help. But this\n",
    "is above all a cost-benefit tradeoff. Does the improvement we bring to\n",
    "the model worth the effort we are putting in? It's possible that the\n",
    "information in the data likelihood will completely overwhelm the\n",
    "information in the prior, and if that is the case the extra effort spent\n",
    "in improving the priors wouldn't really worth it. Again, what's\n",
    "important is, à priori, not to limit our model to regions of the ambient\n",
    "space not covering the observed data. This doesn't seem to be the case\n",
    "here, so we'll move on.\n",
    "\n",
    "Prior prediction conditioned on data\n",
    "\n",
    "``` python\n",
    "lgtrs_prior_fixed = Predictive(lgtrs, num_samples=500)(rng, lgtrs_x[:-10], lgtrs_y[:-10])\n",
    "\n",
    "plot_locals(lgtrs_prior_fixed, lgtrs_lvs, v_ref='mu', val_ref=500,\n",
    "            nrows=3, ncols=3, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "Like before, now we chose one sample from the prior predictive samples\n",
    "and do parameter recovery on it.\n",
    "\n",
    "``` python\n",
    "n = 74\n",
    "y = lgtrs_prior['y'][n]\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "This does not look like our observed data at all. And inference.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(lgtrs)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, lgtrs_x[:-10], y)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "check_inference(mcmc, lgtrs_prior, lgtrs_gvs, n=n)\n",
    "```\n",
    "\n",
    "Although the data look quite different from the observed, we are still\n",
    "able to recover the parameters reasonably well, this should give us some\n",
    "confidence in our model. Next we do inference on the real data.\n",
    "\n",
    "``` python\n",
    "lgtrs_kernel = NUTS(lgtrs, target_accept_prob=0.95)\n",
    "lgtrs_mcmc = MCMC(lgtrs_kernel, num_warmup=2000, num_samples=2000,\n",
    "                  num_chains=4, progress_bar=False)\n",
    "lgtrs_mcmc.run(random.PRNGKey(1), lgtrs_x[:-10], lgtrs_y[:-10])\n",
    "lgtrs_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Prediction\n",
    "\n",
    "``` python\n",
    "lgtrs_sample = lgtrs_mcmc.get_samples()\n",
    "lgtrs_post = Predictive(lgtrs, posterior_samples=lgtrs_sample)(rng, lgtrs_x, lgtrs_y[:-10])\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in lgtrs_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in lgtrs_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "and visualisation\n",
    "\n",
    "``` python\n",
    "plot_post(lgtrs_y, lgtrs_post, lgtrs_lvs, nrow=3, ncol=3)\n",
    "```\n",
    "\n",
    "This time we indeed have a much more precise estimation of the standard\n",
    "deviation $\\sigma$, we can see that $\\sigma$ increases when the time\n",
    "series has a sudden change of global trend. But it is very small and for\n",
    "the prediction of future periods, the standard deviation returned to a\n",
    "flat prediction like in previous models.\n",
    "\n",
    "Compare the scores.\n",
    "\n",
    "``` python\n",
    "check_scores(lgtrs_post, lgtrs_y, 10)\n",
    "```\n",
    "\n",
    "The scores are slightly better than the previous model.\n",
    "\n",
    "In the last three models, we have used three different methods to model\n",
    "the outcome variance, and two different methods for the hidden level.\n",
    "This flexibility is the most important feature of our Bayesian modeling\n",
    "approach: we take different components capable of capturing different\n",
    "features, and freely assemble them to fit the need of a specific data\n",
    "set we want to model.\n",
    "\n",
    "In the previous models, to capture the local variation, we first used a\n",
    "Markov process, in which we modeled the local variation as the change in\n",
    "global trend; then, since we also have some predictors corresponding to\n",
    "each time period, we also added a regression component. But we didn't\n",
    "introduce seasonality to the model, because there doesn't seem to be any\n",
    "such feature present in this specific data set. However, if the data are\n",
    "indeed seasonal, we can also add new components to address this.\n",
    "\n",
    "Still, seasonality can be represented in infinitely many different ways,\n",
    "and in our modeling framework, there remains the question of how\n",
    "seasonality should be incorporated into it. Should it be an additional\n",
    "component, like the regression component we have just added, to capture\n",
    "extra local variation, or should it be an enhancing factor, that impacts\n",
    "that outcome by changing the global trend? In the next section we first\n",
    "introduce a multiplicative seasonality, which affects the outcome by\n",
    "increasing or decreasing the global trend, and in the section that\n",
    "follows, we'll introduce an additive seasonality as an independent\n",
    "component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9255608",
   "metadata": {},
   "source": [
    "## Adding multiplicative seasonality to the model\n",
    "\n",
    "For this model we'll use the monthly air Passengers data, which contains\n",
    "144 observations of passenger counts. We'll use the first 120 points for\n",
    "modeling and the last 24 for prediction.\n",
    "\n",
    "Although the passenger counts are clearly integers, the outcome of the\n",
    "Student's T distribution is real number. Later we might try another\n",
    "heavy tailed integer valued distribution, such as Negative Binomial, to\n",
    "better fit the observation.\n",
    "\n",
    "``` python\n",
    "sgt_y = jnp.array(air_passengers, dtype='float64')\n",
    "\n",
    "plt.plot(air_passengers, '.')\n",
    "plt.title('Monthly Air passengers');\n",
    "```\n",
    "\n",
    "In this model we'll replace the local trend with a seasonal effect.\n",
    "Specifically, we'll start with a multiplicative model in this section\n",
    "and explore the additive one in the next.\n",
    "\n",
    "This model assumes the number of periods for seasonality is already\n",
    "known and thus not inferred from the data. This works with datasets with\n",
    "natural seasonality, which is the case here with monthly data.\n",
    "\n",
    "Because this is a multiplicative model, when computing the global trend\n",
    "and seasonality, we have to remove from the outcome the respective\n",
    "effects of each other, that's why we have the divisions in the smoothed\n",
    "average process.\n",
    "\n",
    "``` python\n",
    "def sgt(y, T, future=3):\n",
    "    N = y.shape[0]\n",
    "    nu = sample(\"nu\", dist.Uniform(1, 50))\n",
    "\n",
    "    xi = sample(\"xi\", dist.HalfNormal(2))\n",
    "    tau = sample(\"tau\", dist.Beta(1, 4))\n",
    "    kappa = sample(\"kappa\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.TruncatedCauchy(y[0], 10, low=0))\n",
    "\n",
    "    with plate(\"T\", size=T):\n",
    "        s_init = sample(\"s_init\", dist.HalfNormal(4))\n",
    "\n",
    "    def transition_fn(carry, t):\n",
    "        g, s = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        mu = deterministic(\"mu\", jnp.clip(G * s[0], 0))\n",
    "        sigma = deterministic(\"sigma\", xi + kappa * mu ** tau)\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * (yt/s[0]) + (1-alpha) * G, 0.1))\n",
    "        su = deterministic(\"su\", jnp.clip(beta * (yt/G) + (1 - beta) * s[0], 0.1))\n",
    "        s_new = deterministic(\"s\", jnp.concatenate([s[1:], su[None]], axis=0))\n",
    "\n",
    "        return (g_new, s_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, s_init),  jnp.arange(0, N+future))\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", sgt_y[:9])\n",
    "\n",
    "with seed(rng_seed=3):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(sgt(sgt_y[:1], T=3, future=8))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(sgt(sgt_y[:9], T=3, future=0))\n",
    "```\n",
    "\n",
    "Sample from prior\n",
    "\n",
    "``` python\n",
    "sgt_prior = Predictive(sgt, num_samples=500)(rng, sgt_y[:1], T=12, future=119)\n",
    "\n",
    "sgt_lvs, sgt_gvs = check_prior(sgt_prior)\n",
    "sgt_lvs.remove('s')\n",
    "```\n",
    "\n",
    "Check global parameters\n",
    "\n",
    "``` python\n",
    "compare_global_params(sgt_prior,\n",
    "                      ['alpha', 'beta', 'gamma', 'kappa', 'rho', 'tau'],\n",
    "                      val_ref=10000)\n",
    "```\n",
    "\n",
    "And local variables\n",
    "\n",
    "``` python\n",
    "plot_locals(sgt_prior, sgt_lvs, val_ref=50000, nrows=3, ncols=2, figsize=(8,8))\n",
    "```\n",
    "\n",
    "Prior prediction conditioned on data\n",
    "\n",
    "``` python\n",
    "sgt_prior_fixed = Predictive(sgt, num_samples=500)(rng, sgt_y[:-24], T=12, future=0)\n",
    "\n",
    "plot_locals(sgt_prior_fixed, sgt_lvs, v_ref='sigma', val_ref=100,\n",
    "            nrows=3, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "Like before, now we chose one sample from the prior predictive samples\n",
    "and do parameter recovery on it.\n",
    "\n",
    "``` python\n",
    "n = 34\n",
    "y = sgt_prior['y'][n]\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "And inference.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(sgt)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, y, 12, 0)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "check_inference(mcmc, sgt_prior, sgt_gvs, n=n)\n",
    "```\n",
    "\n",
    "This time, let's also look at the prediction.\n",
    "\n",
    "``` python\n",
    "samples = mcmc.get_samples()\n",
    "post = Predictive(sgt, posterior_samples=samples)(rng, y, T=12, future=0)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "And visualisation.\n",
    "\n",
    "``` python\n",
    "plot_post(y, post, sgt_lvs)\n",
    "```\n",
    "\n",
    "This looks passable enough. Next we do inference on the real data.\n",
    "\n",
    "``` python\n",
    "sgt_kernel = NUTS(sgt)\n",
    "sgt_mcmc = MCMC(sgt_kernel, num_warmup=2000, num_samples=2000,\n",
    "                num_chains=4, progress_bar=False)\n",
    "sgt_mcmc.run(rng, sgt_y[:-24], 12, 0)\n",
    "sgt_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Now we can make predictions for future periods.\n",
    "\n",
    "``` python\n",
    "sgt_sample = sgt_mcmc.get_samples()\n",
    "sgt_post = Predictive(sgt, posterior_samples=sgt_sample)(rng, sgt_y[:-24], T=12, future=24)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in sgt_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in sgt_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "Let's look at the model prediction.\n",
    "\n",
    "``` python\n",
    "plot_post(sgt_y, sgt_post, sgt_lvs)\n",
    "```\n",
    "\n",
    "The model has been able to capture the growing global trend, and the\n",
    "seasonality present in it. However, looking at the prediction for future\n",
    "periods, it looks clear that the model has been systematically\n",
    "underestimating the outcome.\n",
    "\n",
    "We have saw similar problem before, in the first LGT model. Back then,\n",
    "the model is also underestimating, and because we have another predictor\n",
    "variable, we have been able to use it to compensate for some extra local\n",
    "information. With this data set, however, we don't have any predictor\n",
    "available.\n",
    "\n",
    "One way we can get around this, is to simply use lagged outcomes to\n",
    "construct a predictor matrix, and add a regression component to the\n",
    "model as before. However, there is another way to make use of local\n",
    "information, and that is using a moving window of outcomes. In a later\n",
    "model, we'll use this moving window information to compute the global\n",
    "trend. But first, we'll return to, as promised, an additive approach to\n",
    "modeling seasonality.\n",
    "\n",
    "Finally, look at the scores.\n",
    "\n",
    "``` python\n",
    "check_scores(sgt_post, sgt_y, 24)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496697b",
   "metadata": {},
   "source": [
    "## Using additive seasonality\n",
    "\n",
    "Next we replace the multiplicative seasonality with an additive one. For\n",
    "the multiplicative seasonality, we assume it affects the outcome by\n",
    "enhancing or decreasing the global trend, an amplifying factor, so it\n",
    "naturally only takes positive values. But for the additive seasonality,\n",
    "we assume it adds or deducts from the global trend, so it can be both\n",
    "positive and negative.\n",
    "\n",
    "``` python\n",
    "sgta_y = sgt_y\n",
    "```\n",
    "\n",
    "The model can be represented as:\n",
    "\n",
    "We'll put a relatively wide prior on the seasonality effect. And because\n",
    "updating the seasonality effect needs the outcome before, we need to\n",
    "keep at least T periods of outcomes in memory, when doing the update.\n",
    "\n",
    "``` python\n",
    "def sgta(y, T, future=3):\n",
    "    N = y.shape[0]\n",
    "    nu = sample(\"nu\", dist.Uniform(1, 50))\n",
    "\n",
    "    xi = sample(\"xi\", dist.HalfNormal(2))\n",
    "    tau = sample(\"tau\", dist.Beta(1, 3))\n",
    "    kappa = sample(\"kappa\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.TruncatedCauchy(y[0], 10,low=0))\n",
    "\n",
    "    with plate(\"T\", size=T):\n",
    "        s_init = sample(\"s_init\", dist.Cauchy(0, 10))\n",
    "\n",
    "    def transition_fn(carry, t):\n",
    "        g, s = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        mu = deterministic(\"mu\", jnp.clip(G + s[0], 0))\n",
    "        sigma = deterministic(\"sigma\", xi + kappa * mu ** tau)\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * (yt-s[0]) + (1-alpha) * g, 0.1))\n",
    "        su = deterministic(\"su\", beta * (yt-G) + (1-beta) * s[0])\n",
    "        s_new = deterministic(\"s\", jnp.concatenate([s[1:], su[None]], axis=0))\n",
    "\n",
    "        return (g_new, s_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, s_init),  jnp.arange(0, N+future))\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", sgta_y[:9])\n",
    "\n",
    "with seed(rng_seed=3):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(sgta(sgta_y[:1], T=3, future=8))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(sgta(sgta_y[:9], T=3, future=0))\n",
    "```\n",
    "\n",
    "Sample from prior\n",
    "\n",
    "``` python\n",
    "sgta_prior = Predictive(sgta, num_samples=500)(rng, sgta_y[:1], T=12, future=119)\n",
    "\n",
    "sgta_lvs, sgta_gvs = check_prior(sgta_prior)\n",
    "sgta_lvs.remove('s')\n",
    "```\n",
    "\n",
    "Check global parameters\n",
    "\n",
    "``` python\n",
    "compare_global_params(sgta_prior,\n",
    "                      ['alpha', 'beta', 'gamma', 'kappa', 'rho', 'tau'],\n",
    "                      val_ref=10000)\n",
    "```\n",
    "\n",
    "Nothing special. And local variables\n",
    "\n",
    "``` python\n",
    "plot_locals(sgta_prior, sgta_lvs, val_ref=1000, nrows=3, ncols=2, figsize=(8,8))\n",
    "```\n",
    "\n",
    "Prior prediction conditioned on data\n",
    "\n",
    "``` python\n",
    "sgta_prior_fixed = Predictive(sgta, num_samples=500)(rng, sgta_y[:-24], T=12, future=0)\n",
    "\n",
    "plot_locals(sgta_prior_fixed, sgta_lvs, v_ref='mu', val_ref=500, nrows=3, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "This prior predictive distribution, conditioned on data, looks a bit\n",
    "problematic because as we can see, the seasonality is still present in\n",
    "the global trend, which defeats our intention of separating the two.\n",
    "Later we may want to find better updating mechanisms for them.\n",
    "\n",
    "Like before, now we chose one sample from the prior predictive samples\n",
    "and do parameter recovery on it.\n",
    "\n",
    "``` python\n",
    "n = 24\n",
    "y = sgta_prior['y'][n]\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "And inference.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(sgta)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, y, 12, 0)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "check_inference(mcmc, sgta_prior, sgta_gvs, n=n)\n",
    "```\n",
    "\n",
    "Seems passable, though certainly not very precise, and there are a few\n",
    "initial seasonality values a bit far from true value. Let's also look at\n",
    "the prediction.\n",
    "\n",
    "``` python\n",
    "samples = mcmc.get_samples()\n",
    "post = Predictive(sgta, posterior_samples=samples)(rng, y, T=12, future=0)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "And visualisation.\n",
    "\n",
    "``` python\n",
    "plot_post(y, post, sgta_lvs)\n",
    "```\n",
    "\n",
    "The prediction looks alright but there is a visible increasing trend in\n",
    "the seasonality component, this again confirms the problem we've spotted\n",
    "in the prior predictive check, that in this model the global trend and\n",
    "seasonality are not completely separated. Next we do inference on the\n",
    "real data.\n",
    "\n",
    "Next we do inference on the real data.\n",
    "\n",
    "``` python\n",
    "sgta_kernel = NUTS(sgta, target_accept_prob=0.95)\n",
    "sgta_mcmc = MCMC(sgta_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False)\n",
    "sgta_mcmc.run(rng, sgta_y[:-24], 12, 0)\n",
    "sgta_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "And predictions.\n",
    "\n",
    "``` python\n",
    "sgta_sample = sgta_mcmc.get_samples()\n",
    "sgta_post = Predictive(sgta, posterior_samples=sgta_sample)(rng, sgta_y[:-24], T=12, future=24)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in sgta_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in sgta_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "Let's look at the model prediction.\n",
    "\n",
    "``` python\n",
    "plot_post(sgta_y, sgta_post, sgta_lvs, nrow=3, ncol=2)\n",
    "```\n",
    "\n",
    "Compared to the previous multiplicative seasonality, although the\n",
    "predictions for the individual components are quite different, but the\n",
    "final predictions for the outcome are quite similar. And as a model\n",
    "defect, we can see that both in this and the previous model, we are\n",
    "slightly underestimating the outcome.\n",
    "\n",
    "Let's also check the scores.\n",
    "\n",
    "``` python\n",
    "check_scores(sgta_post, sgta_y, 24)\n",
    "```\n",
    "\n",
    "The scores are also similar to the previous model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c6e0e",
   "metadata": {},
   "source": [
    "## Calculating hidden level with moving average\n",
    "\n",
    "There are, of course, infinite ways to model the hidden level in the\n",
    "time series. Up to this point we have been using the current outcome to\n",
    "calculate a weighted average, but we can also use a moving window of\n",
    "past outcomes to do the calculation. In the previous models the outcome\n",
    "seems to be underestimated, so we also have to improve a little bit on\n",
    "this issue, by using more local information. Because the model is\n",
    "seasonal, the natural choice for window width is the seasonality.\n",
    "\n",
    "``` python\n",
    "sgtm_y = sgt_y\n",
    "```\n",
    "\n",
    "We'll continue to use the additive seasonality so the only change lies\n",
    "in how we update the hidden level:\n",
    "\n",
    "The moving average is computed with the outcome of the previous T\n",
    "periods, but there are less than T previous outcomes for the first T\n",
    "periods, so for these first T periods we'll calculate the moving average\n",
    "with all the outcomes available. When coding the model, it's easier to\n",
    "keep track of the moving window rather than the moving average, and\n",
    "that's how we are going to code our model.\n",
    "\n",
    "``` python\n",
    "def sgtm(y, T, future=3):\n",
    "    N = y.shape[0]\n",
    "    nu = sample(\"nu\", dist.Uniform(1, 50))\n",
    "\n",
    "    xi = sample(\"xi\", dist.HalfNormal(2))\n",
    "    tau = sample(\"tau\", dist.Beta(1, 3))\n",
    "    kappa = sample(\"kappa\", dist.HalfNormal(2))\n",
    "\n",
    "    gamma = sample(\"gamma\", dist.HalfNormal(2))\n",
    "    rho = sample(\"rho\", dist.Beta(1, 4))\n",
    "\n",
    "    alpha = sample(\"alpha\", dist.Beta(2, 2))\n",
    "    beta = sample(\"beta\", dist.Beta(2, 2))\n",
    "\n",
    "    g_init = sample(\"g_init\", dist.TruncatedCauchy(y[0], 10, low=0))\n",
    "\n",
    "    with plate(\"T\", size=T):\n",
    "        s_init = sample(\"s_init\", dist.Cauchy(0, 10))\n",
    "\n",
    "    def transition_fn(carry, t):\n",
    "        g, s, mw = carry\n",
    "        G = deterministic(\"G\", g + gamma * g ** rho)\n",
    "        mu = deterministic(\"mu\", jnp.clip(G + s[0], 0))\n",
    "        sigma = deterministic(\"sigma\", xi + kappa * mu ** tau)\n",
    "\n",
    "        yt = sample(\"y\", dist.StudentT(nu, mu, sigma))\n",
    "\n",
    "        mw_new = jnp.concatenate([mw[1:], yt[None]], axis=0)\n",
    "        ma = jnp.where(t<T, mw_new.sum()/(t+1), mw_new.sum()/T)\n",
    "        g_new = deterministic(\"g\", jnp.clip(alpha * (ma-s[0]) + (1-alpha) * G, 0))\n",
    "\n",
    "        su = deterministic(\"su\", beta * (yt-G) + (1-beta) * s[0])\n",
    "        s_new = deterministic(\"s\", jnp.concatenate([s[1:], su[None]], axis=0))\n",
    "\n",
    "        return (g_new, s_new, mw_new), yt\n",
    "\n",
    "    with condition(data={'y':y}):\n",
    "        _, ys = scan(transition_fn, (g_init, s_init, jnp.zeros(T)),  jnp.arange(0, N+future))\n",
    "\n",
    "    return ys\n",
    "\n",
    "print(\"Observed data:\", sgtm_y[:9])\n",
    "\n",
    "with seed(rng_seed=3):\n",
    "    print(\"\\nPrior sampling:\")\n",
    "    print(sgtm(sgtm_y[:1], T=3, future=8))\n",
    "    print(\"\\nFixed data sampling:\")\n",
    "    print(sgtm(sgtm_y[:9], T=3, future=0))\n",
    "```\n",
    "\n",
    "Sample from prior\n",
    "\n",
    "``` python\n",
    "sgtm_prior = Predictive(sgtm, num_samples=500)(rng, sgtm_y[:1], T=12, future=119)\n",
    "\n",
    "sgtm_lvs, sgtm_gvs = check_prior(sgtm_prior)\n",
    "sgtm_lvs.remove('s')\n",
    "```\n",
    "\n",
    "Check global parameters\n",
    "\n",
    "``` python\n",
    "compare_global_params(sgtm_prior,\n",
    "                      ['alpha', 'beta', 'gamma', 'kappa', 'rho', 'tau'],\n",
    "                      val_ref=10000)\n",
    "```\n",
    "\n",
    "And local variables. We limit the range of outcome values to focus on\n",
    "the region most useful to our inference.\n",
    "\n",
    "``` python\n",
    "plot_locals(sgtm_prior, sgtm_lvs, val_ref=1000, nrows=3, ncols=2, figsize=(8,8))\n",
    "```\n",
    "\n",
    "Prior prediction conditioned on data\n",
    "\n",
    "``` python\n",
    "sgtm_prior_fixed = Predictive(sgtm, num_samples=500)(rng, sgtm_y[:-24], T=12, future=0)\n",
    "\n",
    "plot_locals(sgtm_prior_fixed, sgtm_lvs, v_ref='mu', val_ref=600, nrows=3, ncols=2, figsize=(8, 8))\n",
    "```\n",
    "\n",
    "Like before, now we chose one sample from the prior predictive samples\n",
    "and do parameter recovery on it.\n",
    "\n",
    "``` python\n",
    "n = 34\n",
    "y = sgtm_prior['y'][n]\n",
    "plt.plot(y)\n",
    "plt.title('Fake outcome no. {} from prior'.format(n));\n",
    "```\n",
    "\n",
    "And inference.\n",
    "\n",
    "``` python\n",
    "kernel = NUTS(sgtm)\n",
    "mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False)\n",
    "mcmc.run(rng, y, 12, 0)\n",
    "mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Let's visualise the parameter posterior.\n",
    "\n",
    "``` python\n",
    "check_inference(mcmc, sgtm_prior, sgtm_gvs, n=n)\n",
    "```\n",
    "\n",
    "And the prediction.\n",
    "\n",
    "``` python\n",
    "samples = mcmc.get_samples()\n",
    "post = Predictive(sgtm, posterior_samples=samples)(rng, y, T=12, future=0)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "And visualisation.\n",
    "\n",
    "``` python\n",
    "plot_post(y, post, sgtm_lvs)\n",
    "```\n",
    "\n",
    "This looks passable enough. Next we do inference on the real data.\n",
    "\n",
    "``` python\n",
    "sgtm_kernel = NUTS(sgtm)\n",
    "sgtm_mcmc = MCMC(sgtm_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False)\n",
    "sgtm_mcmc.run(rng, sgtm_y[:-24], 12, 0)\n",
    "sgtm_mcmc.print_summary()\n",
    "```\n",
    "\n",
    "Now we can make predictions for future periods.\n",
    "\n",
    "``` python\n",
    "sgtm_sample = sgtm_mcmc.get_samples()\n",
    "sgtm_post = Predictive(sgtm, posterior_samples=sgtm_sample)(rng, sgtm_y[:-24], T=12, future=24)\n",
    "\n",
    "print(\"posterior samples:\")\n",
    "for k, v in sgtm_sample.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "\n",
    "print(\"\\n\\nposterior Prediction samples:\")\n",
    "for k, v in sgtm_post.items():\n",
    "    print(k, v.shape, end='; ')\n",
    "```\n",
    "\n",
    "Let's look at the model prediction.\n",
    "\n",
    "``` python\n",
    "plot_post(sgtm_y, sgtm_post, sgtm_lvs, ncol=2)\n",
    "```\n",
    "\n",
    "Here the prediction is slightly better than before, the underestimation\n",
    "problem seems abatted, but we're spotting another problem. The global\n",
    "trend still has a very strong seasonality effect built in it. Upon some\n",
    "reflection it's not difficult to understand why: we're now using the\n",
    "moving average to calculate the global trend, and since the moving\n",
    "average uses all the outcome for one cycle, the seasonality is clearly\n",
    "present in it.\n",
    "\n",
    "And finally the scores.\n",
    "\n",
    "``` python\n",
    "check_scores(sgtm_post, sgtm_y, 24)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21fa12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post we studied some time series models for global trend, local\n",
    "variation, and seasonality. Our goal is not to build the perfect model;\n",
    "rather, our goal is to build models using composable components, observe\n",
    "their effects, and make changes when necessary, so that we can model the\n",
    "specific characteristics in each data set.\n",
    "\n",
    "The models are all based on a smoothed exponential process, which is\n",
    "quite flexible in its wide range of behaviours, but sometimes when\n",
    "modeling data with strong pattern this flexibility might not be\n",
    "necessary.\n",
    "\n",
    "The original Rlgt package also includes a double seasonality model, and\n",
    "a seasonality modeled by an extra exponentially smoothed process, these\n",
    "models are not implemented here, because they don't enhance the\n",
    "performance, and induce more identifiability issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jax_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1555e325bc5288a9dc4df98b620a48cd5e1288075b9e35863e020868891fac9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
