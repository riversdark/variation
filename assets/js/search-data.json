{
  
    
        "post0": {
            "title": "ColabFold, AlphaFold2 using MMseqs2",
            "content": ". This notebook is used to understand how it works; to use ColabFold for structure prediction please refer to the original Colab notebook. . from google.colab import files import os.path import re import hashlib import random . Input protein sequence(s) . Use : to specify inter-protein chainbreaks for modeling complexes (supports homo- and hetro-oligomers). For example PI...SK:PI...SK for a homodimer . def add_hash(x,y): return x+&quot;_&quot;+hashlib.sha1(y.encode()).hexdigest()[:5] query_sequence = &#39;PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK&#39; jobname = &#39;test&#39; . remove whitespaces . query_sequence = &quot;&quot;.join(query_sequence.split()) basejobname = &quot;&quot;.join(jobname.split()) basejobname = re.sub(r&#39; W+&#39;, &#39;&#39;, basejobname) . jobname = add_hash(basejobname, query_sequence) . while os.path.isfile(f&quot;{jobname}.csv&quot;): jobname = add_hash(basejobname, &#39;&#39;.join(random.sample(query_sequence,len(query_sequence)))) with open(f&quot;{jobname}.csv&quot;, &quot;w&quot;) as text_file: text_file.write(f&quot;id,sequence n{jobname},{query_sequence}&quot;) queries_path=f&quot;{jobname}.csv&quot; . number of models to use . templates to use: [&quot;none&quot;, &quot;pdb70&quot;,&quot;custom&quot;] . &quot;none&quot; = no template information is used, | &quot;pdb70&quot; = detect templates in pdb70, | &quot;custom&quot; - upload and search own templates (PDB or mmCIF format, see notes below) | . use_amber = False template_mode = &quot;none&quot; if template_mode == &quot;pdb70&quot;: use_templates = True custom_template_path = None elif template_mode == &quot;custom&quot;: custom_template_path = f&quot;{jobname}_template&quot; os.mkdir(custom_template_path) uploaded = files.upload() use_templates = True for fn in uploaded.keys(): os.rename(fn, f&quot;{jobname}_template/{fn}&quot;) else: custom_template_path = None use_templates = False . MSA options (custom MSA upload, single sequence, pairing mode) . mas modes: [&quot;MMseqs2 (UniRef+Environmental)&quot;, &quot;MMseqs2 (UniRef only)&quot;,&quot;single_sequence&quot;,&quot;custom&quot;] . msa_mode = &quot;MMseqs2 (UniRef+Environmental)&quot; . pair modes: [&quot;unpaired+paired&quot;,&quot;paired&quot;,&quot;unpaired&quot;] . &quot;unpaired+paired&quot; = pair sequences from same species + unpaired MSA, | &quot;unpaired&quot; = seperate MSA for each chain, | &quot;paired&quot; - only use paired sequences. | . pair_mode = &quot;unpaired+paired&quot; . decide which a3m to use . if msa_mode.startswith(&quot;MMseqs2&quot;): a3m_file = f&quot;{jobname}.a3m&quot; elif msa_mode == &quot;custom&quot;: a3m_file = f&quot;{jobname}.custom.a3m&quot; if not os.path.isfile(a3m_file): custom_msa_dict = files.upload() custom_msa = list(custom_msa_dict.keys())[0] header = 0 import fileinput for line in fileinput.FileInput(custom_msa,inplace=1): if line.startswith(&quot;&gt;&quot;): header = header + 1 if not line.rstrip(): continue if line.startswith(&quot;&gt;&quot;) == False and header == 1: query_sequence = line.rstrip() print(line, end=&#39;&#39;) os.rename(custom_msa, a3m_file) queries_path=a3m_file print(f&quot;moving {custom_msa} to {a3m_file}&quot;) else: a3m_file = f&quot;{jobname}.single_sequence.a3m&quot; with open(a3m_file, &quot;w&quot;) as text_file: text_file.write(&quot;&gt;1 n%s&quot; % query_sequence) . model type: [&quot;auto&quot;, &quot;AlphaFold2-ptm&quot;, &quot;AlphaFold2-multimer-v1&quot;, &quot;AlphaFold2-multimer-v2&quot;] . &quot;auto&quot; = protein structure prediction using &quot;AlphaFold2-ptm&quot; and complex prediction &quot;AlphaFold-multimer-v2&quot;. For complexes &quot;AlphaFold-multimer-v[1,2]&quot; and &quot;AlphaFold-ptm&quot; can be used. . model_type = &quot;auto&quot; . number of recycles: [1,3,6,12,24,48] {type:&quot;raw&quot;} . num_recycles = 3 . whether to save prediction results to Google Drive . if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive . save_to_google_drive = False . set dpi for image resolution . dpi = 200 . if save_to_google_drive: from pydrive.drive import GoogleDrive from pydrive.auth import GoogleAuth from google.colab import auth from oauth2client.client import GoogleCredentials auth.authenticate_user() gauth = GoogleAuth() gauth.credentials = GoogleCredentials.get_application_default() drive = GoogleDrive(gauth) print(&quot;You are logged into Google Drive and are good to go!&quot;) . Install dependencies . %%bash -s $use_amber $use_templates set -e USE_AMBER=$1 USE_TEMPLATES=$2 if [ ! -f COLABFOLD_READY ]; then # install dependencies # We have to use &quot;--no-warn-conflicts&quot; because colab already has a lot preinstalled with requirements different to ours pip install -q --no-warn-conflicts &quot;colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold&quot; # high risk high gain pip install -q &quot;jax[cuda11_cudnn805]&gt;=0.3.8,&lt;0.4&quot; -f https://storage.googleapis.com/jax-releases/jax_releases.html touch COLABFOLD_READY fi # setup conda if [ ${USE_AMBER} == &quot;True&quot; ] || [ ${USE_TEMPLATES} == &quot;True&quot; ]; then if [ ! -f CONDA_READY ]; then wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2&gt;&amp;1 1&gt;/dev/null rm Miniconda3-latest-Linux-x86_64.sh touch CONDA_READY fi fi # setup template search if [ ${USE_TEMPLATES} == &quot;True&quot; ] &amp;&amp; [ ! -f HH_READY ]; then conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=3.7 2&gt;&amp;1 1&gt;/dev/null touch HH_READY fi # setup openmm for amber refinement if [ ${USE_AMBER} == &quot;True&quot; ] &amp;&amp; [ ! -f AMBER_READY ]; then conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2&gt;&amp;1 1&gt;/dev/null touch AMBER_READY fi . Run Prediction . import sys from colabfold.download import download_alphafold_params, default_data_dir from colabfold.utils import setup_logging from colabfold.batch import get_queries, run, set_model_type K80_chk = !nvidia-smi | grep &quot;Tesla K80&quot; | wc -l if &quot;1&quot; in K80_chk: print(&quot;WARNING: found GPU Tesla K80: limited to total length &lt; 1000&quot;) if &quot;TF_FORCE_UNIFIED_MEMORY&quot; in os.environ: del os.environ[&quot;TF_FORCE_UNIFIED_MEMORY&quot;] if &quot;XLA_PYTHON_CLIENT_MEM_FRACTION&quot; in os.environ: del os.environ[&quot;XLA_PYTHON_CLIENT_MEM_FRACTION&quot;] . from colabfold.colabfold import plot_protein from pathlib import Path import matplotlib.pyplot as plt . # For some reason we need that to get pdbfixer to import if use_amber and &#39;/usr/local/lib/python3.7/site-packages/&#39; not in sys.path: sys.path.insert(0, &#39;/usr/local/lib/python3.7/site-packages/&#39;) . def prediction_callback(unrelaxed_protein, length, prediction_result, input_features, type): fig = plot_protein(unrelaxed_protein, Ls=length, dpi=150) plt.show() plt.close() . result_dir=&quot;.&quot; setup_logging(Path(&quot;.&quot;).joinpath(&quot;log.txt&quot;)) queries, is_complex = get_queries(queries_path) model_type = set_model_type(is_complex, model_type) download_alphafold_params(model_type, Path(&quot;.&quot;)) . run( queries=queries, result_dir=result_dir, use_templates=use_templates, custom_template_path=custom_template_path, use_amber=use_amber, msa_mode=msa_mode, model_type=model_type, num_models=5, num_recycles=num_recycles, model_order=[1, 2, 3, 4, 5], is_complex=is_complex, data_dir=Path(&quot;.&quot;), keep_existing_results=False, recompile_padding=1.0, rank_by=&quot;auto&quot;, pair_mode=pair_mode, stop_at_score=float(100), prediction_callback=prediction_callback, dpi=dpi ) . 2022-07-19 03:21:31,930 Found 5 citations for tools or databases 2022-07-19 03:21:40,470 Query 1/1: test_ce6be (length 59) . COMPLETE: 100%|██████████| 150/150 [elapsed: 00:01 remaining: 00:00] . 2022-07-19 03:21:42,989 Running model_1 2022-07-19 03:22:35,784 model_1 took 45.5s (3 recycles) with pLDDT 95.5 and ptmscore 0.752 . 2022-07-19 03:22:41,951 Running model_2 2022-07-19 03:23:00,695 model_2 took 16.7s (3 recycles) with pLDDT 96.4 and ptmscore 0.764 . 2022-07-19 03:23:06,363 Running model_3 2022-07-19 03:23:25,141 model_3 took 16.7s (3 recycles) with pLDDT 97.4 and ptmscore 0.782 . 2022-07-19 03:23:31,634 Running model_4 2022-07-19 03:23:50,219 model_4 took 16.5s (3 recycles) with pLDDT 96.8 and ptmscore 0.775 . 2022-07-19 03:23:55,579 Running model_5 2022-07-19 03:24:14,273 model_5 took 16.7s (3 recycles) with pLDDT 96.2 and ptmscore 0.776 . 2022-07-19 03:24:19,591 reranking models by plddt 2022-07-19 03:24:21,169 Done . Display 3D structure {run: &quot;auto&quot;} . import py3Dmol import glob import matplotlib.pyplot as plt from colabfold.colabfold import plot_plddt_legend . rank number: [&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;] {type:&quot;raw&quot;} . rank_num = 1 . color theme [&quot;chain&quot;, &quot;lDDT&quot;, &quot;rainbow&quot;] . color = &quot;lDDT&quot; . show_sidechains = False show_mainchains = False . jobname_prefix = &quot;.custom&quot; if msa_mode == &quot;custom&quot; else &quot;&quot; if use_amber: pdb_filename = f&quot;{jobname}{jobname_prefix}_relaxed_rank_{rank_num}_model_*.pdb&quot; else: pdb_filename = f&quot;{jobname}{jobname_prefix}_unrelaxed_rank_{rank_num}_model_*.pdb&quot; pdb_file = glob.glob(pdb_filename) def show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=&quot;lDDT&quot;): model_name = f&quot;rank_{rank_num}&quot; view = py3Dmol.view(js=&#39;https://3dmol.org/build/3Dmol.js&#39;,) view.addModel(open(pdb_file[0],&#39;r&#39;).read(),&#39;pdb&#39;) if color == &quot;lDDT&quot;: view.setStyle({&#39;cartoon&#39;: {&#39;colorscheme&#39;: {&#39;prop&#39;:&#39;b&#39;,&#39;gradient&#39;: &#39;roygb&#39;,&#39;min&#39;:50,&#39;max&#39;:90}}}) elif color == &quot;rainbow&quot;: view.setStyle({&#39;cartoon&#39;: {&#39;color&#39;:&#39;spectrum&#39;}}) elif color == &quot;chain&quot;: chains = len(queries[0][1]) + 1 if is_complex else 1 for n,chain,color in zip(range(chains),list(&quot;ABCDEFGH&quot;), [&quot;lime&quot;,&quot;cyan&quot;,&quot;magenta&quot;,&quot;yellow&quot;,&quot;salmon&quot;,&quot;white&quot;,&quot;blue&quot;,&quot;orange&quot;]): view.setStyle({&#39;chain&#39;:chain},{&#39;cartoon&#39;: {&#39;color&#39;:color}}) if show_sidechains: BB = [&#39;C&#39;,&#39;O&#39;,&#39;N&#39;] view.addStyle({&#39;and&#39;:[{&#39;resn&#39;:[&quot;GLY&quot;,&quot;PRO&quot;],&#39;invert&#39;:True},{&#39;atom&#39;:BB,&#39;invert&#39;:True}]}, {&#39;stick&#39;:{&#39;colorscheme&#39;:f&quot;WhiteCarbon&quot;,&#39;radius&#39;:0.3}}) view.addStyle({&#39;and&#39;:[{&#39;resn&#39;:&quot;GLY&quot;},{&#39;atom&#39;:&#39;CA&#39;}]}, {&#39;sphere&#39;:{&#39;colorscheme&#39;:f&quot;WhiteCarbon&quot;,&#39;radius&#39;:0.3}}) view.addStyle({&#39;and&#39;:[{&#39;resn&#39;:&quot;PRO&quot;},{&#39;atom&#39;:[&#39;C&#39;,&#39;O&#39;],&#39;invert&#39;:True}]}, {&#39;stick&#39;:{&#39;colorscheme&#39;:f&quot;WhiteCarbon&quot;,&#39;radius&#39;:0.3}}) if show_mainchains: BB = [&#39;C&#39;,&#39;O&#39;,&#39;N&#39;,&#39;CA&#39;] view.addStyle({&#39;atom&#39;:BB},{&#39;stick&#39;:{&#39;colorscheme&#39;:f&quot;WhiteCarbon&quot;,&#39;radius&#39;:0.3}}) view.zoomTo() return view . show_pdb(rank_num,show_sidechains, show_mainchains, color).show() if color == &quot;lDDT&quot;: plot_plddt_legend().show() . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . from IPython.display import display, HTML import base64 from html import escape # see: https://stackoverflow.com/a/53688522 def image_to_data_url(filename): ext = filename.split(&#39;.&#39;)[-1] prefix = f&#39;data:image/{ext};base64,&#39; with open(filename, &#39;rb&#39;) as f: img = f.read() return prefix + base64.b64encode(img).decode(&#39;utf-8&#39;) pae = image_to_data_url(f&quot;{jobname}{jobname_prefix}_PAE.png&quot;) cov = image_to_data_url(f&quot;{jobname}{jobname_prefix}_coverage.png&quot;) plddt = image_to_data_url(f&quot;{jobname}{jobname_prefix}_plddt.png&quot;) display(HTML(f&quot;&quot;&quot; &lt;style&gt; img {{ float:left; }} .full {{ max-width:100%; }} .half {{ max-width:50%; }} @media (max-width:640px) {{ .half {{ max-width:100%; }} }} &lt;/style&gt; &lt;div style=&quot;max-width:90%; padding:2em;&quot;&gt; &lt;h1&gt;Plots for {escape(jobname)}&lt;/h1&gt; &lt;img src=&quot;{pae}&quot; class=&quot;full&quot; /&gt; &lt;img src=&quot;{cov}&quot; class=&quot;half&quot; /&gt; &lt;img src=&quot;{plddt}&quot; class=&quot;half&quot; /&gt; &lt;/div&gt; &quot;&quot;&quot;)) . Plots for test_ce6be . Package and download results . If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: jobname.result.zip, right-click and select &quot;Download &quot; (see screenshot). . if msa_mode == &quot;custom&quot;: print(&quot;Don&#39;t forget to cite your custom MSA generation method.&quot;) !zip -FSr $jobname&quot;.result.zip&quot; config.json $jobname*&quot;.json&quot; $jobname*&quot;.a3m&quot; $jobname*&quot;relaxed_rank_&quot;*&quot;.pdb&quot; &quot;cite.bibtex&quot; $jobname*&quot;.png&quot; files.download(f&quot;{jobname}.result.zip&quot;) if save_to_google_drive == True and drive: uploaded = drive.CreateFile({&#39;title&#39;: f&quot;{jobname}.result.zip&quot;}) uploaded.SetContentFile(f&quot;{jobname}.result.zip&quot;) uploaded.Upload() print(f&quot;Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get(&#39;id&#39;)}&quot;) . adding: config.json (deflated 49%) adding: test_ce6be_predicted_aligned_error_v1.json (deflated 89%) adding: test_ce6be_unrelaxed_rank_1_model_3_scores.json (deflated 72%) adding: test_ce6be_unrelaxed_rank_2_model_4_scores.json (deflated 72%) adding: test_ce6be_unrelaxed_rank_3_model_2_scores.json (deflated 72%) adding: test_ce6be_unrelaxed_rank_4_model_5_scores.json (deflated 72%) adding: test_ce6be_unrelaxed_rank_5_model_1_scores.json (deflated 71%) adding: test_ce6be.a3m (deflated 60%) adding: test_ce6be_unrelaxed_rank_1_model_3.pdb (deflated 78%) adding: test_ce6be_unrelaxed_rank_2_model_4.pdb (deflated 78%) adding: test_ce6be_unrelaxed_rank_3_model_2.pdb (deflated 78%) adding: test_ce6be_unrelaxed_rank_4_model_5.pdb (deflated 78%) adding: test_ce6be_unrelaxed_rank_5_model_1.pdb (deflated 78%) adding: cite.bibtex (deflated 52%) adding: test_ce6be_coverage.png (deflated 12%) adding: test_ce6be_PAE.png (deflated 15%) adding: test_ce6be_plddt.png (deflated 13%) . Instructions . Quick start . Paste your protein sequence(s) in the input field. | Press &quot;Runtime&quot; -&gt; &quot;Run all&quot;. | The pipeline consists of 5 steps. The currently running step is indicated by a circle with a stop sign next to it. | Result zip file contents . PDB formatted structures sorted by avg. pLDDT and complexes are sorted by pTMscore. (unrelaxed and relaxed if use_amber is enabled). | Plots of the model quality. | Plots of the MSA coverage. | Parameter log file. | A3M formatted input MSA. | A predicted_aligned_error_v1.json using AlphaFold-DB&#39;s format and a scores.json for each model which contains an array (list of lists) for PAE, a list with the average pLDDT and the pTMscore. | BibTeX file with citations for all used tools and databases. | At the end of the job a download modal box will pop up with a jobname.result.zip file. Additionally, if the save_to_google_drive option was selected, the jobname.result.zip will be uploaded to your Google Drive. . MSA generation for complexes . For the complex prediction we use unpaired and paired MSAs. Unpaired MSA is generated the same way as for the protein structures prediction by searching the UniRef100 and environmental sequences three iterations each. . The paired MSA is generated by searching the UniRef100 database and pairing the best hits sharing the same NCBI taxonomic identifier (=species or sub-species). We only pair sequences if all of the query sequences are present for the respective taxonomic identifier. . Using a custom MSA as input . To predict the structure with a custom MSA (A3M formatted): (1) Change the msa_mode: to &quot;custom&quot;, (2) Wait for an upload box to appear at the end of the &quot;MSA options ...&quot; box. Upload your A3M. The first fasta entry of the A3M must be the query sequence without gaps. . It is also possilbe to proide custom MSAs for complex predictions. Read more about the format here. . As an alternative for MSA generation the HHblits Toolkit server can be used. After submitting your query, click &quot;Query Template MSA&quot; -&gt; &quot;Download Full A3M&quot;. Download the A3M file and upload it in this notebook. . Using custom templates . To predict the structure with a custom template (PDB or mmCIF formatted): (1) change the template_mode to &quot;custom&quot; in the execute cell and (2) wait for an upload box to appear at the end of the &quot;Input Protein&quot; box. Select and upload your templates (multiple choices are possible). . Templates must follow the four letter PDB naming. . | Templates in mmCIF format must contain _entity_poly_seq. An error is thrown if this field is not present. The field _pdbx_audit_revision_history.revision_date is automatically generated if it is not present. . | Templates in PDB format are automatically converted to the mmCIF format. _entity_poly_seq and _pdbx_audit_revision_history.revision_date are automatically generated. . | . If you encounter problems, please report them to this issue. . Comparison to the full AlphaFold2 and Alphafold2 colab . This notebook replaces the homology detection and MSA pairing of AlphaFold2 with MMseqs2. For a comparison against the AlphaFold2 Colab and the full AlphaFold2 system read our preprint. . Troubleshooting . Check that the runtime type is set to GPU at &quot;Runtime&quot; -&gt; &quot;Change runtime type&quot;. | Try to restart the session &quot;Runtime&quot; -&gt; &quot;Factory reset runtime&quot;. | Check your input sequence. | . Known issues . Google Colab assigns different types of GPUs with varying amount of memory. Some might not have enough memory to predict the structure for a long sequence. | Your browser can block the pop-up for downloading the result file. You can choose the save_to_google_drive option to upload to Google Drive instead or manually download the result file: Click on the little folder icon to the left, navigate to file: jobname.result.zip, right-click and select &quot;Download &quot; (see screenshot). | . Limitations . Computing resources: Our MMseqs2 API can handle ~20-50k requests per day. | MSAs: MMseqs2 is very precise and sensitive but might find less hits compared to HHblits/HMMer searched against BFD or MGnify. | We recommend to additionally use the full AlphaFold2 pipeline. | . Description of the plots . Number of sequences per position - We want to see at least 30 sequences per position, for best performance, ideally 100 sequences. | Predicted lDDT per position - model confidence (out of 100) at each position. The higher the better. | Predicted Alignment Error - For homooligomers, this could be a useful metric to assess how confident the model is about the interface. The lower the better. | . Bugs . If you encounter any bugs, please report the issue to https://github.com/sokrypton/ColabFold/issues | . License . The source code of ColabFold is licensed under MIT. Additionally, this notebook uses the AlphaFold2 source code and its parameters licensed under Apache 2.0 and CC BY 4.0 respectively. Read more about the AlphaFold license here. . Acknowledgments . We thank the AlphaFold team for developing an excellent model and open sourcing the software. . | KOBIC and Söding Lab for providing the computational resources for the MMseqs2 MSA server. . | Richard Evans for helping to benchmark the ColabFold&#39;s Alphafold-multimer support. . | David Koes for his awesome py3Dmol plugin, without whom these notebooks would be quite boring! . | Do-Yoon Kim for creating the ColabFold logo. . | A colab by Sergey Ovchinnikov (@sokrypton), Milot Mirdita (@milot_mirdita) and Martin Steinegger (@thesteinegger). . | .",
            "url": "https://riversdark.github.io/variation/protein/2022/07/19/ColabFold.html",
            "relUrl": "/protein/2022/07/19/ColabFold.html",
            "date": " • Jul 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Binning 2D data and doing JV assignments",
            "content": "Generate data from 2D Gaussian distribution . Here we generate random samples from a 2D Gaussian distribution and plot the data. . !pip install lapjv . Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/ Requirement already satisfied: lapjv in /usr/local/lib/python3.7/dist-packages (1.3.22) Requirement already satisfied: numpy&gt;=1.20.0 in /usr/local/lib/python3.7/dist-packages (from lapjv) (1.21.6) . %config InlineBackend.figure_format=&#39;retina&#39; import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.spatial.distance import cdist from lapjv import lapjv np.random.seed(5) . mean1 = (-3, 2) cov1 = [[1, 0.8], [0.8, 1]] N1 = 25471 X1 = np.random.multivariate_normal(mean1, cov1, N1) . plt.hist2d(X1[:, 0], X1[:, 1], bins=100); . mean2 = (1, -2) cov2 = [[1, -0.8], [-0.8, 1]] N2 = 38472 X2 = np.random.multivariate_normal(mean2, cov2, N2) . plt.hist2d(X2[:, 0], X2[:, 1], bins=100); . X = np.concatenate((X1, X2)) X.shape . (63943, 2) . X3 = np.repeat([0, 1], [N1, N2]) X3 . array([0, 0, 0, ..., 1, 1, 1]) . plt.scatter(X[:, 0], X[:, 1], s=0.1, c=X3); . Split the data by percentiles . df = pd.DataFrame(X, columns=[&#39;x&#39;, &#39;y&#39;]) . df[&#39;label&#39;] = X3 . df.sample(10) . x y label . 26217 0.738606 | -2.028300 | 1 | . 55488 1.740267 | -1.976211 | 1 | . 52389 0.129738 | -0.606100 | 1 | . 55468 1.662917 | -2.309127 | 1 | . 57885 1.461722 | -2.425636 | 1 | . 13248 -2.662216 | 2.082329 | 0 | . 32111 0.853344 | -3.162861 | 1 | . 9769 -2.326941 | 1.784017 | 0 | . 40270 0.347402 | -1.538968 | 1 | . 44296 -0.224807 | -0.654308 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; n_cuts = 4 . pcts = np.linspace(0, 100, n_cuts+1) . pcts . array([ 0., 25., 50., 75., 100.]) . check the global percentiles . pcts_x = [np.percentile(df[&#39;x&#39;], p) for p in pcts] pcts_y = [np.percentile(df[&#39;y&#39;], p) for p in pcts] . pcts_x . [-7.056800776237176, -2.6761948261993025, 0.03237242940172069, 1.1987142330067349, 5.1813333873647105] . pcts_y . [-5.944252577679023, -2.202556488992252, -1.0339395474546924, 1.6699986040654207, 6.244821243128097] . len(pcts_x), len(pcts_x) . (5, 5) . generate catogorical colors for visualisation . colors = [&#39;#%06X&#39; % np.random.randint(0, 0xFFFFFF) for _ in range(n_cuts**2)] colors . [&#39;#7CD57F&#39;, &#39;#C2FB94&#39;, &#39;#2E20B9&#39;, &#39;#AC82F5&#39;, &#39;#A4C620&#39;, &#39;#06A653&#39;, &#39;#8A96DB&#39;, &#39;#2D148D&#39;, &#39;#BFFF53&#39;, &#39;#070D6D&#39;, &#39;#1D6CDA&#39;, &#39;#88477D&#39;, &#39;#CD8528&#39;, &#39;#8DB8EF&#39;, &#39;#2BCE56&#39;, &#39;#697773&#39;] . split the data according to the x and y percentiles . pcts_x = [np.percentile(df[&#39;x&#39;], p) for p in pcts] fig, axes = plt.subplots(1, n_cuts, figsize=(12, 4), sharex=True, sharey=True) for i in range(n_cuts): di = df[(df[&#39;x&#39;]&gt;=pcts_x[i]) &amp; (df[&#39;x&#39;]&lt;=pcts_x[i+1])] print(i, len(di)) axes[i].scatter(di[&#39;x&#39;], di[&#39;y&#39;], s=0.1, color=colors[i]) . 0 15986 1 15986 2 15986 3 15986 . pcts_y = [np.percentile(df[&#39;y&#39;], p) for p in pcts] fig, axes = plt.subplots(1, n_cuts, figsize=(12, 4), sharex=True, sharey=True) for j in range(n_cuts): dj = df[(df[&#39;y&#39;]&gt;=pcts_y[j]) &amp; (df[&#39;y&#39;]&lt;=pcts_y[j+1])] print(j, len(dj)) axes[j].scatter(dj[&#39;x&#39;], dj[&#39;y&#39;], s=0.1, color=colors[j]) . 0 15986 1 15986 2 15986 3 15986 . split the data along one axis, then along the other axis. . Note here that when splitting the data sequentially, we can&#39;t use the overall percentiles, but the percentiles of the split data. . fig, axes = plt.subplots(n_cuts, n_cuts, figsize=(16, 12), sharex=True, sharey=True) for i in range(n_cuts): pcts_x = [np.percentile(df[&#39;x&#39;], p) for p in pcts] di = df[(df[&#39;x&#39;]&gt;=pcts_x[i]) &amp; (df[&#39;x&#39;]&lt;=pcts_x[i+1])] print(i, len(di)) pcts_y = [np.percentile(di[&#39;y&#39;], p) for p in pcts] for j in range(n_cuts): dj = di[(di[&#39;y&#39;]&gt;=pcts_y[j]) &amp; (di[&#39;y&#39;]&lt;=pcts_y[j+1])] nc = i * n_cuts + j print(i, j, nc, len(dj)) axes[i, j].scatter(dj[&#39;x&#39;], dj[&#39;y&#39;], s=0.1, color=colors[nc]) . 0 15986 0 0 0 3997 0 1 1 3996 0 2 2 3996 0 3 3 3997 1 15986 1 0 4 3997 1 1 5 3996 1 2 6 3996 1 3 7 3997 2 15986 2 0 8 3997 2 1 9 3996 2 2 10 3996 2 3 11 3997 3 15986 3 0 12 3997 3 1 13 3996 3 2 14 3996 3 3 15 3997 . plot all the blocks on the some canvas . fig, axes = plt.subplots(figsize=(16, 12)) for i in range(n_cuts): pcts_x = [np.percentile(df[&#39;x&#39;], p) for p in pcts] di = df[(df[&#39;x&#39;]&gt;=pcts_x[i]) &amp; (df[&#39;x&#39;]&lt;=pcts_x[i+1])] pcts_y = [np.percentile(di[&#39;y&#39;], p) for p in pcts] print(i, len(di)) for j in range(n_cuts): dj = di[(di[&#39;y&#39;]&gt;=pcts_y[j]) &amp; (di[&#39;y&#39;]&lt;=pcts_y[j+1])] nc = i * n_cuts + j print(i, j, n_cuts, len(dj)) axes.scatter(dj[&#39;x&#39;], dj[&#39;y&#39;], s=0.1, color=colors[nc]) . 0 15986 0 0 4 3997 0 1 4 3996 0 2 4 3996 0 3 4 3997 1 15986 1 0 4 3997 1 1 4 3996 1 2 4 3996 1 3 4 3997 2 15986 2 0 4 3997 2 1 4 3996 2 2 4 3996 2 3 4 3997 3 15986 3 0 4 3997 3 1 4 3996 3 2 4 3996 3 3 4 3997 . collect the code for export . def percentile_split(X, label, n_cuts=4): pcts = np.linspace(0, 100, n_cuts+1) df = pd.DataFrame(X, columns=[&#39;x&#39;, &#39;y&#39;]) df[&#39;label&#39;] = label ds = {} for i in range(n_cuts): pcts_x = [np.percentile(df[&#39;x&#39;], p) for p in pcts] di = df[(df[&#39;x&#39;]&gt;=pcts_x[i]) &amp; (df[&#39;x&#39;]&lt;=pcts_x[i+1])] pcts_y = [np.percentile(di[&#39;y&#39;], p) for p in pcts] for j in range(n_cuts): dj = di[(di[&#39;y&#39;]&gt;=pcts_y[j]) &amp; (di[&#39;y&#39;]&lt;=pcts_y[j+1])] ds[&#39;&#39;.join([str(i), &#39;_&#39;, str(j)])] = dj return ds . ds = percentile_split(X, X3, n_cuts=4) for k, v in ds.items(): print(k, v.shape) . 0_0 (3997, 3) 0_1 (3996, 3) 0_2 (3996, 3) 0_3 (3997, 3) 1_0 (3997, 3) 1_1 (3996, 3) 1_2 (3996, 3) 1_3 (3997, 3) 2_0 (3997, 3) 2_1 (3996, 3) 2_2 (3996, 3) 2_3 (3997, 3) 3_0 (3997, 3) 3_1 (3996, 3) 3_2 (3996, 3) 3_3 (3997, 3) . ds[&#39;1_2&#39;][&#39;label&#39;] . 3 0 10 0 11 0 20 0 24 0 .. 25433 0 25434 0 25437 0 25438 0 25464 0 Name: label, Length: 3996, dtype: int64 . fig, axes = plt.subplots(figsize=(16, 12)) for i, (k, v) in enumerate(ds.items()): # print(i, v.shape) axes.scatter(v[&#39;x&#39;], v[&#39;y&#39;], s=0.1, color=colors[i]) . test the function with different number of splits . n_cuts = 10 ds = percentile_split(X, X3, n_cuts) len(ds) . 100 . for k, v in ds.items(): print(k, v.shape) . 0_0 (640, 3) 0_1 (639, 3) 0_2 (640, 3) 0_3 (639, 3) 0_4 (640, 3) 0_5 (640, 3) 0_6 (639, 3) 0_7 (640, 3) 0_8 (639, 3) 0_9 (640, 3) 1_0 (640, 3) 1_1 (639, 3) 1_2 (639, 3) 1_3 (640, 3) 1_4 (639, 3) 1_5 (639, 3) 1_6 (640, 3) 1_7 (639, 3) 1_8 (639, 3) 1_9 (640, 3) 2_0 (640, 3) 2_1 (639, 3) 2_2 (639, 3) 2_3 (640, 3) 2_4 (639, 3) 2_5 (639, 3) 2_6 (640, 3) 2_7 (639, 3) 2_8 (639, 3) 2_9 (640, 3) 3_0 (640, 3) 3_1 (639, 3) 3_2 (639, 3) 3_3 (640, 3) 3_4 (639, 3) 3_5 (639, 3) 3_6 (640, 3) 3_7 (639, 3) 3_8 (639, 3) 3_9 (640, 3) 4_0 (640, 3) 4_1 (639, 3) 4_2 (640, 3) 4_3 (639, 3) 4_4 (640, 3) 4_5 (640, 3) 4_6 (639, 3) 4_7 (640, 3) 4_8 (639, 3) 4_9 (640, 3) 5_0 (640, 3) 5_1 (639, 3) 5_2 (640, 3) 5_3 (639, 3) 5_4 (640, 3) 5_5 (640, 3) 5_6 (639, 3) 5_7 (640, 3) 5_8 (639, 3) 5_9 (640, 3) 6_0 (640, 3) 6_1 (639, 3) 6_2 (639, 3) 6_3 (640, 3) 6_4 (639, 3) 6_5 (639, 3) 6_6 (640, 3) 6_7 (639, 3) 6_8 (639, 3) 6_9 (640, 3) 7_0 (640, 3) 7_1 (639, 3) 7_2 (639, 3) 7_3 (640, 3) 7_4 (639, 3) 7_5 (639, 3) 7_6 (640, 3) 7_7 (639, 3) 7_8 (639, 3) 7_9 (640, 3) 8_0 (640, 3) 8_1 (639, 3) 8_2 (639, 3) 8_3 (640, 3) 8_4 (639, 3) 8_5 (639, 3) 8_6 (640, 3) 8_7 (639, 3) 8_8 (639, 3) 8_9 (640, 3) 9_0 (640, 3) 9_1 (639, 3) 9_2 (640, 3) 9_3 (639, 3) 9_4 (640, 3) 9_5 (640, 3) 9_6 (639, 3) 9_7 (640, 3) 9_8 (639, 3) 9_9 (640, 3) . colors = [&#39;#%06X&#39; % np.random.randint(0, 0xFFFFFF) for _ in range(n_cuts**2)] fig, axes = plt.subplots(figsize=(16, 12)) for i, (k, v) in enumerate(ds.items()): # print(i, v.shape) axes.scatter(v[&#39;x&#39;], v[&#39;y&#39;], s=0.1, color=colors[i]) . Our target data have close to 200 million data points, and we aim to split them into 10000 blocks, with each block having about 20000 data points. . as the number of splits gets bigger and bigger, it will be more and more difficult to differentiate the individual blocks . Doing linear assignments on individual blocks . use the grid defined by the biggest block . lens = [len(v) for _, v in ds.items()] length = max(lens) length . 640 . size1 = int(np.ceil(np.sqrt(length))) size2 = int(np.ceil(length/size1)) grid_size = (size1, size2) . grid = np.dstack(np.meshgrid(np.linspace(0, 1, size2), np.linspace(0, 1, size1))).reshape(-1, 2) grid_map = grid[:length] # non-rectangular grid . grid_map.shape, grid_size . ((640, 2), (26, 25)) . 25 * 26 . 650 . jvs = {} for k, v in ds.items(): length = len(v) size1 = int(np.ceil(np.sqrt(length))) size2 = int(np.ceil(length/size1)) grid_size = (size1, size2) grid = np.dstack(np.meshgrid(np.linspace(0, 1, size2), np.linspace(0, 1, size1))).reshape(-1, 2) grid_map = grid[:length] # non-rectangular grid print(&quot;Doing JV assignment on &quot;, k) cost_matrix = cdist(grid_map, v[[&#39;x&#39;, &#39;y&#39;]], &quot;sqeuclidean&quot;) cost_matrix = cost_matrix * (100000 / cost_matrix.max()) row_asses, col_asses, _ = lapjv(cost_matrix) grid_jv = grid_map[col_asses] jvs[k] = (v, grid_jv) print(&quot;Assignment finished&quot;) . Doing JV assignment on 0_0 Doing JV assignment on 0_1 Doing JV assignment on 0_2 Doing JV assignment on 0_3 Doing JV assignment on 0_4 Doing JV assignment on 0_5 Doing JV assignment on 0_6 Doing JV assignment on 0_7 Doing JV assignment on 0_8 Doing JV assignment on 0_9 Doing JV assignment on 1_0 Doing JV assignment on 1_1 Doing JV assignment on 1_2 Doing JV assignment on 1_3 Doing JV assignment on 1_4 Doing JV assignment on 1_5 Doing JV assignment on 1_6 Doing JV assignment on 1_7 Doing JV assignment on 1_8 Doing JV assignment on 1_9 Doing JV assignment on 2_0 Doing JV assignment on 2_1 Doing JV assignment on 2_2 Doing JV assignment on 2_3 Doing JV assignment on 2_4 Doing JV assignment on 2_5 Doing JV assignment on 2_6 Doing JV assignment on 2_7 Doing JV assignment on 2_8 Doing JV assignment on 2_9 Doing JV assignment on 3_0 Doing JV assignment on 3_1 Doing JV assignment on 3_2 Doing JV assignment on 3_3 Doing JV assignment on 3_4 Doing JV assignment on 3_5 Doing JV assignment on 3_6 Doing JV assignment on 3_7 Doing JV assignment on 3_8 Doing JV assignment on 3_9 Doing JV assignment on 4_0 Doing JV assignment on 4_1 Doing JV assignment on 4_2 Doing JV assignment on 4_3 Doing JV assignment on 4_4 Doing JV assignment on 4_5 Doing JV assignment on 4_6 Doing JV assignment on 4_7 Doing JV assignment on 4_8 Doing JV assignment on 4_9 Doing JV assignment on 5_0 Doing JV assignment on 5_1 Doing JV assignment on 5_2 Doing JV assignment on 5_3 Doing JV assignment on 5_4 Doing JV assignment on 5_5 Doing JV assignment on 5_6 Doing JV assignment on 5_7 Doing JV assignment on 5_8 Doing JV assignment on 5_9 Doing JV assignment on 6_0 Doing JV assignment on 6_1 Doing JV assignment on 6_2 Doing JV assignment on 6_3 Doing JV assignment on 6_4 Doing JV assignment on 6_5 Doing JV assignment on 6_6 Doing JV assignment on 6_7 Doing JV assignment on 6_8 Doing JV assignment on 6_9 Doing JV assignment on 7_0 Doing JV assignment on 7_1 Doing JV assignment on 7_2 Doing JV assignment on 7_3 Doing JV assignment on 7_4 Doing JV assignment on 7_5 Doing JV assignment on 7_6 Doing JV assignment on 7_7 Doing JV assignment on 7_8 Doing JV assignment on 7_9 Doing JV assignment on 8_0 Doing JV assignment on 8_1 Doing JV assignment on 8_2 Doing JV assignment on 8_3 Doing JV assignment on 8_4 Doing JV assignment on 8_5 Doing JV assignment on 8_6 Doing JV assignment on 8_7 Doing JV assignment on 8_8 Doing JV assignment on 8_9 Doing JV assignment on 9_0 Doing JV assignment on 9_1 Doing JV assignment on 9_2 Doing JV assignment on 9_3 Doing JV assignment on 9_4 Doing JV assignment on 9_5 Doing JV assignment on 9_6 Doing JV assignment on 9_7 Doing JV assignment on 9_8 Doing JV assignment on 9_9 Assignment finished . fig, axes = plt.subplots(n_cuts, n_cuts, figsize=(24, 16), sharex=True, sharey=True) axes = axes.flatten() for i, (k, (v, grid_jv)) in enumerate(jvs.items()): # print(i, k, v.shape) labels = v[&#39;label&#39;].values for j, end in enumerate(grid_jv): axes[i].plot(end[0], end[1], color=colors[labels[j]], marker=&#39;o&#39;); . we are using the class belongings of the mixture Gaussian with 2 components as labels, so it&#39;s unsurprising that in most cases, the points from the same component are lumped together. . Here the JV assignment are done sequentially in a for loop; if we have a big number of splits (10 thousand), and each block have much bigger number of data points (10 to 20 thousand data points), we&#39;d need to parallelise them properly. . This is what we&#39;ll do next. .",
            "url": "https://riversdark.github.io/variation/data/2022/07/06/binned_jv.html",
            "relUrl": "/data/2022/07/06/binned_jv.html",
            "date": " • Jul 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Build, criticise, and expand time series models with NumPyro",
            "content": "In this post we&#39;ll implement a time series framework based on a flexible smoothed exponential process. The framework can be used to model global trend, local variation, seasonality and other features that are essential for flexible time series modeling. . This post also covers several other features under this general framework, notably: . modeling over-dispersed observations with Student-T distribution | explicit and extensive modeling of the variation to capture data heteroscedasticity | adding regression components when necessary | . Some other general but important features include: . extensive usage of visualisation for model criticism | prior and posterior simulation for model inspection | and a unified prediction framework. | . Last but maybe most importantly, apart from the specific features and techniques, the post gives a general procedure for Bayesian model building and criticism, which should be useful for building any kind of model for any kind of data. . This is mostly a port of the R package Rlgt: Bayesian Exponential Smoothing Models with Trend Modifications, although the implementation details might differ here and there. One of the models has already been ported to Numpyro in one of the Numpyro tutorials, so that specific model is not covered here. . Different options for modeling trends and seasonalities have been considered in this post, to understand their varying capabilities and characteristics, and to prepare an arsenal for building bespoke models for future data sets with different characteristics. . !pip install numpyro[cpu] !pip install rdatasets . import matplotlib.pyplot as plt import arviz as az import pandas as pd import seaborn as sns sns.set_theme(palette=&#39;Set2&#39;) colors = sns.color_palette() import jax.numpy as jnp from jax import random from jax import lax, nn from jax.config import config; config.update(&quot;jax_enable_x64&quot;, True) jnp.set_printoptions(precision=2) rng = random.PRNGKey(123) import numpyro numpyro.set_host_device_count(2) import numpyro.distributions as dist from numpyro.contrib.control_flow import scan from numpyro.diagnostics import autocorrelation, hpdi from numpyro.infer import MCMC, NUTS, Predictive from numpyro import deterministic, sample from numpyro.handlers import seed, condition, plate from rdatasets import data, descr, summary %config InlineBackend.figure_format = &#39;retina&#39; . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . We are going to use two data sets, the BJ sales data and the monthly airline passenger data set. For the BJ sales data set we have an extra lead indicator, which can be used to build a regression component to the model. . bj = data(&quot;BJsales&quot;)[&#39;value&#39;] air_passengers = data(&quot;AirPassengers&quot;)[&#39;value&#39;] bj_lead = jnp.array( [10.01,10.07,10.32, 9.75,10.33,10.13,10.36,10.32,10.13,10.16,10.58,10.62, 10.86,11.20,10.74,10.56,10.48,10.77,11.33,10.96,11.16,11.70,11.39,11.42, 11.94,11.24,11.59,10.96,11.40,11.02,11.01,11.23,11.33,10.83,10.84,11.14, 10.38,10.90,11.05,11.11,11.01,11.22,11.21,11.91,11.69,10.93,10.99,11.01, 10.84,10.76,10.77,10.88,10.49,10.50,11.00,10.98,10.61,10.48,10.53,11.07, 10.61,10.86,10.34,10.78,10.80,10.33,10.44,10.50,10.75,10.40,10.40,10.34, 10.55,10.46,10.82,10.91,10.87,10.67,11.11,10.88,11.28,11.27,11.44,11.52, 12.10,11.83,12.62,12.41,12.43,12.73,13.01,12.74,12.73,12.76,12.92,12.64, 12.79,13.05,12.69,13.01,12.90,13.12,12.47,12.47,12.94,13.10,12.91,13.39, 13.13,13.34,13.34,13.14,13.49,13.87,13.39,13.59,13.27,13.70,13.20,13.32, 13.15,13.30,12.94,13.29,13.26,13.08,13.24,13.31,13.52,13.02,13.25,13.12, 13.26,13.11,13.30,13.06,13.32,13.10,13.27,13.64,13.58,13.87,13.53,13.41, 13.25,13.50,13.58,13.51,13.77,13.40]) . These data sets have different characteristics and as such will demand different modeling considerations. . Modeling global trend and local variation . The data used for this model is the BJsales data set, which contains 150 sales observations. The data are bounded by zero, as can be expected for sales data, and range between 200 and 300, and there is very little variation between neighbouring points. . lgt_y = jnp.array(bj) plt.plot(bj, &#39;.&#39;) plt.title(&#39;BJ sales data&#39;); . The first model we are building can be described as . begin{align*} y_{t} &amp; sim text{T} ( nu, mu_{t}, sigma_{t}) sigma_{t} &amp;= kappa mu_{t}^{ tau} + xi mu_{t} &amp;= text{G}_{t} + text{L}_{t} text{G}_{t} &amp;= g_{t-1} + gamma g_{t-1}^{ rho } text{L}_{t} &amp;= lambda l_{t-1} g_{t} &amp;= alpha y_{t} + ( 1- alpha ) text{G}_{t} l_{t} &amp;= beta left( g_{t}-g_{t-1} right) + left( 1- beta right) l_{t-1} end{align*} This first model is already quite complex so let&#39;s spend a moment to digest it. We can visualise the model as follows . . The variable $y$ in the above graph is shaded because it&#39;s the observed variable. One thing to keep in mind with Bayesian networks, is that they form directed acyclical graphs, that is, there shouldn&#39;t be circles anywhere in the model. We can check that this is indeed the case with the above model. . To model the outcome we&#39;ll use the Student&#39;s T distribution, which has three parameters: the degree of freedom $ nu$, the expected value $ mu$, and the standard deviation $ sigma$. . The degree of freedom determines how heavy tailed the distribution will be: when the degree of freedom is one, the T distribution becomes the Cauchy distribution, which is so heavy tailed that the mean and standard deviation can not even be properly defined; when the degree of freedom approaches infinity the distribution approaches Normal, with quite narrow tails. Using the T distribution, we can account for heavy tail data when the data are indeed heavy tailed, but it&#39;s also flexible enough for when they are not. Since à priori we don&#39;t have much information on the degree of freedom, we&#39;ll put a wide prior on it. . The expected value is where we&#39;ll put most of our modeling focus on. In this first model we&#39;ll consider modeling it as the sum of a global trend $ text{G}$ and a local variation $ text{L}$, with the global trend being modeled as a smoothed exponential function of the hidden level $g$, and the local variation as a dampened baseline variation $l$. Furthermore, both the hidden level $g$ and the variation $l$ follow a smoothed autoregressive process. For hidden level $g$, it&#39;s a weighted average between the outcome, and the previous global trend. For the baseline variation $l$ it&#39;s a weighted average between the increment in hidden levels, and the previous baseline variation. $ alpha$ and $ beta$ are the corresponding smoothing weights. . The standard deviation is modeled with another smoothed exponential process of the expected value, so this is a heterogeneous model, with the variance varying alongside the expected value. . Some of the modeling assumptions might look a bit arbitrary; and they indeed are. There are a thousand choices we can make about how each piece of the model is formulated, and sometimes we have to try different alternatives to see which works best for our data. . Because both the standard deviation $ sigma$ and the expected value $ mu$ follow a smoothed exponential process, we have to guarantee the expected value and the hidden level $g$ be always positive. This assumption is reasonable when the data is also positive, as is the case here. . It&#39;s important to note that the models we are building might be too complex for the data we are using. Here by exploratory analysis we can see that the BJ sales data have a global trend, and there are clearly local variations, but there doesn&#39;t seem to be many sudden changes in the series, and as such the smoothed exponential process for the standard deviation might not be strictly necessary. Still, we use a more complex model to account for the potential heterogeneities in the data. . Now we can chose some proper priors for the model parameters and code the model in NumPyro. Of course, choosing priors is an integral part of Bayesian modeling and it is no easy task, we need to consider our prior knowledge carefully and gradually update that in model criticism. The priors chosen for this model implementation take into account both the model structure, and the general knowledge we have gained from exploratory data analysis. In Bayesian analysis each model should be a bespoke model for the specific data set we are modeling. . A closer look at the above graph and the model definition, we notice that for all variables at all time periods, their distributions are parameterised by other variables, with the exceptions of two variables: the first $g$ and the first $ ell$. So our model isn&#39;t complete: we need also choose the prior distributions for these two variables. . We can remind ourselves them $g$ and $ ell$ are the hidden global and local levels, respectively; and $g$ is defined as a weight average of the observed outcome value and the global trend; for these reasons it&#39;s not unreasonable to define the prior mean of $g$ to be the first observed outcome value. As for $ ell$, since it&#39;s the hidden level of the local variation, a prior centered at zero seems also reasonable. . And because we&#39;ve used the first observation to initialise the model, we&#39;ll model the observed data starting at the second. . def lgt(y, future=4): N = y.shape[0] nu = sample(&quot;nu&quot;, dist.Uniform(1, 50)) xi = sample(&quot;xi&quot;, dist.HalfNormal(2)) tau = sample(&quot;tau&quot;, dist.Beta(1, 4)) kappa = sample(&quot;kappa&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) lbda = sample(&quot;lbda&quot;, dist.Beta(2, 2)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) g_init = sample(&quot;g_init&quot;, dist.Normal(y[0], 5)) l_init = sample(&quot;l_init&quot;, dist.Normal(0, 10)) def transition_fn(carry, t): g, l = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) L = deterministic(&quot;L&quot;, lbda * l) mu = deterministic(&quot;mu&quot;, jnp.clip(G + L, 0)) sigma = deterministic(&quot;sigma&quot;, xi + kappa * mu ** tau) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * yt + (1 - alpha) * G, 0)) l_new = deterministic(&quot;l&quot;, beta * (g_new - g) + (1 - beta) * l) return (g_new, l_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, l_init), jnp.arange(0, N+future)) return ys print(&quot;Observed data: n&quot;, lgt_y[:5]) with seed(rng_seed=3): print(&quot; nPrior sampling:&quot;) print(lgt(lgt_y[:1], future=4)) print(&quot; nFixed data sampling:&quot;) print(lgt(lgt_y[:5], future=0)) . Observed data: [200.1 199.5 199.4 198.9 199. ] Prior sampling: [200.1 200.36 198.58 200.16 200.29] Fixed data sampling: [200.1 199.5 199.4 198.9 199. ] . With the model specified we can draw prior samples from it. We can generate prior samples without the observed data, because our model is completely generative. We also write a simple helper function check_prior to collect model variables and print out summary statistics. The model variables are separated into two groups: global variables, which determine the behaviour of the whole model; and local variables which are specific to each observed outcome. . lgt_prior = Predictive(lgt, num_samples=500)(rng, lgt_y[:1], 139) def check_prior(prior): &quot;&quot;&quot;Print out prior shapes, collect local and global variables into lists.&quot;&quot;&quot; lvs = [] gvs = [] y_length = prior[&#39;y&#39;].shape[-1] for k, v in prior.items(): if v.ndim == 1: gvs.append(k) elif v.shape[1] == y_length: lvs.append(k) else: gvs.append(k) print(&quot;Global variables:&quot;) for k in gvs: print(k, prior[k].shape, end=&#39;; &#39;) print(&quot; n nLocal variables:&quot;) for k in lvs: print(k, prior[k].shape, end=&#39;; &#39;) return lvs, gvs lgt_lvs, lgt_gvs = check_prior(lgt_prior) . It&#39;s important to check the model prior predictions to make sure the model indeed has the desired properties we intended for. We can use a parallel plot in which each sample will be plotted as one single line across different coordinates. While doing so, we&#39;ll also differentiate the normal samples, that is that intended samples of our model, from samples with extremely large outcomes, and also from samples with numerical overflows. The samples with numerical overflow will be in red, while those with extreme outcomes will be in blue. . Here we first write another helper function concat_arrays to concatenate one or two dimensional arrays stored in a dict, as is the case with our prior and posterior samples. And if the variable is multivariate, we also break down its dimensions into separate variables. This is to facilitate later visualisation. . def concat_arrays(dct, vs): nvs = [] ds = [] for v in vs: d = dct[v] if d.ndim == 2: vplus = [v+str(i) for i in range(d.shape[-1])] nvs += vplus ds.append(d) else: nvs.append(v) ds.append(d[:, None]) ds = jnp.concatenate(ds, axis=1) return nvs, ds nvs, ds = concat_arrays(lgt_prior, [&#39;gamma&#39;, &#39;rho&#39;]) print(len(nvs), ds.shape) # 1+1=2, (500, 2) nvs, ds = concat_arrays(lgt_prior, [&#39;g&#39;, &#39;l&#39;]) print(len(nvs), ds.shape) # 140+140=280, (500, 280) nvs, ds = concat_arrays(lgt_prior, [&#39;gamma&#39;, &#39;l&#39;, &#39;rho&#39;]) print(len(nvs), ds.shape) # 1+140+1=142, (500, 142) . And plot the global parameters of the model . def compare_global_params(dct, vs, v_ref=&#39;y&#39;, val_ref=10000): # TODO add custom labels to different groups nans = jnp.any(jnp.isnan(dct[v_ref]), axis=1) extremes = jnp.any(dct[v_ref] &gt; val_ref, axis=1) print(&#39;Number of samples with numerical overflows: &#39;, nans.sum()) print(&#39;Number of samples with extreme values : &#39;, extremes.sum()) fig, ax = plt.subplots(1, 1, sharex=True, sharey=True) nvs, ds = concat_arrays(dct, vs) for i in range(ds.shape[0]): if not nans[i]: if not extremes[i]: ax.plot(ds[i], color=colors[0], alpha=0.1, lw=1) for i in range(ds.shape[0]): if not nans[i]: if extremes[i]: ax.plot(ds[i], color=&#39;r&#39;, alpha=0.2, lw=1) for i in range(ds.shape[0]): if nans[i]: ax.plot(ds[i], color=&#39;k&#39;, alpha=0.2, lw=1) ax.set_xticks(range(len(nvs))) ax.set_xticklabels(nvs, rotation=30) plt.tight_layout(); compare_global_params(lgt_prior, lgt_gvs) . Because different variables have different scales, to get a better idea of some certain variables we can limit the list of variables to be plotted: . compare_global_params(lgt_prior, [&#39;rho&#39;, &#39;tau&#39;, &#39;gamma&#39;, &#39;kappa&#39;, &#39;lbda&#39;]) . Even though we have consciously limited the prior distribution for $ rho$ to favour smaller values, when its values are still relatively large, we&#39;re likely to see samples with extreme outcomes. This is understandable because $ rho$ decides how fast the exponential grows. . We can also plot the distribution of one single variable of interest: . sns.histplot(lgt_prior[&#39;rho&#39;], bins=30, stat=&#39;probability&#39;); . This corresponds to the samples at coordinate rho in the previous plot. A priori the expected value of $ rho$ is 0.2, but as we can see, that are many samples will much bigger values. We can try to change the priors for these parameters to limit their behavior, but the problem here doesn&#39;t seem very serious so we&#39;ll leave them be for now. . We now plot the model outcome, and other local variables to check the implications of the model priors. . def plot_locals(dct, vs, v_ref=&#39;y&#39;, val_ref=5000, **plt_kws): extremes = jnp.any(dct[v_ref] &gt; val_ref, axis=1) print(&#39;Number of samples with extreme values : &#39;, extremes.sum()) fig, axes = plt.subplots(**plt_kws) axes = axes.flatten() for i, v in enumerate(vs): data = dct[v] for k in range(data.shape[0]): if not extremes[k]: axes[i].plot(data[k], color=colors[0], alpha=0.2) axes[i].set_title(v) plt.tight_layout(); plot_locals(lgt_prior, lgt_lvs, nrows=4, ncols=2, figsize=(8, 8)) . We can see that the outcome generally follow the smoothed exponential process, with some cases the outcome growing faster than other cases. Also notice that the local variation is roughly one to two orders of magnitudes smaller than the global trend. This is intended because we want to to able to properly identify them. . We can also generate prior samples conditioned on the observed data. Since time series models are Markovian, i.e. path dependent, the observed data will serve to limit some of the model parameters that depend on the outcome to a region in the ambient space that is compatible with the observed data. . lgt_prior_fixed = Predictive(lgt, num_samples=500)(rng, lgt_y[:140], 0) plot_locals(lgt_prior_fixed, lgt_lvs, v_ref=&#39;mu&#39;, val_ref=500, nrows=4, ncols=2, figsize=(8,8)) . This might be useful for understanding the general interactions between the model and the data, and also for comparison with the posterior, to see how much we have learned. . To test that our inference engine and the model works properly, We can then chose one of our prior samples, feed it to our inference engine to see if we can recover the parameters used to generate them. It&#39;s important to realise that we are not always able to recover the parameters, because as we have see in the prior check, there are some highly irregular samples. . First chose one sample . n = 14 y = lgt_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . The sample seems ordinary enough, not one of the extreme ones. We then run inference on it and compare the posterior samples with the real parameter value that we used to generate the outcome. . kernel = NUTS(lgt) mcmc = MCMC(kernel, num_warmup=1000, num_samples=2000, num_chains=4, progress_bar=False) mcmc.run(rng, y, future=0) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . def check_inference(mcmc, prior, vs, n): refs = [] for v in vs: val = prior[v][n] if val.ndim == 0: refs.append(val.item()) else: refs += list(val) d = az.from_numpyro(mcmc) az.plot_posterior(d, var_names=vs, ref_val=refs, point_estimate=None, hdi_prob=&#39;hide&#39;, figsize=(8, 8)); check_inference(mcmc, lgt_prior, lgt_gvs, n=14) . The recovered parameters are well in the range of possibilities, but there are large uncertainties around them. This is to be expected: when building complex models, there is only so much we can learn from the limited data. We move on to the inference on real data. . lgt_kernel = NUTS(lgt, target_accept_prob=0.95) lgt_mcmc = MCMC(lgt_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) lgt_mcmc.run(rng, lgt_y[:-10], future=0) lgt_mcmc.print_summary() . Now we can make predictions for future periods. . lgt_sample = lgt_mcmc.get_samples() lgt_post = Predictive(lgt, posterior_samples=lgt_sample)(rng, lgt_y[:140], 10) print(&quot;posterior samples:&quot;) for k, v in lgt_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in lgt_post.items(): print(k, v.shape, end=&#39;; &#39;) . And plot the predictions. We&#39;ll plot the mean, along with 5th, 25th, 75th, and 95th prediction distribution percentiles. . def plot_post(y, post, vs, nrow=3, ncol=2, size=(8, 8)): fig, axes = plt.subplots(nrow, ncol, figsize=size) axes = axes.flatten() for i, v in enumerate(vs): if v == &#39;y&#39;: axes[i].plot(y, color=colors[0], lw=1); mean = post[v].mean(0) p5 = jnp.percentile(post[v], 5, axis=0) p25 = jnp.percentile(post[v], 25, axis=0) p75 = jnp.percentile(post[v], 75, axis=0) p95 = jnp.percentile(post[v], 95, axis=0) sd = post[v].std(0) axes[i].fill_between(range(len(mean)), p5, p95, color=colors[4], alpha=0.2) axes[i].fill_between(range(len(mean)), p25, p75, color=colors[1], alpha=0.4) axes[i].plot(mean, color=colors[2], lw=1) axes[i].set_title(v) plt.tight_layout(); plot_post(lgt_y, lgt_post, lgt_lvs, nrow=4, ncol=2) . We can see that the prediction is not that far off: we get the basic trend right, and the 50% credible interval does cover the observed data. But still, our model prediction is basically a linear extrapolation, the model prediction is almost entirely determined by the global trend, and the local variation in our model contributed little, if anything at all, in modifying the global behaviour. This motivates us to add some new information of locality to our model, and this is why we&#39;ll add a new regression component in the next part. . Besides, the posterior of $ sigma$ is very small and almost constant, this indicates that the smoothed exponential model for the standard deviation is entirely redundant, we might as well just use a simple homogeneous formulation. . Apart from visualising the posterior prediction, there are also many different scores we can check. The first one is the symmetric mean absolute percentage error, which is based on the percentage error of the mean prediction. Notice that the &quot;mean&quot; here refers to the mean of the point estimate across all the observations, and for the point estimate itself, we can use the mean, the median, or any other point estimate of the posterior. . def eval_smape(pred, truth): pred = jnp.mean(pred, 0) return 200 * jnp.mean(jnp.abs(pred - truth) / (pred + truth)).item() . And the mean absolute error. This time we will use the median of the posterior prediction to calculate the point estimate. . def eval_mae(pred, truth): pred = jnp.median(pred, 0) return jnp.mean(jnp.abs(pred - truth)).item() . And the root mean squared error, using sample mean as point estimate. . def eval_rmse(pred, truth): pred = jnp.mean(pred, 0) return jnp.sqrt(jnp.mean(jnp.square(pred - truth))).item() . And finally, the continuous ranked probability score. The previous scores all use point estimates of the posterior, CRPS takes the whole posterior into consideration. . def eval_crps(pred, truth): # ref: https://github.com/pyro-ppl/pyro/pull/2045 num_samples = pred.shape[0] pred = jnp.sort(pred, axis=0) diff = pred[1:] - pred[:-1] weight = jnp.arange(1, num_samples) * jnp.arange(num_samples - 1, 0, -1) weight = weight.reshape(weight.shape + (1,) * truth.ndim) crps_empirical = jnp.mean(jnp.abs(pred - truth), 0) - (diff * weight).sum(axis=0) / num_samples ** 2 return jnp.mean(crps_empirical).item() . Let&#39;s see how well we have done. . def check_scores(post, y, n_test): pred = post[&#39;y&#39;][:, -n_test:] ytest = y[-n_test:] print(&quot;sMAPE: {:.2f}%, MAE: {:.2f}, RMSE: {:.2f}, CPRS: {:.2f}&quot;.format( eval_smape(pred, ytest), eval_mae(pred, ytest), eval_rmse(pred, ytest), eval_crps(pred, ytest))) check_scores(lgt_post, lgt_y, 10) . Of course these scores are only useful when compared with performance from other models. We can come back and compare them to other models in the later parts of the post. . Adding a regression component to the model . As we&#39;ve seen before, the model prediction from the previous model is far from satisfactory. Luckily with the BJsales data set we have another accompanying lead indicator which we can integrate into our model as a regression component. Also, since the smoothed exponential process for the standard deviation doesn&#39;t contributed much, we&#39;ll remove this model component, and directly put a prior on $ sigma$. . The lead indicator . plt.plot(bj_lead, &#39;.&#39;) plt.title(&#39;BJ sales lead indicator&#39;); . we can see that the indicator variable follows a similar progression pattern as that of the sales data, so it should offer us useful information to improve the model prediction. . We&#39;ll use the lead indicator, lagged 3 and 4 periods, as predictors. The total length of the data, since we have to remove the first 4 data points for lack of predictors, becomes 146. We&#39;ll use the last 10 period for prediction as before. The regression component of the model will also have an intercept. . x0 = jnp.ones([146]) x1 = bj_lead[:-4] x2 = bj_lead[1:-3] lgtr_x = jnp.stack((x0, x1, x2), axis=-1) lgtr_y = jnp.array(bj[4:]) print(&#39;Predictors shape:&#39;, lgtr_x.shape, &#39; nOutcome shape:&#39;, lgtr_y.shape) . We change the expected value of the model so that it now consists of three components: a global trend, a local variation, and a local regression component. . Also, with increasing components for the expected value, it&#39;s getting more difficult to assign a proper prior to the hidden level $g$. For this reason we give the starting level a wider Cauchy prior, and limit its value to be positive. . def lgtr(x, y): nu = sample(&quot;nu&quot;, dist.Uniform(2, 20)) sigma = sample(&quot;sigma&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) lbda = sample(&quot;lbda&quot;, dist.Beta(2, 2)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) with plate(&quot;D&quot;, size=x.shape[1]): eta = sample(&quot;eta&quot;, dist.Normal(0, 5)) g_init = sample(&quot;g_init&quot;, dist.TruncatedCauchy(y[0], 10, low=0)) l_init = sample(&quot;l_init&quot;, dist.Normal(0, 10)) def transition_fn(carry, xt): g, l = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) L = deterministic(&quot;L&quot;, lbda * l) R = deterministic(&quot;R&quot;, jnp.dot(xt, eta)) # LR = deterministic(&quot;LR&quot;, L+R) mu = deterministic(&quot;mu&quot;, jnp.clip(G + L + R, 0)) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * yt + (1 - alpha) * G, 0)) l_new = deterministic(&quot;l&quot;, beta * (g_new - g) + (1 - beta) * l) return (g_new, l_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, l_init), x) return ys print(&quot;Observed data:&quot;, lgtr_y[:4]) with seed(rng_seed=3): print(&quot; nPrior sampling:&quot;) print(lgtr(lgtr_x[:4], lgtr_y[:1])) print(&quot; nFixed data sampling:&quot;) print(lgtr(lgtr_x[:4], lgtr_y[:4])) . Sample from the prior . lgtr_prior = Predictive(lgtr, num_samples=500)(rng, lgtr_x[:-10], lgtr_y[:1]) lgtr_lvs, lgtr_gvs = check_prior(lgtr_prior) . First let&#39;s take a look at our truncated Cauchy prior for the initial hidden level . sns.histplot(lgtr_prior[&#39;g_init&#39;]); . We can see the bulk of the samples are around 200, but the samples have quite a wide range, from zero to a few thousands, so it should be flexible enough for our model. . And check for global parameters . compare_global_params(lgtr_prior, [&#39;eta&#39;, &#39;gamma&#39;, &#39;l_init&#39;, &#39;sigma&#39;, ]) . We don&#39;t have numerical overflows, this is good. We do have more samples with very large outcomes, but this is to be expected, because we are introducing more model components to the model, and this expands our modeling space and consequently the outcome range. What matters here, is that there are no clear patterns to the extreme outcomes, which implies that the extreme outcomes are the natural result of expanding the model space, not that of some malfunctionaling model component. . And the prior prediction for local variables . plot_locals(lgtr_prior, lgtr_lvs, val_ref=10000, nrows=4, ncols=2, figsize=(8, 8)) . Again, we can clearly see the effect of the smoothed exponential model. We can compare it with prior predictions conditioning on the data . lgtr_prior_fixed = Predictive(lgtr, num_samples=500)(rng, lgtr_x[:-10], lgtr_y[:-10]) plot_locals(lgtr_prior_fixed, lgtr_lvs, v_ref=&#39;mu&#39;, val_ref=500, nrows=4, ncols=2, figsize=(8, 8)) . The introduction of the regression component makes the relationship between the hidden level $l$ and the model outcome $y$ less prominent. This implies that the current model has the potential to improve on the previous model, since the model now has more flexibility introduced by the R component. However, we should also be aware that if the regression part does not explain the outcome very well, it can also potentially reduce the overall predictive power of the model, as it obscures the previously dominating relationship between the outcome and the hidden level. . Like before, now we chose one sample from the prior predictive samples and do parameter recovery. . n = 14 y = lgtr_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . And inference. . kernel = NUTS(lgtr) mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False) mcmc.run(rng, lgtr_x[:-10], y) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . check_inference(mcmc, lgtr_prior, lgtr_gvs, n=14) . We have been able to recover most of the parameters. The posterior for $ nu$ is quite far off, but considering the model and the data used, this is also understandable: this specific sample hardly has any fluctuation at all, there is no way we can learn much about the true degree of freedom. . Next we do inference on the real data. . lgtr_kernel = NUTS(lgtr, target_accept_prob=0.95) lgtr_mcmc = MCMC(lgtr_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) lgtr_mcmc.run(rng, lgtr_x[:-10], lgtr_y[:-10]) lgtr_mcmc.print_summary() . Now we can make predictions for future periods. If we feed the Predictive function with more predictors than outcomes, the function will automatically make prediction for future periods. . lgtr_sample = lgtr_mcmc.get_samples() lgtr_post = Predictive(lgtr, posterior_samples=lgtr_sample)(rng, lgtr_x, lgtr_y[:-10]) print(&quot;posterior samples:&quot;) for k, v in lgtr_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in lgtr_post.items(): print(k, v.shape, end=&#39;; &#39;) . Let&#39;s look at the model prediction. . plot_post(lgtr_y, lgtr_post, lgtr_lvs, nrow=4, ncol=2) . This time the model prediction follows much closer to the local variations, we have made significant improvement to the model&#39;s predictive power. The reason for this improvement, as we can see from the above plot, is that we now have two model components, R and L, that contribute very meaningfully to the prediction. In the previous model, although the local variation component has been able to capture some local change, but the prediction for future periods is outright flat, which means it hardly alters the global trend at all. In this model, on the contrary, there is some uncertainty around the global trend G all along the time series, and this uncertainty permits the other model components, R and L, to make changes to the global trend when the data demands it, and to consequently change the overall mean and ultimately the outcome. . We can also check the posterior distribution of $ sigma$: . sns.histplot(lgtr_sample[&#39;sigma&#39;], bins=30); . Indeed the standard deviation is very small, and with little variation. We can also check the scores. . check_scores(lgtr_post, lgtr_y, 10) . All the scores have significantly improved (that is, reduced), compared to the previous model. . Using a different variation modeling approach . From the previous two models we can see that with the BJsales data set, the expected value does not affect the outcome variation very much, so the exponentially smoothed model is not very helpful. . Here we propose another model for the variation. In this model, the variation is a linear function of another hidden variable $w$, which itself follows an autoregressive process as the smoothed average between its previous value and the current absolute error, just like $l$ and $g$. . lgtrs_x = lgtr_x lgtrs_y = lgtr_y print(&#39;Predictors shape:&#39;, lgtrs_x.shape, &#39; nOutcome shape:&#39;, lgtrs_y.shape) . The full model can be written down as: . And we update our model . def lgtrs(x, y): nu = sample(&quot;nu&quot;, dist.Uniform(1, 50)) xi = sample(&quot;xi&quot;, dist.HalfNormal(2)) kappa = sample(&quot;kappa&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) lbda = sample(&quot;lbda&quot;, dist.Beta(2, 2)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) zeta = sample(&quot;zeta&quot;, dist.Beta(2, 2)) with plate(&quot;D&quot;, size=x.shape[1]): eta = sample(&quot;eta&quot;, dist.Normal(0, 5)) g_init = sample(&quot;g_init&quot;, dist.TruncatedCauchy(y[0], 10, low=0)) l_init = sample(&quot;l_init&quot;, dist.Normal(0, 10)) w_init = sample(&quot;w_init&quot;, dist.HalfNormal(5)) def transition_fn(carry, xt): g, l, w = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) L = deterministic(&quot;L&quot;, lbda * l) R = deterministic(&quot;R&quot;, jnp.dot(xt, eta)) mu = deterministic(&quot;mu&quot;, jnp.clip(G + L + R, 0)) sigma = deterministic(&quot;sigma&quot;, xi + kappa * w) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * yt + (1 - alpha) * G, 0)) l_new = deterministic(&quot;l&quot;, beta * (g_new - g) + (1 - beta) * l) w_new = deterministic(&quot;w&quot;, zeta * jnp.abs(yt - mu) + (1 - zeta) * w) return (g_new, l_new, w_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, l_init, w_init), x) return ys print(&quot;Observed data:&quot;, lgtrs_y[:4]) with seed(rng_seed=5): print(&quot; nPrior sampling:&quot;) print(lgtrs(lgtrs_x[:4], lgtrs_y[:1])) print(&quot; nFixed data sampling:&quot;) print(lgtrs(lgtrs_x[:4], lgtrs_y[:4])) . Sample from the prior model . lgtrs_prior = Predictive(lgtrs, num_samples=500)(rng, lgtrs_x[:-10], lgtrs_y[:1]) lgtrs_lvs, lgtrs_gvs = check_prior(lgtrs_prior) . Check global parameters . compare_global_params(lgtrs_prior, [&#39;eta&#39;, &#39;kappa&#39;, &#39;l_init&#39;, &#39;w_init&#39;, &#39;xi&#39;], val_ref=50000) . And local variables . plot_locals(lgtrs_prior, lgtrs_lvs, val_ref=50000, nrows=3, ncols=3, figsize=(8,8)) . The outcome range has greatly increased, we now have almost half the samples having outcomes greater than 50000. This is way bigger than the range of data we&#39;d expect from the real world. If a store has regular sales between 200 and 300 per day, we can be sure that that store won&#39;t hold an inventory of 50000. If they do, well, statistical modeling of their sales figure should be the last of their concern. . Do we need to revise the model before we carry on doing inference? That depends. If we have some extra information readily available that we can use to make our priors more precise, that will certainly help. But this is above all a cost-benefit tradeoff. Does the improvement we bring to the model worth the effort we are putting in? It&#39;s possible that the information in the data likelihood will completely overwhelm the information in the prior, and if that is the case the extra effort spent in improving the priors wouldn&#39;t really worth it. Again, what&#39;s important is, à priori, not to limit our model to regions of the ambient space not covering the observed data. This doesn&#39;t seem to be the case here, so we&#39;ll move on. . Prior prediction conditioned on data . lgtrs_prior_fixed = Predictive(lgtrs, num_samples=500)(rng, lgtrs_x[:-10], lgtrs_y[:-10]) plot_locals(lgtrs_prior_fixed, lgtrs_lvs, v_ref=&#39;mu&#39;, val_ref=500, nrows=3, ncols=3, figsize=(8, 8)) . Like before, now we chose one sample from the prior predictive samples and do parameter recovery on it. . n = 74 y = lgtrs_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . This does not look like our observed data at all. And inference. . kernel = NUTS(lgtrs) mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False) mcmc.run(rng, lgtrs_x[:-10], y) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . check_inference(mcmc, lgtrs_prior, lgtrs_gvs, n=n) . Although the data look quite different from the observed, we are still able to recover the parameters reasonably well, this should give us some confidence in our model. Next we do inference on the real data. . lgtrs_kernel = NUTS(lgtrs, target_accept_prob=0.95) lgtrs_mcmc = MCMC(lgtrs_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) lgtrs_mcmc.run(random.PRNGKey(1), lgtrs_x[:-10], lgtrs_y[:-10]) lgtrs_mcmc.print_summary() . Prediction . lgtrs_sample = lgtrs_mcmc.get_samples() lgtrs_post = Predictive(lgtrs, posterior_samples=lgtrs_sample)(rng, lgtrs_x, lgtrs_y[:-10]) print(&quot;posterior samples:&quot;) for k, v in lgtrs_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in lgtrs_post.items(): print(k, v.shape, end=&#39;; &#39;) . and visualisation . plot_post(lgtrs_y, lgtrs_post, lgtrs_lvs, nrow=3, ncol=3) . This time we indeed have a much more precise estimation of the standard deviation $ sigma$, we can see that $ sigma$ increases when the time series has a sudden change of global trend. But it is very small and for the prediction of future periods, the standard deviation returned to a flat prediction like in previous models. . Compare the scores. . check_scores(lgtrs_post, lgtrs_y, 10) . The scores are slightly better than the previous model. . In the last three models, we have used three different methods to model the outcome variance, and two different methods for the hidden level. This flexibility is the most important feature of our Bayesian modeling approach: we take different components capable of capturing different features, and freely assemble them to fit the need of a specific data set we want to model. . In the previous models, to capture the local variation, we first used a Markov process, in which we modeled the local variation as the change in global trend; then, since we also have some predictors corresponding to each time period, we also added a regression component. But we didn&#39;t introduce seasonality to the model, because there doesn&#39;t seem to be any such feature present in this specific data set. However, if the data are indeed seasonal, we can also add new components to address this. . Still, seasonality can be represented in infinitely many different ways, and in our modeling framework, there remains the question of how seasonality should be incorporated into it. Should it be an additional component, like the regression component we have just added, to capture extra local variation, or should it be an enhancing factor, that impacts that outcome by changing the global trend? In the next section we first introduce a multiplicative seasonality, which affects the outcome by increasing or decreasing the global trend, and in the section that follows, we&#39;ll introduce an additive seasonality as an independent component. . Adding multiplicative seasonality to the model . For this model we&#39;ll use the monthly air Passengers data, which contains 144 observations of passenger counts. We&#39;ll use the first 120 points for modeling and the last 24 for prediction. . Although the passenger counts are clearly integers, the outcome of the Student&#39;s T distribution is real number. Later we might try another heavy tailed integer valued distribution, such as Negative Binomial, to better fit the observation. . sgt_y = jnp.array(air_passengers, dtype=&#39;float64&#39;) plt.plot(air_passengers, &#39;.&#39;) plt.title(&#39;Monthly Air passengers&#39;); . In this model we&#39;ll replace the local trend with a seasonal effect. Specifically, we&#39;ll start with a multiplicative model in this section and explore the additive one in the next. . This model assumes the number of periods for seasonality is already known and thus not inferred from the data. This works with datasets with natural seasonality, which is the case here with monthly data. . Because this is a multiplicative model, when computing the global trend and seasonality, we have to remove from the outcome the respective effects of each other, that&#39;s why we have the divisions in the smoothed average process. . def sgt(y, T, future=3): N = y.shape[0] nu = sample(&quot;nu&quot;, dist.Uniform(1, 50)) xi = sample(&quot;xi&quot;, dist.HalfNormal(2)) tau = sample(&quot;tau&quot;, dist.Beta(1, 4)) kappa = sample(&quot;kappa&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) g_init = sample(&quot;g_init&quot;, dist.TruncatedCauchy(y[0], 10, low=0)) with plate(&quot;T&quot;, size=T): s_init = sample(&quot;s_init&quot;, dist.HalfNormal(4)) def transition_fn(carry, t): g, s = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) mu = deterministic(&quot;mu&quot;, jnp.clip(G * s[0], 0)) sigma = deterministic(&quot;sigma&quot;, xi + kappa * mu ** tau) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * (yt/s[0]) + (1-alpha) * G, 0.1)) su = deterministic(&quot;su&quot;, jnp.clip(beta * (yt/G) + (1 - beta) * s[0], 0.1)) s_new = deterministic(&quot;s&quot;, jnp.concatenate([s[1:], su[None]], axis=0)) return (g_new, s_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, s_init), jnp.arange(0, N+future)) return ys print(&quot;Observed data:&quot;, sgt_y[:9]) with seed(rng_seed=3): print(&quot; nPrior sampling:&quot;) print(sgt(sgt_y[:1], T=3, future=8)) print(&quot; nFixed data sampling:&quot;) print(sgt(sgt_y[:9], T=3, future=0)) . Sample from prior . sgt_prior = Predictive(sgt, num_samples=500)(rng, sgt_y[:1], T=12, future=119) sgt_lvs, sgt_gvs = check_prior(sgt_prior) sgt_lvs.remove(&#39;s&#39;) . Check global parameters . compare_global_params(sgt_prior, [&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;kappa&#39;, &#39;rho&#39;, &#39;tau&#39;], val_ref=10000) . And local variables . plot_locals(sgt_prior, sgt_lvs, val_ref=50000, nrows=3, ncols=2, figsize=(8,8)) . Prior prediction conditioned on data . sgt_prior_fixed = Predictive(sgt, num_samples=500)(rng, sgt_y[:-24], T=12, future=0) plot_locals(sgt_prior_fixed, sgt_lvs, v_ref=&#39;sigma&#39;, val_ref=100, nrows=3, ncols=2, figsize=(8, 8)) . Like before, now we chose one sample from the prior predictive samples and do parameter recovery on it. . n = 34 y = sgt_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . And inference. . kernel = NUTS(sgt) mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False) mcmc.run(rng, y, 12, 0) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . check_inference(mcmc, sgt_prior, sgt_gvs, n=n) . This time, let&#39;s also look at the prediction. . samples = mcmc.get_samples() post = Predictive(sgt, posterior_samples=samples)(rng, y, T=12, future=0) print(&quot;posterior samples:&quot;) for k, v in samples.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in post.items(): print(k, v.shape, end=&#39;; &#39;) . And visualisation. . plot_post(y, post, sgt_lvs) . This looks passable enough. Next we do inference on the real data. . sgt_kernel = NUTS(sgt) sgt_mcmc = MCMC(sgt_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) sgt_mcmc.run(rng, sgt_y[:-24], 12, 0) sgt_mcmc.print_summary() . Now we can make predictions for future periods. . sgt_sample = sgt_mcmc.get_samples() sgt_post = Predictive(sgt, posterior_samples=sgt_sample)(rng, sgt_y[:-24], T=12, future=24) print(&quot;posterior samples:&quot;) for k, v in sgt_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in sgt_post.items(): print(k, v.shape, end=&#39;; &#39;) . Let&#39;s look at the model prediction. . plot_post(sgt_y, sgt_post, sgt_lvs) . The model has been able to capture the growing global trend, and the seasonality present in it. However, looking at the prediction for future periods, it looks clear that the model has been systematically underestimating the outcome. . We have saw similar problem before, in the first LGT model. Back then, the model is also underestimating, and because we have another predictor variable, we have been able to use it to compensate for some extra local information. With this data set, however, we don&#39;t have any predictor available. . One way we can get around this, is to simply use lagged outcomes to construct a predictor matrix, and add a regression component to the model as before. However, there is another way to make use of local information, and that is using a moving window of outcomes. In a later model, we&#39;ll use this moving window information to compute the global trend. But first, we&#39;ll return to, as promised, an additive approach to modeling seasonality. . Finally, look at the scores. . check_scores(sgt_post, sgt_y, 24) . Using additive seasonality . Next we replace the multiplicative seasonality with an additive one. For the multiplicative seasonality, we assume it affects the outcome by enhancing or decreasing the global trend, an amplifying factor, so it naturally only takes positive values. But for the additive seasonality, we assume it adds or deducts from the global trend, so it can be both positive and negative. . sgta_y = sgt_y . The model can be represented as: . We&#39;ll put a relatively wide prior on the seasonality effect. And because updating the seasonality effect needs the outcome before, we need to keep at least T periods of outcomes in memory, when doing the update. . def sgta(y, T, future=3): N = y.shape[0] nu = sample(&quot;nu&quot;, dist.Uniform(1, 50)) xi = sample(&quot;xi&quot;, dist.HalfNormal(2)) tau = sample(&quot;tau&quot;, dist.Beta(1, 3)) kappa = sample(&quot;kappa&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) g_init = sample(&quot;g_init&quot;, dist.TruncatedCauchy(y[0], 10,low=0)) with plate(&quot;T&quot;, size=T): s_init = sample(&quot;s_init&quot;, dist.Cauchy(0, 10)) def transition_fn(carry, t): g, s = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) mu = deterministic(&quot;mu&quot;, jnp.clip(G + s[0], 0)) sigma = deterministic(&quot;sigma&quot;, xi + kappa * mu ** tau) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * (yt-s[0]) + (1-alpha) * g, 0.1)) su = deterministic(&quot;su&quot;, beta * (yt-G) + (1-beta) * s[0]) s_new = deterministic(&quot;s&quot;, jnp.concatenate([s[1:], su[None]], axis=0)) return (g_new, s_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, s_init), jnp.arange(0, N+future)) return ys print(&quot;Observed data:&quot;, sgta_y[:9]) with seed(rng_seed=3): print(&quot; nPrior sampling:&quot;) print(sgta(sgta_y[:1], T=3, future=8)) print(&quot; nFixed data sampling:&quot;) print(sgta(sgta_y[:9], T=3, future=0)) . Sample from prior . sgta_prior = Predictive(sgta, num_samples=500)(rng, sgta_y[:1], T=12, future=119) sgta_lvs, sgta_gvs = check_prior(sgta_prior) sgta_lvs.remove(&#39;s&#39;) . Check global parameters . compare_global_params(sgta_prior, [&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;kappa&#39;, &#39;rho&#39;, &#39;tau&#39;], val_ref=10000) . Nothing special. And local variables . plot_locals(sgta_prior, sgta_lvs, val_ref=1000, nrows=3, ncols=2, figsize=(8,8)) . Prior prediction conditioned on data . sgta_prior_fixed = Predictive(sgta, num_samples=500)(rng, sgta_y[:-24], T=12, future=0) plot_locals(sgta_prior_fixed, sgta_lvs, v_ref=&#39;mu&#39;, val_ref=500, nrows=3, ncols=2, figsize=(8, 8)) . This prior predictive distribution, conditioned on data, looks a bit problematic because as we can see, the seasonality is still present in the global trend, which defeats our intention of separating the two. Later we may want to find better updating mechanisms for them. . Like before, now we chose one sample from the prior predictive samples and do parameter recovery on it. . n = 24 y = sgta_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . And inference. . kernel = NUTS(sgta) mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False) mcmc.run(rng, y, 12, 0) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . check_inference(mcmc, sgta_prior, sgta_gvs, n=n) . Seems passable, though certainly not very precise, and there are a few initial seasonality values a bit far from true value. Let&#39;s also look at the prediction. . samples = mcmc.get_samples() post = Predictive(sgta, posterior_samples=samples)(rng, y, T=12, future=0) print(&quot;posterior samples:&quot;) for k, v in samples.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in post.items(): print(k, v.shape, end=&#39;; &#39;) . And visualisation. . plot_post(y, post, sgta_lvs) . The prediction looks alright but there is a visible increasing trend in the seasonality component, this again confirms the problem we&#39;ve spotted in the prior predictive check, that in this model the global trend and seasonality are not completely separated. Next we do inference on the real data. . Next we do inference on the real data. . sgta_kernel = NUTS(sgta, target_accept_prob=0.95) sgta_mcmc = MCMC(sgta_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) sgta_mcmc.run(rng, sgta_y[:-24], 12, 0) sgta_mcmc.print_summary() . And predictions. . sgta_sample = sgta_mcmc.get_samples() sgta_post = Predictive(sgta, posterior_samples=sgta_sample)(rng, sgta_y[:-24], T=12, future=24) print(&quot;posterior samples:&quot;) for k, v in sgta_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in sgta_post.items(): print(k, v.shape, end=&#39;; &#39;) . Let&#39;s look at the model prediction. . plot_post(sgta_y, sgta_post, sgta_lvs, nrow=3, ncol=2) . Compared to the previous multiplicative seasonality, although the predictions for the individual components are quite different, but the final predictions for the outcome are quite similar. And as a model defect, we can see that both in this and the previous model, we are slightly underestimating the outcome. . Let&#39;s also check the scores. . check_scores(sgta_post, sgta_y, 24) . The scores are also similar to the previous model. . Calculating hidden level with moving average . There are, of course, infinite ways to model the hidden level in the time series. Up to this point we have been using the current outcome to calculate a weighted average, but we can also use a moving window of past outcomes to do the calculation. In the previous models the outcome seems to be underestimated, so we also have to improve a little bit on this issue, by using more local information. Because the model is seasonal, the natural choice for window width is the seasonality. . sgtm_y = sgt_y . We&#39;ll continue to use the additive seasonality so the only change lies in how we update the hidden level: . The moving average is computed with the outcome of the previous T periods, but there are less than T previous outcomes for the first T periods, so for these first T periods we&#39;ll calculate the moving average with all the outcomes available. When coding the model, it&#39;s easier to keep track of the moving window rather than the moving average, and that&#39;s how we are going to code our model. . def sgtm(y, T, future=3): N = y.shape[0] nu = sample(&quot;nu&quot;, dist.Uniform(1, 50)) xi = sample(&quot;xi&quot;, dist.HalfNormal(2)) tau = sample(&quot;tau&quot;, dist.Beta(1, 3)) kappa = sample(&quot;kappa&quot;, dist.HalfNormal(2)) gamma = sample(&quot;gamma&quot;, dist.HalfNormal(2)) rho = sample(&quot;rho&quot;, dist.Beta(1, 4)) alpha = sample(&quot;alpha&quot;, dist.Beta(2, 2)) beta = sample(&quot;beta&quot;, dist.Beta(2, 2)) g_init = sample(&quot;g_init&quot;, dist.TruncatedCauchy(y[0], 10, low=0)) with plate(&quot;T&quot;, size=T): s_init = sample(&quot;s_init&quot;, dist.Cauchy(0, 10)) def transition_fn(carry, t): g, s, mw = carry G = deterministic(&quot;G&quot;, g + gamma * g ** rho) mu = deterministic(&quot;mu&quot;, jnp.clip(G + s[0], 0)) sigma = deterministic(&quot;sigma&quot;, xi + kappa * mu ** tau) yt = sample(&quot;y&quot;, dist.StudentT(nu, mu, sigma)) mw_new = jnp.concatenate([mw[1:], yt[None]], axis=0) ma = jnp.where(t&lt;T, mw_new.sum()/(t+1), mw_new.sum()/T) g_new = deterministic(&quot;g&quot;, jnp.clip(alpha * (ma-s[0]) + (1-alpha) * G, 0)) su = deterministic(&quot;su&quot;, beta * (yt-G) + (1-beta) * s[0]) s_new = deterministic(&quot;s&quot;, jnp.concatenate([s[1:], su[None]], axis=0)) return (g_new, s_new, mw_new), yt with condition(data={&#39;y&#39;:y}): _, ys = scan(transition_fn, (g_init, s_init, jnp.zeros(T)), jnp.arange(0, N+future)) return ys print(&quot;Observed data:&quot;, sgtm_y[:9]) with seed(rng_seed=3): print(&quot; nPrior sampling:&quot;) print(sgtm(sgtm_y[:1], T=3, future=8)) print(&quot; nFixed data sampling:&quot;) print(sgtm(sgtm_y[:9], T=3, future=0)) . Sample from prior . sgtm_prior = Predictive(sgtm, num_samples=500)(rng, sgtm_y[:1], T=12, future=119) sgtm_lvs, sgtm_gvs = check_prior(sgtm_prior) sgtm_lvs.remove(&#39;s&#39;) . Check global parameters . compare_global_params(sgtm_prior, [&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;kappa&#39;, &#39;rho&#39;, &#39;tau&#39;], val_ref=10000) . And local variables. We limit the range of outcome values to focus on the region most useful to our inference. . plot_locals(sgtm_prior, sgtm_lvs, val_ref=1000, nrows=3, ncols=2, figsize=(8,8)) . Prior prediction conditioned on data . sgtm_prior_fixed = Predictive(sgtm, num_samples=500)(rng, sgtm_y[:-24], T=12, future=0) plot_locals(sgtm_prior_fixed, sgtm_lvs, v_ref=&#39;mu&#39;, val_ref=600, nrows=3, ncols=2, figsize=(8, 8)) . Like before, now we chose one sample from the prior predictive samples and do parameter recovery on it. . n = 34 y = sgtm_prior[&#39;y&#39;][n] plt.plot(y) plt.title(&#39;Fake outcome no. {} from prior&#39;.format(n)); . And inference. . kernel = NUTS(sgtm) mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000, num_chains=4, progress_bar=False) mcmc.run(rng, y, 12, 0) mcmc.print_summary() . Let&#39;s visualise the parameter posterior. . check_inference(mcmc, sgtm_prior, sgtm_gvs, n=n) . And the prediction. . samples = mcmc.get_samples() post = Predictive(sgtm, posterior_samples=samples)(rng, y, T=12, future=0) print(&quot;posterior samples:&quot;) for k, v in samples.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in post.items(): print(k, v.shape, end=&#39;; &#39;) . And visualisation. . plot_post(y, post, sgtm_lvs) . This looks passable enough. Next we do inference on the real data. . sgtm_kernel = NUTS(sgtm) sgtm_mcmc = MCMC(sgtm_kernel, num_warmup=2000, num_samples=2000, num_chains=4, progress_bar=False) sgtm_mcmc.run(rng, sgtm_y[:-24], 12, 0) sgtm_mcmc.print_summary() . Now we can make predictions for future periods. . sgtm_sample = sgtm_mcmc.get_samples() sgtm_post = Predictive(sgtm, posterior_samples=sgtm_sample)(rng, sgtm_y[:-24], T=12, future=24) print(&quot;posterior samples:&quot;) for k, v in sgtm_sample.items(): print(k, v.shape, end=&#39;; &#39;) print(&quot; n nposterior Prediction samples:&quot;) for k, v in sgtm_post.items(): print(k, v.shape, end=&#39;; &#39;) . Let&#39;s look at the model prediction. . plot_post(sgtm_y, sgtm_post, sgtm_lvs, ncol=2) . Here the prediction is slightly better than before, the underestimation problem seems abatted, but we&#39;re spotting another problem. The global trend still has a very strong seasonality effect built in it. Upon some reflection it&#39;s not difficult to understand why: we&#39;re now using the moving average to calculate the global trend, and since the moving average uses all the outcome for one cycle, the seasonality is clearly present in it. . And finally the scores. . check_scores(sgtm_post, sgtm_y, 24) . Conclusion . In this post we studied some time series models for global trend, local variation, and seasonality. Our goal is not to build the perfect model; rather, our goal is to build models using composable components, observe their effects, and make changes when necessary, so that we can model the specific characteristics in each data set. . The models are all based on a smoothed exponential process, which is quite flexible in its wide range of behaviours, but sometimes when modeling data with strong pattern this flexibility might not be necessary. . The original Rlgt package also includes a double seasonality model, and a seasonality modeled by an extra exponentially smoothed process, these models are not implemented here, because they don&#39;t enhance the performance, and induce more identifiability issues. .",
            "url": "https://riversdark.github.io/variation/data/2021/05/18/lgts.html",
            "relUrl": "/data/2021/05/18/lgts.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://riversdark.github.io/variation/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://riversdark.github.io/variation/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riversdark.github.io/variation/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riversdark.github.io/variation/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}